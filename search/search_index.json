{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#what-is-it","title":"What is it?","text":"<p>mistral-common is a set of tools to help you work with Mistral models.</p> <p>We open-source the tokenizers, validation and normalization code that can be used with our models.</p> <p>This ensures that you can take full advantage of our models for the following features:</p> <ul> <li>tokenization of text, images and tools calls.</li> <li>validation and normalization of requests, messages, tool calls, and responses. This is built on top of the Pydantic library.</li> </ul> <p>We also version our tokenizers to guarantee backward compatibility for the models that we release.</p>"},{"location":"#for-who","title":"For who ?","text":"<p>This library is for you if you want to:</p> <ul> <li>use our models in your own application.</li> <li>build your own models and want to use the same tokenization and validation code as we do.</li> </ul>"},{"location":"#table-of-contents","title":"Table of contents","text":"<p>Explore the following sections:</p> <ul> <li>Quickstart for simple code snippets to use the library for different tasks.</li> <li>Usage section to install and use the library.</li> <li>Code Reference section to see the code documentation.</li> </ul>"},{"location":"#launch-the-documentation-locally","title":"Launch the documentation locally","text":"<p>To launch the documentation locally, simply run the following command at the root of the repository: <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"code_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mistral_common<ul> <li>audio</li> <li>base</li> <li>exceptions</li> <li>experimental<ul> <li>app<ul> <li>main</li> <li>models</li> <li>routers</li> </ul> </li> <li>think</li> <li>tools</li> <li>utils</li> </ul> </li> <li>image</li> <li>imports</li> <li>protocol<ul> <li>base</li> <li>fim<ul> <li>request</li> </ul> </li> <li>instruct<ul> <li>chunk</li> <li>converters</li> <li>messages</li> <li>normalize</li> <li>request</li> <li>tool_calls</li> <li>validator</li> </ul> </li> <li>transcription<ul> <li>request</li> </ul> </li> <li>utils</li> </ul> </li> <li>tokens<ul> <li>instruct<ul> <li>request</li> </ul> </li> <li>tokenizers<ul> <li>audio</li> <li>base</li> <li>image</li> <li>instruct</li> <li>mistral</li> <li>sentencepiece</li> <li>tekken</li> <li>utils</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"code_reference/mistral_common/audio/","title":"audio","text":""},{"location":"code_reference/mistral_common/audio/#mistral_common.audio","title":"<code>mistral_common.audio</code>","text":""},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio","title":"<code>Audio(audio_array, sampling_rate, format)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>audio_array</code> <code>ndarray</code> <p>The audio data as a numpy array.</p> required <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the audio in Hz.</p> required <code>format</code> <code>str</code> <p>The format of the audio file.</p> required Source code in <code>src/mistral_common/audio.py</code> <pre><code>def __init__(self, audio_array: np.ndarray, sampling_rate: int, format: str) -&gt; None:\n    r\"\"\"Initialize an Audio instance with audio data, sampling rate, and format.\n\n    Args:\n        audio_array: The audio data as a numpy array.\n        sampling_rate: The sampling rate of the audio in Hz.\n        format: The format of the audio file.\n    \"\"\"\n    self.audio_array = audio_array\n    self.sampling_rate = sampling_rate\n    self.format = format\n    self._check_valid()\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Calculate the duration of the audio in seconds.</p> <p>Returns:</p> Type Description <code>float</code> <p>The duration of the audio in seconds.</p>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.from_base64","title":"<code>from_base64(audio_base64, strict=True)</code>  <code>staticmethod</code>","text":"<p>Create an Audio instance from a base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>audio_base64</code> <code>str</code> <p>The base64 encoded audio data.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce mono audio. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Audio</code> <p>An instance of the Audio class.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>@staticmethod\ndef from_base64(audio_base64: str, strict: bool = True) -&gt; \"Audio\":\n    r\"\"\"Create an Audio instance from a base64 encoded string.\n\n    Args:\n        audio_base64: The base64 encoded audio data.\n        strict: Whether to strictly enforce mono audio. Defaults to True.\n\n    Returns:\n        An instance of the Audio class.\n    \"\"\"\n    assert_soundfile_installed()\n\n    if re.match(r\"^data:audio/\\w+;base64,\", audio_base64):  # Remove the prefix if it exists\n        audio_base64 = audio_base64.split(\",\")[1]\n\n    try:\n        audio_bytes = base64.b64decode(audio_base64)\n    except Exception as e:\n        raise ValueError(\"base64 decoding failed. Please check the input string is a valid base64.\") from e\n\n    return Audio.from_bytes(audio_bytes, strict=strict)\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.from_bytes","title":"<code>from_bytes(audio_bytes, strict=True)</code>  <code>staticmethod</code>","text":"<p>Create an Audio instance from bytes.</p> <p>Parameters:</p> Name Type Description Default <code>audio_bytes</code> <code>bytes</code> <p>The audio data as bytes.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce mono audio. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Audio</code> <p>An instance of the Audio class.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>@staticmethod\ndef from_bytes(audio_bytes: bytes, strict: bool = True) -&gt; \"Audio\":\n    r\"\"\"Create an Audio instance from bytes.\n\n    Args:\n        audio_bytes: The audio data as bytes.\n        strict: Whether to strictly enforce mono audio. Defaults to True.\n\n    Returns:\n        An instance of the Audio class.\n    \"\"\"\n    # Read the bytes into an audio file.\n    with io.BytesIO(audio_bytes) as audio_file:\n        with sf.SoundFile(audio_file) as f:\n            # Read the entire audio data\n            audio_array = f.read(dtype=\"float32\")\n            sampling_rate = f.samplerate\n            audio_format = f.format\n\n    format_enum = AudioFormat(audio_format)\n    format = format_enum.value.lower()\n\n    if audio_array.ndim != 1:\n        if strict:\n            raise ValueError(f\"{audio_array.ndim=}\")\n        else:\n            audio_array = audio_array.mean(axis=1)\n\n    return Audio(audio_array=audio_array, sampling_rate=sampling_rate, format=format)\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.from_file","title":"<code>from_file(file, strict=True)</code>  <code>staticmethod</code>","text":"<p>Create an Audio instance from an audio file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the audio file.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce mono audio. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Audio</code> <p>An instance of the Audio class.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>@staticmethod\ndef from_file(file: str, strict: bool = True) -&gt; \"Audio\":\n    r\"\"\"Create an Audio instance from an audio file.\n\n    Args:\n        file: Path to the audio file.\n        strict: Whether to strictly enforce mono audio. Defaults to True.\n\n    Returns:\n        An instance of the Audio class.\n    \"\"\"\n    assert_soundfile_installed()\n\n    if isinstance(file, str) and file.startswith(\"file://\"):\n        file = file[7:]\n\n    if not Path(file).exists():\n        raise FileNotFoundError(f\"{file=} does not exist\")\n\n    with open(file, \"rb\") as f:\n        audio_bytes = f.read()\n\n    return Audio.from_bytes(audio_bytes, strict=strict)\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.from_raw_audio","title":"<code>from_raw_audio(audio)</code>  <code>staticmethod</code>","text":"<p>Create an Audio instance from a RawAudio object.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>RawAudio</code> <p>The RawAudio object containing audio data.</p> required <p>Returns:</p> Type Description <code>Audio</code> <p>An instance of the Audio class.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>@staticmethod\ndef from_raw_audio(audio: \"RawAudio\") -&gt; \"Audio\":\n    r\"\"\"Create an Audio instance from a RawAudio object.\n\n    Args:\n        audio: The RawAudio object containing audio data.\n\n    Returns:\n        An instance of the Audio class.\n    \"\"\"\n    if isinstance(audio.data, bytes):\n        return Audio.from_bytes(audio.data)\n    elif isinstance(audio.data, str):\n        return Audio.from_base64(audio.data)\n    else:\n        raise ValueError(f\"Unsupported audio data type: {type(audio.data)}\")\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.from_url","title":"<code>from_url(url, strict=True)</code>  <code>staticmethod</code>","text":"<p>Create an Audio instance from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the audio file.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce mono audio.</p> <code>True</code> <p>Returns:</p> Type Description <code>Audio</code> <p>An instance of the Audio class.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>@staticmethod\ndef from_url(url: str, strict: bool = True) -&gt; \"Audio\":\n    r\"\"\"Create an Audio instance from a URL.\n\n    Args:\n        url: The URL of the audio file.\n        strict: Whether to strictly enforce mono audio.\n\n    Returns:\n        An instance of the Audio class.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        return Audio.from_bytes(response.content, strict=strict)\n    except requests.RequestException as e:  # Something went wrong with the request.\n        raise ValueError(f\"Failed to download audio from URL: {url}\") from e\n    except Exception as e:  # Something went wrong with the audio file.\n        raise ValueError(f\"Failed to create Audio instance from URL: {url} .\") from e\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.resample","title":"<code>resample(new_sampling_rate)</code>","text":"<p>Resample audio data to a new sampling rate.</p> <p>Parameters:</p> Name Type Description Default <code>new_sampling_rate</code> <code>int</code> <p>The new sampling rate to resample the audio to.</p> required Source code in <code>src/mistral_common/audio.py</code> <pre><code>def resample(self, new_sampling_rate: int) -&gt; None:\n    r\"\"\"Resample audio data to a new sampling rate.\n\n    Args:\n        new_sampling_rate: The new sampling rate to resample the audio to.\n    \"\"\"\n    if self.sampling_rate == new_sampling_rate:\n        return\n\n    assert_soxr_installed()\n\n    self.audio_array = soxr.resample(self.audio_array, self.sampling_rate, new_sampling_rate, quality=\"HQ\")\n    self.sampling_rate = new_sampling_rate\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.Audio.to_base64","title":"<code>to_base64(format, prefix=False)</code>","text":"<p>Convert the audio data to a base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>The format to encode the audio in.</p> required <code>prefix</code> <code>bool</code> <p>Whether to add a data prefix to the base64 encoded string.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The base64 encoded audio data.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>def to_base64(self, format: str, prefix: bool = False) -&gt; str:\n    r\"\"\"Convert the audio data to a base64 encoded string.\n\n    Args:\n        format: The format to encode the audio in.\n        prefix: Whether to add a data prefix to the base64 encoded string.\n\n    Returns:\n        The base64 encoded audio data.\n    \"\"\"\n    assert_soundfile_installed()\n\n    assert format in EXPECTED_FORMAT_VALUES, f\"{format=} not in {EXPECTED_FORMAT_VALUES=}\"\n\n    with io.BytesIO() as audio_file:\n        sf.write(audio_file, self.audio_array, self.sampling_rate, format=format.upper())\n        audio_file.seek(0)\n        base64_str = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n    if prefix:\n        base64_str = f\"data:audio/{format.lower()};base64,{base64_str}\"\n    return base64_str\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.hertz_to_mel","title":"<code>hertz_to_mel(freq)</code>","text":"<p>Convert frequency from hertz to mels using the \"slaney\" mel-scale.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>float | ndarray</code> <p>The frequency, or multiple frequencies, in hertz (Hz).</p> required <p>Returns:</p> Type Description <code>float | ndarray</code> <p>The frequencies on the mel scale.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>def hertz_to_mel(freq: float | np.ndarray) -&gt; float | np.ndarray:\n    r\"\"\"Convert frequency from hertz to mels using the \"slaney\" mel-scale.\n\n    Args:\n        freq: The frequency, or multiple frequencies, in hertz (Hz).\n\n    Returns:\n        The frequencies on the mel scale.\n    \"\"\"\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = 27.0 / np.log(6.4)\n    mels = 3.0 * freq / 200.0\n\n    if isinstance(freq, np.ndarray):\n        assert isinstance(mels, np.ndarray), type(mels)\n        log_region = freq &gt;= min_log_hertz\n        mels[log_region] = min_log_mel + np.log(freq[log_region] / min_log_hertz) * logstep\n    elif freq &gt;= min_log_hertz:\n        mels = min_log_mel + np.log(freq / min_log_hertz) * logstep\n\n    return mels\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.mel_filter_bank","title":"<code>mel_filter_bank(num_frequency_bins, num_mel_bins, min_frequency, max_frequency, sampling_rate)</code>  <code>cached</code>","text":"<p>Create a Mel filter bank matrix for converting frequency bins to the Mel scale.</p> <p>This function generates a filter bank matrix that can be used to transform a spectrum represented in frequency bins to the Mel scale. The Mel scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequency_bins</code> <code>int</code> <p>The number of frequency bins in the input spectrum.</p> required <code>num_mel_bins</code> <code>int</code> <p>The number of desired Mel bins in the output.</p> required <code>min_frequency</code> <code>float</code> <p>The minimum frequency (in Hz) to consider.</p> required <code>max_frequency</code> <code>float</code> <p>The maximum frequency (in Hz) to consider.</p> required <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the audio signal.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A filter bank matrix of shape (num_mel_bins, num_frequency_bins)</p> <code>ndarray</code> <p>that can be used to project frequency bin energies onto Mel bins.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>@cache\ndef mel_filter_bank(\n    num_frequency_bins: int,\n    num_mel_bins: int,\n    min_frequency: float,\n    max_frequency: float,\n    sampling_rate: int,\n) -&gt; np.ndarray:\n    r\"\"\"Create a Mel filter bank matrix for converting frequency bins to the Mel scale.\n\n    This function generates a filter bank matrix that can be used to transform a\n    spectrum represented in frequency bins to the Mel scale. The Mel scale is a\n    perceptual scale of pitches judged by listeners to be equal in distance from one another.\n\n    Args:\n        num_frequency_bins: The number of frequency bins in the input spectrum.\n        num_mel_bins: The number of desired Mel bins in the output.\n        min_frequency: The minimum frequency (in Hz) to consider.\n        max_frequency: The maximum frequency (in Hz) to consider.\n        sampling_rate: The sampling rate of the audio signal.\n\n    Returns:\n        A filter bank matrix of shape (num_mel_bins, num_frequency_bins)\n        that can be used to project frequency bin energies onto Mel bins.\n    \"\"\"\n    if num_frequency_bins &lt; 2:\n        raise ValueError(f\"Require num_frequency_bins: {num_frequency_bins} &gt;= 2\")\n\n    if min_frequency &gt; max_frequency:\n        raise ValueError(f\"Require min_frequency: {min_frequency} &lt;= max_frequency: {max_frequency}\")\n\n    # center points of the triangular mel filters\n    mel_min = hertz_to_mel(min_frequency)\n    mel_max = hertz_to_mel(max_frequency)\n    mel_freqs = np.linspace(mel_min, mel_max, num_mel_bins + 2)\n    filter_freqs = mel_to_hertz(mel_freqs)\n\n    # frequencies of FFT bins in Hz\n    fft_freqs = np.linspace(0, sampling_rate // 2, num_frequency_bins)\n\n    mel_filters = _create_triangular_filter_bank(fft_freqs, filter_freqs)\n\n    # Slaney-style mel is scaled to be approx constant energy per channel\n    enorm = 2.0 / (filter_freqs[2 : num_mel_bins + 2] - filter_freqs[:num_mel_bins])\n    mel_filters *= np.expand_dims(enorm, 0)\n\n    if (mel_filters.max(axis=0) == 0.0).any():\n        raise ValueError(\n            \"At least one mel filter has all zero values. \"\n            f\"The value for `num_mel_filters` ({num_mel_bins}) \"\n            \"may be set too high. \"\n            \"Or, the value for `num_frequency_bins` \"\n            f\"({num_frequency_bins}) may be set too low.\"\n        )\n    return mel_filters\n</code></pre>"},{"location":"code_reference/mistral_common/audio/#mistral_common.audio.mel_to_hertz","title":"<code>mel_to_hertz(mels)</code>","text":"<p>Convert frequency from mels to hertz using the \"slaney\" mel-scale.</p> <p>Parameters:</p> Name Type Description Default <code>mels</code> <code>ndarray</code> <p>The frequency, or multiple frequencies, in mels.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The frequencies in hertz.</p> Source code in <code>src/mistral_common/audio.py</code> <pre><code>def mel_to_hertz(mels: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Convert frequency from mels to hertz using the \"slaney\" mel-scale.\n\n    Args:\n        mels: The frequency, or multiple frequencies, in mels.\n\n    Returns:\n        The frequencies in hertz.\n    \"\"\"\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = np.log(6.4) / 27.0\n    freq = 200.0 * mels / 3.0\n\n    log_region = mels &gt;= min_log_mel\n    freq[log_region] = min_log_hertz * np.exp(logstep * (mels[log_region] - min_log_mel))\n    return freq\n</code></pre>"},{"location":"code_reference/mistral_common/base/","title":"base","text":""},{"location":"code_reference/mistral_common/base/#mistral_common.base","title":"<code>mistral_common.base</code>","text":""},{"location":"code_reference/mistral_common/base/#mistral_common.base.MistralBase","title":"<code>MistralBase(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all Mistral Pydantic models.</p> <p>Forbids extra attributes, validates default values and use enum values.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/","title":"exceptions","text":""},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions","title":"<code>mistral_common.exceptions</code>","text":""},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidAssistantMessageException","title":"<code>InvalidAssistantMessageException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid assistant messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidAssistantMessageException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidFunctionCallException","title":"<code>InvalidFunctionCallException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid function calls.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidFunctionCallException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidMessageStructureException","title":"<code>InvalidMessageStructureException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid message structures.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidMessageStructureException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidRequestException","title":"<code>InvalidRequestException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid requests.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidRequestException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidSystemPromptException","title":"<code>InvalidSystemPromptException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid system prompts.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidSystemPromptException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidToolException","title":"<code>InvalidToolException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid tools.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidToolException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidToolMessageException","title":"<code>InvalidToolMessageException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid tool messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidToolMessageException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidToolSchemaException","title":"<code>InvalidToolSchemaException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid tool schemas.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidToolSchemaException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.InvalidUserMessageException","title":"<code>InvalidUserMessageException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for invalid user messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `InvalidUserMessageException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.MistralCommonException","title":"<code>MistralCommonException(message=None)</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all Mistral exceptions.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> <p>If no message is provided, the default message is used.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | None</code> <p>A human-readable message describing the error.</p> <code>None</code> Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the `MistralCommonException` with an optional message.\n\n    If no message is provided, the default message is used.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    if message:\n        self.message = message\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.TokenizerException","title":"<code>TokenizerException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for errors in the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `TokenizerException` with a message.\n\n    Args:\n      message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/exceptions/#mistral_common.exceptions.UnsupportedTokenizerFeatureException","title":"<code>UnsupportedTokenizerFeatureException(message)</code>","text":"<p>               Bases: <code>MistralCommonException</code></p> <p>Exception raised for unsupported features in the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A human-readable message describing the error.</p> required Source code in <code>src/mistral_common/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    r\"\"\"Initialize the `UnsupportedTokenizerFeatureException` with a message.\n\n    Args:\n       message: A human-readable message describing the error.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"code_reference/mistral_common/image/","title":"image","text":""},{"location":"code_reference/mistral_common/image/#mistral_common.image","title":"<code>mistral_common.image</code>","text":""},{"location":"code_reference/mistral_common/image/#mistral_common.image.download_image","title":"<code>download_image(url)</code>","text":"<p>Download an image from a URL and return it as a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image to download.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The downloaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def download_image(url: str) -&gt; Image.Image:\n    r\"\"\"Download an image from a URL and return it as a PIL Image.\n\n    Args:\n        url: The URL of the image to download.\n\n    Returns:\n       The downloaded image as a PIL Image object.\n    \"\"\"\n    headers = {\"User-Agent\": f\"mistral-common/{__version__}\"}\n    try:\n        # Make a request to download the image\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n\n        # Convert the image content to a PIL Image\n        img = Image.open(io.BytesIO(response.content))\n        return img\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error downloading the image from {url}: {e}.\")\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to PIL image: {e}\")\n</code></pre>"},{"location":"code_reference/mistral_common/image/#mistral_common.image.maybe_load_image_from_str_or_bytes","title":"<code>maybe_load_image_from_str_or_bytes(x)</code>","text":"<p>Load an image from a string or bytes.</p> <p>If the input is already a PIL Image, return it as is.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Image | str | bytes</code> <p>The input to load the image from. Can be a PIL Image, a string, or bytes. If it's a string, it's assumed to be a base64 encoded string of bytes.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The loaded image as a PIL Image object.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def maybe_load_image_from_str_or_bytes(x: Image.Image | str | bytes) -&gt; Image.Image:\n    r\"\"\"Load an image from a string or bytes.\n\n    If the input is already a PIL Image, return it as is.\n\n    Args:\n        x: The input to load the image from. Can be a PIL Image, a string, or bytes.\n            If it's a string, it's assumed to be a base64 encoded string of bytes.\n\n    Returns:\n       The loaded image as a PIL Image object.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x\n    if isinstance(x, bytes):\n        try:\n            return Image.open(io.BytesIO(x))\n        except Exception:\n            raise RuntimeError(\"Encountered an error when loading image from bytes.\")\n\n    try:\n        image = Image.open(io.BytesIO(base64.b64decode(x.encode(\"ascii\"))))\n        return image\n    except Exception as e:\n        raise RuntimeError(\n            f\"Encountered an error when loading image from bytes starting \"\n            f\"with '{x[:20]}'. Expected either a PIL.Image.Image or a base64 \"\n            f\"encoded string of bytes.\"\n        ) from e\n</code></pre>"},{"location":"code_reference/mistral_common/image/#mistral_common.image.serialize_image_to_byte_str","title":"<code>serialize_image_to_byte_str(im, info)</code>","text":"<p>Serialize an image to a base64 encoded string of bytes.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>The image to serialize.</p> required <code>info</code> <code>SerializationInfo</code> <p>The serialization info.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The serialized image as a base64 encoded string of bytes.</p> Source code in <code>src/mistral_common/image.py</code> <pre><code>def serialize_image_to_byte_str(im: Image.Image, info: SerializationInfo) -&gt; str:\n    r\"\"\"Serialize an image to a base64 encoded string of bytes.\n\n    Args:\n        im: The image to serialize.\n        info: The serialization info.\n\n    Returns:\n        The serialized image as a base64 encoded string of bytes.\n    \"\"\"\n    if hasattr(info, \"context\"):\n        context = info.context or {}\n    else:\n        context = {}\n\n    stream = io.BytesIO()\n    im_format = im.format or \"PNG\"\n    im.save(stream, format=im_format)\n    im_b64 = base64.b64encode(stream.getvalue()).decode(\"ascii\")\n    if context and (max_image_b64_len := context.get(\"max_image_b64_len\")):\n        return im_b64[:max_image_b64_len] + \"...\"\n    if context and context.get(\"add_format_prefix\"):\n        im_b64 = f\"data:image/{im_format.lower()};base64,\" + im_b64\n    return im_b64\n</code></pre>"},{"location":"code_reference/mistral_common/imports/","title":"imports","text":""},{"location":"code_reference/mistral_common/imports/#mistral_common.imports","title":"<code>mistral_common.imports</code>","text":""},{"location":"code_reference/mistral_common/experimental/think/","title":"think","text":""},{"location":"code_reference/mistral_common/experimental/think/#mistral_common.experimental.think","title":"<code>mistral_common.experimental.think</code>","text":""},{"location":"code_reference/mistral_common/experimental/tools/","title":"tools","text":""},{"location":"code_reference/mistral_common/experimental/tools/#mistral_common.experimental.tools","title":"<code>mistral_common.experimental.tools</code>","text":""},{"location":"code_reference/mistral_common/experimental/utils/","title":"utils","text":""},{"location":"code_reference/mistral_common/experimental/utils/#mistral_common.experimental.utils","title":"<code>mistral_common.experimental.utils</code>","text":""},{"location":"code_reference/mistral_common/experimental/app/main/","title":"main","text":""},{"location":"code_reference/mistral_common/experimental/app/main/#mistral_common.experimental.app.main","title":"<code>mistral_common.experimental.app.main</code>","text":""},{"location":"code_reference/mistral_common/experimental/app/main/#mistral_common.experimental.app.main.cli","title":"<code>cli()</code>","text":"<p>Mistral-common CLI.</p> Source code in <code>src/mistral_common/experimental/app/main.py</code> <pre><code>@click.group()\ndef cli() -&gt; None:\n    r\"\"\"Mistral-common CLI.\"\"\"\n    pass\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/main/#mistral_common.experimental.app.main.create_app","title":"<code>create_app(tokenizer, validation_mode=ValidationMode.test, engine_url='127.0.0.1', engine_backend=EngineBackend.llama_cpp, timeout=60)</code>","text":"<p>Create a Mistral-common FastAPI app with the given tokenizer and validation mode.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>str | Path | MistralTokenizer</code> <p>The tokenizer path or a MistralTokenizer instance.</p> required <code>validation_mode</code> <code>ValidationMode</code> <p>The validation mode to use.</p> <code>test</code> <code>engine_url</code> <code>str</code> <p>The URL of the engine API.</p> <code>'127.0.0.1'</code> <code>timeout</code> <code>int</code> <p>The timeout of the engine API.</p> <code>60</code> <p>Returns:</p> Type Description <code>FastAPI</code> <p>The Mistral-common FastAPI app.</p> Source code in <code>src/mistral_common/experimental/app/main.py</code> <pre><code>def create_app(\n    tokenizer: str | Path | MistralTokenizer,\n    validation_mode: ValidationMode = ValidationMode.test,\n    engine_url: str = \"127.0.0.1\",\n    engine_backend: EngineBackend = EngineBackend.llama_cpp,\n    timeout: int = 60,\n) -&gt; FastAPI:\n    r\"\"\"Create a Mistral-common FastAPI app with the given tokenizer and validation mode.\n\n    Args:\n        tokenizer: The tokenizer path or a MistralTokenizer instance.\n        validation_mode: The validation mode to use.\n        engine_url: The URL of the engine API.\n        timeout: The timeout of the engine API.\n\n    Returns:\n        The Mistral-common FastAPI app.\n    \"\"\"\n    if not isinstance(tokenizer, (MistralTokenizer, str, Path)):\n        raise ValueError(\"Tokenizer must be a path or a MistralTokenizer instance.\")\n\n    app = FastAPI()\n    app.include_router(tokenize_router)\n    app.include_router(decode_router)\n    app.include_router(main_router)\n\n    @lru_cache\n    def get_settings_override() -&gt; Settings:\n        settings = Settings(\n            engine_url=engine_url,\n            engine_backend=engine_backend,\n            timeout=timeout,\n        )\n        if isinstance(tokenizer, MistralTokenizer):\n            settings.tokenizer = tokenizer\n        else:\n            settings._load_tokenizer(tokenizer, validation_mode)\n        return settings\n\n    get_settings_override()\n    app.dependency_overrides[get_settings] = get_settings_override\n\n    return app\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/main/#mistral_common.experimental.app.main.serve","title":"<code>serve(tokenizer_path, validation_mode=ValidationMode.test, host='127.0.0.1', port=0, engine_url='http://127.0.0.1:8080', engine_backend=EngineBackend.llama_cpp.value, timeout=60)</code>","text":"<p>Serve the Mistral-common API with the given tokenizer path and validation mode.</p> Source code in <code>src/mistral_common/experimental/app/main.py</code> <pre><code>@cli.command(name=\"serve\", context_settings={\"auto_envvar_prefix\": \"UVICORN\"})\n@click.argument(\"tokenizer_path\", type=str)\n@click.argument(\n    \"validation_mode\",\n    type=click.Choice([mode.value for mode in ValidationMode], case_sensitive=False),\n    default=ValidationMode.test.value,\n)\n@click.option(\n    \"--host\",\n    type=str,\n    default=\"127.0.0.1\",\n    help=\"Mistral-common API host\",\n    show_default=True,\n)\n@click.option(\n    \"--port\",\n    type=int,\n    default=0,\n    help=\"Mistral-common API port\",\n    show_default=True,\n)\n@click.option(\n    \"--engine-url\",\n    type=str,\n    default=\"http://127.0.0.1:8080\",\n    help=\"Enginge URL\",\n    show_default=True,\n)\n@click.option(\n    \"--engine-backend\",\n    type=click.Choice([mode.value for mode in EngineBackend], case_sensitive=False),\n    default=EngineBackend.llama_cpp.value,\n    help=\"Engine API backend\",\n    show_default=True,\n)\n@click.option(\n    \"--timeout\",\n    type=int,\n    default=60,\n    help=\"Timeout\",\n    show_default=True,\n)\ndef serve(\n    tokenizer_path: str | Path,\n    validation_mode: ValidationMode | str = ValidationMode.test,\n    host: str = \"127.0.0.1\",\n    port: int = 0,\n    engine_url: str = \"http://127.0.0.1:8080\",\n    engine_backend: str = EngineBackend.llama_cpp.value,\n    timeout: int = 60,\n) -&gt; None:\n    r\"\"\"Serve the Mistral-common API with the given tokenizer path and validation mode.\"\"\"\n    app = create_app(\n        tokenizer=tokenizer_path,\n        validation_mode=ValidationMode(validation_mode),\n        engine_url=engine_url,\n        engine_backend=EngineBackend(engine_backend),\n        timeout=timeout,\n    )\n    uvicorn.run(app, host=host, port=port)\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/models/","title":"models","text":""},{"location":"code_reference/mistral_common/experimental/app/models/#mistral_common.experimental.app.models","title":"<code>mistral_common.experimental.app.models</code>","text":""},{"location":"code_reference/mistral_common/experimental/app/models/#mistral_common.experimental.app.models.EngineBackend","title":"<code>EngineBackend</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The engine backend to use.</p> <p>Attributes:</p> Name Type Description <code>llama_cpp</code> <p>The llama.cpp backend.</p>"},{"location":"code_reference/mistral_common/experimental/app/models/#mistral_common.experimental.app.models.OpenAIChatCompletionRequest","title":"<code>OpenAIChatCompletionRequest(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>OpenAI chat completion request.</p> <p>This class is used to parse the request body for the OpenAI chat completion endpoint.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[dict[str, str | list[dict[str, str | dict[str, Any]]]]]</code> <p>The messages to use for the chat completion.</p> <code>tools</code> <code>list[dict[str, Any]] | None</code> <p>The tools to use for the chat completion.</p> Note <p>This class accepts extra fields that are not validated.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/models/#mistral_common.experimental.app.models.OpenAIChatCompletionRequest.drop_extra_fields","title":"<code>drop_extra_fields()</code>","text":"<p>Drop extra fields from the model.</p> <p>This method is used to drop extra fields from the model, which are not defined in the model fields.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The extra fields that were dropped from the model.</p> Source code in <code>src/mistral_common/experimental/app/models.py</code> <pre><code>def drop_extra_fields(self) -&gt; dict[str, Any]:\n    r\"\"\"Drop extra fields from the model.\n\n    This method is used to drop extra fields from the model, which are not defined in the model fields.\n\n    Returns:\n        The extra fields that were dropped from the model.\n    \"\"\"\n    extra_fields = {\n        field: value for field, value in self.model_dump().items() if field not in type(self).model_fields\n    }\n\n    self.__dict__ = {k: v for k, v in self.__dict__.items() if k not in extra_fields}\n    return extra_fields\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/models/#mistral_common.experimental.app.models.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the Mistral-common API.</p> <p>Attributes:</p> Name Type Description <code>app_name</code> <code>str</code> <p>The name of the application.</p> <code>app_version</code> <code>str</code> <p>The version of the application.</p> <code>engine_url</code> <code>str</code> <p>The URL of the engine.</p> <code>engine_backend</code> <code>EngineBackend</code> <p>The backend to use for the engine.</p> <code>timeout</code> <code>int</code> <p>The timeout to use for the engine API.</p>"},{"location":"code_reference/mistral_common/experimental/app/models/#mistral_common.experimental.app.models.get_settings","title":"<code>get_settings()</code>","text":"<p>Get the settings for the Mistral-common API.</p> Source code in <code>src/mistral_common/experimental/app/models.py</code> <pre><code>def get_settings() -&gt; Settings:\n    r\"\"\"Get the settings for the Mistral-common API.\"\"\"\n    return Settings()\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/routers/","title":"routers","text":""},{"location":"code_reference/mistral_common/experimental/app/routers/#mistral_common.experimental.app.routers","title":"<code>mistral_common.experimental.app.routers</code>","text":""},{"location":"code_reference/mistral_common/experimental/app/routers/#mistral_common.experimental.app.routers.detokenize_to_assistant_message","title":"<code>detokenize_to_assistant_message(settings, tokens=Body(default_factory=list))</code>  <code>async</code>","text":"<p>Detokenize a list of tokens to an assistant message.</p> <p>Parse tool calls from the tokens and extract content before the first tool call.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The tokens to detokenize.</p> <code>Body(default_factory=list)</code> <p>Returns:</p> Type Description <code>AssistantMessage</code> <p>The detokenized assistant message.</p> Source code in <code>src/mistral_common/experimental/app/routers.py</code> <pre><code>@decode_router.post(\"/\")\nasync def detokenize_to_assistant_message(\n    settings: Annotated[Settings, Depends(get_settings)],\n    tokens: list[int] = Body(default_factory=list),\n) -&gt; AssistantMessage:\n    r\"\"\"Detokenize a list of tokens to an assistant message.\n\n    Parse tool calls from the tokens and extract content before the first tool call.\n\n    Args:\n        tokens: The tokens to detokenize.\n\n    Returns:\n        The detokenized assistant message.\n    \"\"\"\n    if len(tokens) == 0:\n        raise HTTPException(status_code=400, detail=\"Tokens list cannot be empty.\")\n\n    if settings.tokenizer.instruct_tokenizer.tokenizer.version &gt; TokenizerVersion.v1:\n        content_tokens, tool_calls_tokens = _split_content_and_tool_calls(\n            tokens, settings.tokenizer.instruct_tokenizer.tokenizer.get_special_token(\"[TOOL_CALLS]\")\n        )\n    else:\n        content_tokens, tool_calls_tokens = tokens, ()\n\n    content: str | list[TextChunk | ThinkChunk] | None = None\n\n    if settings.tokenizer.instruct_tokenizer.tokenizer.version &gt;= TokenizerVersion.v13:\n        assert isinstance(settings.tokenizer.instruct_tokenizer, InstructTokenizerV13)\n\n        begin_think = settings.tokenizer.instruct_tokenizer.BEGIN_THINK\n        end_think = settings.tokenizer.instruct_tokenizer.END_THINK\n    else:\n        begin_think = end_think = None\n\n    if begin_think is not None and end_think is not None:\n        try:\n            content_or_think_tokens = _split_content_and_think_chunks(content_tokens, begin_think, end_think)\n        except ValueError as e:\n            raise HTTPException(status_code=400, detail=str(e))\n\n        eos = settings.tokenizer.instruct_tokenizer.tokenizer.eos_id\n\n        if content_or_think_tokens:\n            content = [\n                TextChunk(text=settings.tokenizer.decode(chunk, special_token_policy=SpecialTokenPolicy.IGNORE))\n                if not is_think\n                else ThinkChunk(\n                    thinking=settings.tokenizer.decode(chunk, special_token_policy=SpecialTokenPolicy.IGNORE),\n                    closed=chunk[-1] == end_think,\n                )\n                for chunk, is_think in content_or_think_tokens\n                if chunk != [eos]  # Don't add a TextChunk with just the EOS token\n            ]\n            if len(content) == 1 and isinstance(content[0], TextChunk):\n                content = content[0].text\n\n    elif content_tokens:\n        content = settings.tokenizer.decode(content_tokens, special_token_policy=SpecialTokenPolicy.IGNORE)\n\n    if tool_calls_tokens:\n        try:\n            tool_calls = _decode_tool_calls(tool_calls_tokens, settings.tokenizer.instruct_tokenizer.tokenizer)\n        except (ValueError, json.JSONDecodeError) as e:\n            raise HTTPException(status_code=400, detail=str(e))\n    else:\n        tool_calls = None\n\n    has_eos = tokens[-1] == settings.tokenizer.instruct_tokenizer.tokenizer.eos_id\n\n    return AssistantMessage(content=content, tool_calls=tool_calls, prefix=not has_eos)\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/routers/#mistral_common.experimental.app.routers.detokenize_to_string","title":"<code>detokenize_to_string(settings, tokens=Body(default_factory=list), special_token_policy=Body(default=SpecialTokenPolicy.IGNORE))</code>  <code>async</code>","text":"<p>Detokenize a list of tokens to a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The tokens to detokenize.</p> <code>Body(default_factory=list)</code> <code>special_token_policy</code> <code>SpecialTokenPolicy</code> <p>The policy to use for special tokens.</p> <code>Body(default=IGNORE)</code> <p>Returns:</p> Type Description <code>str</code> <p>The detokenized string or assistant message.</p> Source code in <code>src/mistral_common/experimental/app/routers.py</code> <pre><code>@decode_router.post(\"/string\")\nasync def detokenize_to_string(\n    settings: Annotated[Settings, Depends(get_settings)],\n    tokens: list[int] = Body(default_factory=list),\n    special_token_policy: SpecialTokenPolicy = Body(default=SpecialTokenPolicy.IGNORE),\n) -&gt; str:\n    r\"\"\"Detokenize a list of tokens to a string.\n\n    Args:\n        tokens: The tokens to detokenize.\n        special_token_policy: The policy to use for special tokens.\n\n    Returns:\n        The detokenized string or assistant message.\n    \"\"\"\n    if len(tokens) == 0:\n        raise HTTPException(status_code=400, detail=\"Tokens list cannot be empty.\")\n    try:\n        return settings.tokenizer.decode(tokens, special_token_policy=special_token_policy)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/routers/#mistral_common.experimental.app.routers.generate","title":"<code>generate(request, settings)</code>  <code>async</code>","text":"<p>Generate a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest | OpenAIChatCompletionRequest</code> <p>The chat completion request.</p> required <code>settings</code> <code>Annotated[Settings, Depends(get_settings)]</code> <p>The settings for the Mistral-common API.</p> required <p>Returns:</p> Type Description <code>AssistantMessage</code> <p>The generated chat completion.</p> Source code in <code>src/mistral_common/experimental/app/routers.py</code> <pre><code>@main_router.post(\"/v1/chat/completions\", tags=[\"chat\", \"completions\"])\nasync def generate(\n    request: ChatCompletionRequest | OpenAIChatCompletionRequest,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; AssistantMessage:\n    r\"\"\"Generate a chat completion.\n\n    Args:\n        request: The chat completion request.\n        settings: The settings for the Mistral-common API.\n\n    Returns:\n        The generated chat completion.\n    \"\"\"\n    if isinstance(request, OpenAIChatCompletionRequest):\n        extra_fields = request.drop_extra_fields()\n        request = ChatCompletionRequest.from_openai(**request.model_dump())\n    else:\n        extra_fields = {}\n    tokens_ids = await tokenize_request(request, settings)\n\n    exclude_fields = {\"messages\", \"tools\"}\n\n    request_json = request.model_dump()\n    request_json.update(extra_fields)\n\n    request_json = {k: v for k, v in request_json.items() if k not in exclude_fields}\n\n    if request_json.get(\"stream\", False):\n        raise HTTPException(status_code=400, detail=\"Streaming is not supported.\")\n\n    try:\n        if settings.engine_backend == EngineBackend.llama_cpp:\n            response = requests.post(\n                f\"{settings.engine_url}/completions\",\n                json={\n                    \"prompt\": tokens_ids,\n                    \"return_tokens\": True,\n                    **request_json,\n                },\n                timeout=settings.timeout,\n            )\n        else:\n            raise ValueError(f\"Unsupported engine backend: {settings.engine_backend}\")\n    except requests.exceptions.Timeout:\n        raise HTTPException(status_code=504, detail=\"Timeout\")\n    except requests.exceptions.RequestException as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    if response.status_code != 200:\n        raise HTTPException(status_code=response.status_code, detail=response.text)\n\n    response_json = response.json()\n    return await detokenize_to_assistant_message(settings, response_json[\"tokens\"])\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/routers/#mistral_common.experimental.app.routers.redirect_to_docs","title":"<code>redirect_to_docs()</code>  <code>async</code>","text":"<p>Redirect to the documentation.</p> Source code in <code>src/mistral_common/experimental/app/routers.py</code> <pre><code>@main_router.get(\"/\")\nasync def redirect_to_docs() -&gt; RedirectResponse:\n    r\"\"\"Redirect to the documentation.\"\"\"\n    return RedirectResponse(url=\"docs\")\n</code></pre>"},{"location":"code_reference/mistral_common/experimental/app/routers/#mistral_common.experimental.app.routers.tokenize_request","title":"<code>tokenize_request(request, settings)</code>  <code>async</code>","text":"<p>Tokenize a chat completion request.</p> Source code in <code>src/mistral_common/experimental/app/routers.py</code> <pre><code>@tokenize_router.post(\"/\")\nasync def tokenize_request(\n    request: ChatCompletionRequest | OpenAIChatCompletionRequest,\n    settings: Annotated[Settings, Depends(get_settings)],\n) -&gt; list[int]:\n    r\"\"\"Tokenize a chat completion request.\"\"\"\n    if isinstance(request, OpenAIChatCompletionRequest):\n        try:\n            request.drop_extra_fields()\n            request = ChatCompletionRequest.from_openai(**request.model_dump())\n        except (ValidationError, ValueError) as e:\n            raise HTTPException(status_code=400, detail=str(e))\n\n    if request.messages == []:\n        raise HTTPException(status_code=400, detail=\"Messages list cannot be empty.\")\n\n    tokenized = settings.tokenizer.encode_chat_completion(request)\n    assert isinstance(tokenized, Tokenized), type(tokenized)\n    return tokenized.tokens\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/base/","title":"base","text":""},{"location":"code_reference/mistral_common/protocol/base/#mistral_common.protocol.base","title":"<code>mistral_common.protocol.base</code>","text":""},{"location":"code_reference/mistral_common/protocol/base/#mistral_common.protocol.base.BaseCompletionRequest","title":"<code>BaseCompletionRequest(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base class for completion requests.</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>float</code> <p>Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>top_p</code> <code>float</code> <p>Nucleus sampling parameter, top-p probability mass, between 0 and 1.</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate.</p> <code>random_seed</code> <code>int | None</code> <p>Random seed for reproducibility.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = BaseCompletionRequest(temperature=0.7, top_p=0.9, max_tokens=100, random_seed=42)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/utils/","title":"utils","text":""},{"location":"code_reference/mistral_common/protocol/utils/#mistral_common.protocol.utils","title":"<code>mistral_common.protocol.utils</code>","text":""},{"location":"code_reference/mistral_common/protocol/utils/#mistral_common.protocol.utils.random_uuid","title":"<code>random_uuid()</code>","text":"<p>Generate a random UUID.</p> Source code in <code>src/mistral_common/protocol/utils.py</code> <pre><code>def random_uuid() -&gt; str:\n    \"\"\"Generate a random UUID.\"\"\"\n    return str(uuid.uuid4().hex)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/fim/request/","title":"request","text":""},{"location":"code_reference/mistral_common/protocol/fim/request/#mistral_common.protocol.fim.request","title":"<code>mistral_common.protocol.fim.request</code>","text":""},{"location":"code_reference/mistral_common/protocol/fim/request/#mistral_common.protocol.fim.request.FIMRequest","title":"<code>FIMRequest(**data)</code>","text":"<p>               Bases: <code>BaseCompletionRequest</code></p> <p>A valid Fill in the Middle completion request to be tokenized.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str</code> <p>The prompt to be completed.</p> <code>suffix</code> <code>str | None</code> <p>The suffix of the prompt. If provided, the model will generate text between the prompt and the suffix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = FIMRequest(prompt=\"Hello, my name is\", suffix=\" and I live in New York.\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/","title":"chunk","text":""},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk","title":"<code>mistral_common.protocol.instruct.chunk</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioChunk","title":"<code>AudioChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Audio chunk containing raw audio data.</p> <p>This class represents a chunk of audio data that can be used as input.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[input_audio]</code> <p>The type of the chunk, which is always ChunkTypes.input_audio.</p> <code>input_audio</code> <code>RawAudio</code> <p>The RawAudio object containing the audio data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; audio_chunk = AudioChunk(input_audio=RawAudio(data=\"base64_encoded_audio_data\", format=\"mp3\"))\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioChunk.from_audio","title":"<code>from_audio(audio)</code>  <code>classmethod</code>","text":"<p>Creates an AudioChunk instance from an Audio object.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Audio</code> <p>An Audio object containing audio data.</p> required <p>Returns:</p> Type Description <code>AudioChunk</code> <p>An AudioChunk instance initialized with the audio data.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_audio(cls, audio: Audio) -&gt; \"AudioChunk\":\n    r\"\"\"Creates an AudioChunk instance from an Audio object.\n\n    Args:\n        audio: An Audio object containing audio data.\n\n    Returns:\n        An AudioChunk instance initialized with the audio data.\n    \"\"\"\n    return cls(input_audio=RawAudio.from_audio(audio))\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> <p>Parameters:</p> Name Type Description Default <code>openai_chunk</code> <code>dict[str, str | dict[str, str]]</code> <p>A dictionary representing the audio chunk in the OpenAI format.</p> required <p>Returns:</p> Type Description <code>AudioChunk</code> <p>An AudioChunk instance initialized with the data from the OpenAI chunk.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"AudioChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\n\n    Args:\n        openai_chunk: A dictionary representing the audio chunk in the OpenAI format.\n\n    Returns:\n        An AudioChunk instance initialized with the data from the OpenAI chunk.\n    \"\"\"\n    return cls.model_validate(openai_chunk)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> <p>Returns:</p> Type Description <code>dict[str, str | dict[str, str]]</code> <p>A dictionary representing the audio chunk in the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\n\n    Returns:\n        A dictionary representing the audio chunk in the OpenAI format.\n    \"\"\"\n    content = (\n        self.input_audio.data.decode(\"utf-8\") if isinstance(self.input_audio.data, bytes) else self.input_audio.data\n    )\n    return {\n        \"type\": self.type,\n        \"input_audio\": RawAudio(data=content, format=self.input_audio.format).model_dump(),\n    }\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioURL","title":"<code>AudioURL(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Audio URL.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the audio file.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioURLChunk","title":"<code>AudioURLChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Audio URL chunk.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[audio_url]</code> <p>The type of the chunk, which is always <code>ChunkTypes.audio_url</code>.</p> <code>audio_url</code> <code>str | AudioURL</code> <p>The URL of the audio file.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioURLChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"AudioURLChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate(openai_chunk)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioURLChunk.get_url_type","title":"<code>get_url_type()</code>","text":"<p>Returns the type of the audio URL.</p> Note <p>URLs should be either: - a valid URL (http:// or https://) - a valid file path (e.g. /path/to/file) - a valid file URI (e.g. file:///path/to/file) - a base64 encoded audio. It is assumed to be base64 encoded if it is not a valid URL or file path.</p> <p>Returns:</p> Type Description <code>AudioURLType</code> <p>The type of the audio URL.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def get_url_type(self) -&gt; AudioURLType:\n    r\"\"\"Returns the type of the audio URL.\n\n    Note:\n        URLs should be either:\n        - a valid URL (http:// or https://)\n        - a valid file path (e.g. /path/to/file)\n        - a valid file URI (e.g. file:///path/to/file)\n        - a base64 encoded audio. It is assumed to be base64 encoded if it is not a valid URL or file path.\n\n    Returns:\n        The type of the audio URL.\n    \"\"\"\n    url_scheme = urlparse(self.url).scheme\n    if url_scheme in {\"http\", \"https\"}:\n        return AudioURLType.url\n    elif url_scheme == \"data\":\n        return AudioURLType.base64\n    elif url_scheme == \"file\":\n        return AudioURLType.file_uri\n\n    try:\n        url_path = Path(self.url)\n        exist_path = url_path.exists()\n    except OSError:  # File name too long\n        exist_path = False\n\n    if exist_path:\n        return AudioURLType.file\n\n    return AudioURLType.base64\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioURLChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    if isinstance(self.audio_url, AudioURL):\n        return self.model_dump()\n    else:\n        return {\"type\": self.type, \"audio_url\": {\"url\": self.audio_url}}\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.AudioURLType","title":"<code>AudioURLType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the types of audio URLs.</p> <p>Attributes:</p> Name Type Description <code>url</code> <p>A URL.</p> <code>base64</code> <p>A base64 encoded audio. Can be prefixed with <code>data:audio/&lt;format&gt;;base64,</code>.</p> <code>file</code> <p>A file path.</p> <code>file_uri</code> <p>A file URI (eg. <code>file:///path/to/file</code>).</p>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.BaseContentChunk","title":"<code>BaseContentChunk(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base class for all content chunks.</p> <p>Content chunks are used to send different types of content to the model.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[text, image, image_url, input_audio, audio_url, thinking]</code> <p>The type of the chunk.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.BaseContentChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"BaseContentChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"from_openai method not implemented for {cls.__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.BaseContentChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"to_openai method not implemented for {type(self).__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ChunkTypes","title":"<code>ChunkTypes</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the types of chunks that can be sent to the model.</p> <p>Attributes:</p> Name Type Description <code>text</code> <p>A text chunk.</p> <code>image</code> <p>An image chunk.</p> <code>image_url</code> <p>An image url chunk.</p> <code>input_audio</code> <p>An input audio chunk.</p> <code>audio_url</code> <p>An audio url chunk.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.chunk import ChunkTypes\n&gt;&gt;&gt; chunk_type = ChunkTypes.text\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageChunk","title":"<code>ImageChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Image chunk.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>SerializableImage</code> <p>The image to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; image_chunk = ImageChunk(image=Image.new('RGB', (200, 200), color='blue'))\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"ImageChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    assert openai_chunk.get(\"type\") == \"image_url\", openai_chunk\n\n    image_url_dict = openai_chunk[\"image_url\"]\n    assert isinstance(image_url_dict, dict) and \"url\" in image_url_dict, image_url_dict\n\n    if re.match(r\"^data:image/\\w+;base64,\", image_url_dict[\"url\"]):  # Remove the prefix if it exists\n        image_url_dict[\"url\"] = image_url_dict[\"url\"].split(\",\")[1]\n\n    return cls.model_validate({\"image\": image_url_dict[\"url\"]})\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    base64_image = self.model_dump(include={\"image\"}, context={\"add_format_prefix\": True})[\"image\"]\n    return {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image}}\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageURL","title":"<code>ImageURL(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Image URL or a base64 encoded image.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the image.</p> <code>detail</code> <code>str | None</code> <p>The detail of the image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_url = ImageURL(url=\"https://example.com/image.png\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageURLChunk","title":"<code>ImageURLChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Image URL chunk.</p> <p>Attributes:</p> Name Type Description <code>image_url</code> <code>ImageURL | str</code> <p>The URL of the image or a base64 encoded image to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_url_chunk = ImageURLChunk(image_url=\"data:image/png;base64,iVBORw0\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageURLChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"ImageURLChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate({\"image_url\": openai_chunk[\"image_url\"]})\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ImageURLChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    image_url_dict = {\"url\": self.get_url()}\n    if isinstance(self.image_url, ImageURL) and self.image_url.detail is not None:\n        image_url_dict[\"detail\"] = self.image_url.detail\n\n    out_dict: dict[str, str | dict[str, str]] = {\n        \"type\": \"image_url\",\n        \"image_url\": image_url_dict,\n    }\n    return out_dict\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.RawAudio","title":"<code>RawAudio(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base64 encoded audio data.</p> <p>This class represents raw audio data encoded in base64 format.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>str | bytes</code> <p>The base64 encoded audio data, which can be a string or bytes.</p> <code>format</code> <code>str</code> <p>The format of the audio data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; audio = RawAudio(data=\"base64_encoded_audio_data\", format=\"mp3\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.RawAudio.from_audio","title":"<code>from_audio(audio)</code>  <code>classmethod</code>","text":"<p>Creates a RawAudio instance from an Audio object.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Audio</code> <p>An Audio object containing audio data, format, and duration.</p> required <p>Returns:</p> Type Description <code>RawAudio</code> <p>An AudioChunk instance initialized with the audio data.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_audio(cls, audio: Audio) -&gt; \"RawAudio\":\n    \"\"\"Creates a RawAudio instance from an Audio object.\n\n    Args:\n        audio: An Audio object containing audio data, format, and duration.\n\n    Returns:\n        An AudioChunk instance initialized with the audio data.\n    \"\"\"\n    format = audio.format\n    data = audio.to_base64(format, False)\n\n    return cls(data=data, format=format)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.TextChunk","title":"<code>TextChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Text chunk.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The text to be sent to the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text_chunk = TextChunk(text=\"Hello, how can I help you?\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.TextChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"TextChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate(openai_chunk)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.TextChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ThinkChunk","title":"<code>ThinkChunk(**data)</code>","text":"<p>               Bases: <code>BaseContentChunk</code></p> <p>Thinking chunk.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal[thinking]</code> <p>The type of the chunk, which is always ChunkTypes.thinking.</p> <code>thinking</code> <code>str</code> <p>The list of text chunks of the thinking.</p> <code>closed</code> <code>bool</code> <p>Whether the thinking chunk is closed or not.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ThinkChunk.from_openai","title":"<code>from_openai(openai_chunk)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI chunk to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_chunk: dict[str, str | dict[str, str]]) -&gt; \"ThinkChunk\":\n    r\"\"\"Converts the OpenAI chunk to the Mistral format.\"\"\"\n    return cls.model_validate(openai_chunk)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/chunk/#mistral_common.protocol.instruct.chunk.ThinkChunk.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the chunk to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/chunk.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | dict[str, str]]:\n    r\"\"\"Converts the chunk to the OpenAI format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/converters/","title":"converters","text":""},{"location":"code_reference/mistral_common/protocol/instruct/converters/#mistral_common.protocol.instruct.converters","title":"<code>mistral_common.protocol.instruct.converters</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/converters/#mistral_common.protocol.instruct.converters.convert_openai_messages","title":"<code>convert_openai_messages(messages)</code>","text":"<p>Convert OpenAI messages to Mistral messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[dict[str, str | list[dict[str, str | dict[str, Any]]]]]</code> <p>The OpenAI messages to convert.</p> required <p>Returns:</p> Type Description <code>list[ChatMessage]</code> <p>The Mistral messages.</p> Source code in <code>src/mistral_common/protocol/instruct/converters.py</code> <pre><code>def convert_openai_messages(\n    messages: list[dict[str, str | list[dict[str, str | dict[str, Any]]]]],\n) -&gt; list[ChatMessage]:\n    r\"\"\"Convert OpenAI messages to Mistral messages.\n\n    Args:\n        messages: The OpenAI messages to convert.\n\n    Returns:\n        The Mistral messages.\n    \"\"\"\n    converted_messages: list[ChatMessage] = []\n    for openai_message in messages:\n        message_role = openai_message.get(\"role\")\n        message: ChatMessage\n        if message_role == \"user\":\n            message = UserMessage.from_openai(openai_message)\n        elif message_role == \"assistant\":\n            message = AssistantMessage.from_openai(openai_message)\n        elif message_role == \"tool\":\n            message = ToolMessage.from_openai(openai_message)\n        elif message_role == \"system\":\n            message = SystemMessage.from_openai(openai_message)\n        else:\n            raise ValueError(f\"Unknown message role: {message_role}\")\n        converted_messages.append(message)\n    return converted_messages\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/converters/#mistral_common.protocol.instruct.converters.convert_openai_tools","title":"<code>convert_openai_tools(tools)</code>","text":"<p>Convert OpenAI tools to Mistral tools.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[dict[str, Any]]</code> <p>The OpenAI tools to convert.</p> required <p>Returns:</p> Type Description <code>list[Tool]</code> <p>The Mistral tools.</p> Source code in <code>src/mistral_common/protocol/instruct/converters.py</code> <pre><code>def convert_openai_tools(\n    tools: list[dict[str, Any]],\n) -&gt; list[Tool]:\n    r\"\"\"Convert OpenAI tools to Mistral tools.\n\n    Args:\n        tools: The OpenAI tools to convert.\n\n    Returns:\n        The Mistral tools.\n    \"\"\"\n    converted_tools = [Tool.from_openai(openai_tool) for openai_tool in tools]\n    return converted_tools\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/","title":"messages","text":""},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages","title":"<code>mistral_common.protocol.instruct.messages</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.AssistantMessage","title":"<code>AssistantMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>Assistant message.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal[assistant]</code> <p>The role of the message.</p> <code>content</code> <code>str | list[TextChunk | ThinkChunk] | None</code> <p>The content of the message.</p> <code>tool_calls</code> <code>list[ToolCall] | None</code> <p>The tool calls of the message.</p> <code>prefix</code> <code>bool</code> <p>Whether the message is a prefix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = AssistantMessage(content=\"Hello, how can I help you?\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.AssistantMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_message: dict[str, str | list[dict[str, str | dict[str, Any]]]]) -&gt; \"AssistantMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    openai_tool_calls = openai_message.get(\"tool_calls\", None)\n    if openai_tool_calls is None:\n        tools_calls: list[ToolCall] | None = None\n    elif isinstance(openai_tool_calls, list):\n        tools_calls = []\n        for openai_tool_call in openai_tool_calls or []:\n            tools_calls.append(ToolCall.from_openai(openai_tool_call))\n    else:\n        raise ValueError(f\"tool_calls must be a list, got {type(openai_tool_calls)}\")\n    openai_content = openai_message.get(\"content\", None)\n    content: str | list[ContentChunk] | None = None\n    if openai_content is None or isinstance(openai_content, str):\n        content = openai_content\n    elif isinstance(openai_content, list):\n        content = [_convert_openai_content_chunks(chunk) for chunk in openai_content]\n    else:\n        raise ValueError(f\"Unknown content type: {type(openai_content)}\")\n\n    return cls.model_validate(\n        {\n            \"role\": openai_message[\"role\"],\n            \"content\": content,\n            \"tool_calls\": tools_calls,\n        }\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.AssistantMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | list[dict[str, str | dict[str, Any]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    out_dict: dict[str, str | list[dict[str, str | dict[str, Any]]]] = {\n        \"role\": self.role,\n    }\n    if self.content is None:\n        pass\n    elif isinstance(self.content, str):\n        out_dict[\"content\"] = self.content\n    else:\n        out_dict[\"content\"] = [chunk.to_openai() for chunk in self.content]\n    if self.tool_calls is not None:\n        out_dict[\"tool_calls\"] = [tool_call.to_openai() for tool_call in self.tool_calls]\n\n    return out_dict\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseMessage","title":"<code>BaseMessage(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Base class for all messages.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal[system, user, assistant, tool]</code> <p>The role of the message.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_message: dict[str, str | list[dict[str, str | dict[str, Any]]]]) -&gt; \"BaseMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"from_openai method not implemented for {cls.__name__}.\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.BaseMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> <p>Should be implemented by subclasses.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | list[dict[str, str | dict[str, Any]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\n\n    Should be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"to_openai method not implemented for {type(self).__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.FinetuningAssistantMessage","title":"<code>FinetuningAssistantMessage(**data)</code>","text":"<p>               Bases: <code>AssistantMessage</code></p> <p>Assistant message for finetuning.</p> <p>Attributes:</p> Name Type Description <code>weight</code> <code>float | None</code> <p>The weight of the message to train on.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = FinetuningAssistantMessage(content=\"Hello, how can I help you?\", weight=0.5)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.Roles","title":"<code>Roles</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the roles of the messages.</p> <p>Attributes:</p> Name Type Description <code>system</code> <p>The system role.</p> <code>user</code> <p>The user role.</p> <code>assistant</code> <p>The assistant role.</p> <code>tool</code> <p>The tool role.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; role = Roles.user\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.SystemMessage","title":"<code>SystemMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>System message.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str | list[TextChunk | ThinkChunk]</code> <p>The content of the message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = SystemMessage(content=\"You are a helpful assistant.\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.SystemMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_message: dict[str, str | list[dict[str, str | dict[str, Any]]]]) -&gt; \"SystemMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    return cls.model_validate(openai_message)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.SystemMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | list[dict[str, str | dict[str, Any]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ToolMessage","title":"<code>ToolMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>Tool message.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str | list[TextChunk]</code> <p>The content of the message.</p> <code>tool_call_id</code> <code>str | None</code> <p>The tool call id of the message.</p> <code>name</code> <code>str | None</code> <p>The name of the tool. (Deprecated in V3 tokenization)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = ToolMessage(content=\"Hello, how can I help you?\", tool_call_id=\"123\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ToolMessage.from_openai","title":"<code>from_openai(messages)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, messages: dict[str, str | list[dict[str, str | dict[str, Any]]]]) -&gt; \"ToolMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    tool_message = cls.model_validate(messages)\n    assert tool_message.tool_call_id is not None, \"tool_call_id must be provided for tool messages.\"\n    return tool_message\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.ToolMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | list[dict[str, str | dict[str, Any]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    assert self.tool_call_id is not None, \"tool_call_id must be provided for tool messages.\"\n    return self.model_dump(exclude={\"name\"})\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.UserMessage","title":"<code>UserMessage(**data)</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>User message.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str | list[UserContentChunk]</code> <p>The content of the message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = UserMessage(content=\"Can you help me to write a poem?\")\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.UserMessage.from_openai","title":"<code>from_openai(openai_message)</code>  <code>classmethod</code>","text":"<p>Converts the OpenAI message to the Mistral format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_message: dict[str, str | list[dict[str, str | dict[str, Any]]]]) -&gt; \"UserMessage\":\n    r\"\"\"Converts the OpenAI message to the Mistral format.\"\"\"\n    if isinstance(openai_message[\"content\"], str):\n        return cls.model_validate(openai_message)\n    return cls.model_validate(\n        {\n            \"role\": openai_message[\"role\"],\n            \"content\": [_convert_openai_content_chunks(chunk) for chunk in openai_message[\"content\"]],\n        },\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/messages/#mistral_common.protocol.instruct.messages.UserMessage.to_openai","title":"<code>to_openai()</code>","text":"<p>Converts the message to the OpenAI format.</p> Source code in <code>src/mistral_common/protocol/instruct/messages.py</code> <pre><code>def to_openai(self) -&gt; dict[str, str | list[dict[str, str | dict[str, Any]]]]:\n    r\"\"\"Converts the message to the OpenAI format.\"\"\"\n    if isinstance(self.content, str):\n        return {\"role\": self.role, \"content\": self.content}\n    return {\"role\": self.role, \"content\": [chunk.to_openai() for chunk in self.content]}\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/","title":"normalize","text":""},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize","title":"<code>mistral_common.protocol.instruct.normalize</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizer","title":"<code>InstructRequestNormalizer(user_message_class, assistant_message_class, tool_message_class, system_message_class, instruct_request_class)</code>","text":"<p>               Bases: <code>Generic[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, InstructRequestType]</code></p> <p>Takes a ChatCompletionRequest and normalizes it into an InstructRequest.</p> <p>The normalization process does several things such as: - Aggregate consecutive messages of the same role - Aggregate system prompts - Normalize json content - Normalize tool calls</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>user_message_class</code> <code>type[UserMessageType]</code> <p>The class for user messages.</p> required <code>assistant_message_class</code> <code>type[AssistantMessageType]</code> <p>The class for assistant messages.</p> required <code>tool_message_class</code> <code>type[ToolMessageType]</code> <p>The class for tool messages.</p> required <code>system_message_class</code> <code>type[SystemMessageType]</code> <p>The class for system messages.</p> required <code>instruct_request_class</code> <code>type[InstructRequestType]</code> <p>The class for instruct requests.</p> required Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def __init__(\n    self,\n    user_message_class: type[UserMessageType],\n    assistant_message_class: type[AssistantMessageType],\n    tool_message_class: type[ToolMessageType],\n    system_message_class: type[SystemMessageType],\n    instruct_request_class: type[InstructRequestType],\n):\n    r\"\"\"Initializes the normalizer with the appropriate message classes.\n\n    Args:\n       user_message_class: The class for user messages.\n       assistant_message_class: The class for assistant messages.\n       tool_message_class: The class for tool messages.\n       system_message_class: The class for system messages.\n       instruct_request_class: The class for instruct requests.\n    \"\"\"\n    self._user_message_class = user_message_class\n    self._assistant_message_class = assistant_message_class\n    self._tool_message_class = tool_message_class\n    self._instruct_request_class = instruct_request_class\n    # this is unused but makes creation nicer\n    self._system_message_class = system_message_class\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizer.from_chat_completion_request","title":"<code>from_chat_completion_request(request)</code>","text":"<p>Converts a chat completion request to an instruct request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest[UATS]</code> <p>The chat completion request to convert.</p> required <p>Returns:</p> Type Description <code>InstructRequestType</code> <p>The converted instruct request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; request = ChatCompletionRequest(\n...     messages=[\n...         UserMessage(content=\"Hello\"),\n...         AssistantMessage(content=\"Hi\"),\n...     ],\n... )\n&gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n&gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def from_chat_completion_request(self, request: ChatCompletionRequest[UATS]) -&gt; InstructRequestType:\n    r\"\"\"Converts a chat completion request to an instruct request.\n\n    Args:\n        request: The chat completion request to convert.\n\n    Returns:\n        The converted instruct request.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n        &gt;&gt;&gt; request = ChatCompletionRequest(\n        ...     messages=[\n        ...         UserMessage(content=\"Hello\"),\n        ...         AssistantMessage(content=\"Hi\"),\n        ...     ],\n        ... )\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n        &gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n    \"\"\"\n    system_prompt = self._aggregate_system_prompts(request.messages)\n    messages = self._aggregate_messages(request.messages)\n\n    return self._instruct_request_class(\n        messages=messages,\n        system_prompt=system_prompt,\n        available_tools=request.tools,\n        continue_final_message=request.continue_final_message,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizer.normalizer","title":"<code>normalizer()</code>  <code>staticmethod</code>","text":"<p>Returns a normalizer for the default instruct request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>@staticmethod\ndef normalizer() -&gt; \"InstructRequestNormalizer\":\n    r\"\"\"Returns a normalizer for the default instruct request.\n\n    Examples:\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizer.normalizer()\n    \"\"\"\n    return InstructRequestNormalizer(\n        UserMessage,\n        AssistantMessage,\n        ToolMessage,\n        SystemMessage,\n        InstructRequest[UATS, Tool],\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV13","title":"<code>InstructRequestNormalizerV13(user_message_class, assistant_message_class, tool_message_class, system_message_class, instruct_request_class)</code>","text":"<p>               Bases: <code>InstructRequestNormalizerV7</code></p> <p>Normalizer for the v13 tokenizer.</p> <p>It reorders tool messages based on the tool call order.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizerV13.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def __init__(\n    self,\n    user_message_class: type[UserMessageType],\n    assistant_message_class: type[AssistantMessageType],\n    tool_message_class: type[ToolMessageType],\n    system_message_class: type[SystemMessageType],\n    instruct_request_class: type[InstructRequestType],\n):\n    r\"\"\"Initializes the normalizer with the appropriate message classes.\n\n    Args:\n       user_message_class: The class for user messages.\n       assistant_message_class: The class for assistant messages.\n       tool_message_class: The class for tool messages.\n       system_message_class: The class for system messages.\n       instruct_request_class: The class for instruct requests.\n    \"\"\"\n    self._user_message_class = user_message_class\n    self._assistant_message_class = assistant_message_class\n    self._tool_message_class = tool_message_class\n    self._instruct_request_class = instruct_request_class\n    # this is unused but makes creation nicer\n    self._system_message_class = system_message_class\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV13.normalizer","title":"<code>normalizer()</code>  <code>staticmethod</code>","text":"<p>Returns a normalizer for the default instruct request.</p> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>@staticmethod\ndef normalizer() -&gt; \"InstructRequestNormalizerV13\":\n    r\"\"\"Returns a normalizer for the default instruct request.\"\"\"\n    return InstructRequestNormalizerV13(\n        UserMessage,\n        AssistantMessage,\n        ToolMessage,\n        SystemMessage,\n        InstructRequest[UATS, Tool],\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV7","title":"<code>InstructRequestNormalizerV7(user_message_class, assistant_message_class, tool_message_class, system_message_class, instruct_request_class)</code>","text":"<p>               Bases: <code>InstructRequestNormalizer</code></p> <p>Normalizer for the v7 tokenizer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def __init__(\n    self,\n    user_message_class: type[UserMessageType],\n    assistant_message_class: type[AssistantMessageType],\n    tool_message_class: type[ToolMessageType],\n    system_message_class: type[SystemMessageType],\n    instruct_request_class: type[InstructRequestType],\n):\n    r\"\"\"Initializes the normalizer with the appropriate message classes.\n\n    Args:\n       user_message_class: The class for user messages.\n       assistant_message_class: The class for assistant messages.\n       tool_message_class: The class for tool messages.\n       system_message_class: The class for system messages.\n       instruct_request_class: The class for instruct requests.\n    \"\"\"\n    self._user_message_class = user_message_class\n    self._assistant_message_class = assistant_message_class\n    self._tool_message_class = tool_message_class\n    self._instruct_request_class = instruct_request_class\n    # this is unused but makes creation nicer\n    self._system_message_class = system_message_class\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV7.from_chat_completion_request","title":"<code>from_chat_completion_request(request)</code>","text":"<p>Converts a chat completion request to an instruct request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest[UATS]</code> <p>The chat completion request to convert.</p> required <p>Returns:</p> Type Description <code>InstructRequestType</code> <p>The converted instruct request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; request = ChatCompletionRequest(\n...     messages=[\n...         UserMessage(content=\"Hello\"),\n...         AssistantMessage(content=\"Hi\"),\n...     ],\n... )\n&gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n&gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def from_chat_completion_request(self, request: ChatCompletionRequest[UATS]) -&gt; InstructRequestType:  # type: ignore[type-var, misc]\n    r\"\"\"Converts a chat completion request to an instruct request.\n\n    Args:\n        request: The chat completion request to convert.\n\n    Returns:\n        The converted instruct request.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n        &gt;&gt;&gt; request = ChatCompletionRequest(\n        ...     messages=[\n        ...         UserMessage(content=\"Hello\"),\n        ...         AssistantMessage(content=\"Hi\"),\n        ...     ],\n        ... )\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n        &gt;&gt;&gt; instruct_request = normalizer.from_chat_completion_request(request)\n    \"\"\"\n    messages = self._aggregate_messages(request.messages)\n    return self._instruct_request_class(messages=messages, system_prompt=None, available_tools=request.tools)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.InstructRequestNormalizerV7.normalizer","title":"<code>normalizer()</code>  <code>staticmethod</code>","text":"<p>Returns a normalizer for the default instruct request</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>@staticmethod\ndef normalizer() -&gt; \"InstructRequestNormalizerV7\":\n    r\"\"\"Returns a normalizer for the default instruct request\n\n    Examples:\n        &gt;&gt;&gt; normalizer = InstructRequestNormalizerV7.normalizer()\n    \"\"\"\n    return InstructRequestNormalizerV7(\n        UserMessage,\n        AssistantMessage,\n        ToolMessage,\n        SystemMessage,\n        InstructRequest[UATS, Tool],\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/normalize/#mistral_common.protocol.instruct.normalize.normalizer_for_tokenizer_version","title":"<code>normalizer_for_tokenizer_version(version)</code>","text":"<p>Gets the appropriate normalizer for the given tokenizer version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>TokenizerVersion</code> <p>The tokenizer version to get the normalizer for.</p> required <p>Returns:</p> Type Description <code>InstructRequestNormalizer</code> <p>The appropriate normalizer for the given tokenizer version.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalizer = normalizer_for_tokenizer_version(TokenizerVersion.v1)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/normalize.py</code> <pre><code>def normalizer_for_tokenizer_version(version: TokenizerVersion) -&gt; InstructRequestNormalizer:\n    r\"\"\"Gets the appropriate normalizer for the given tokenizer version.\n\n    Args:\n        version: The tokenizer version to get the normalizer for.\n\n    Returns:\n        The appropriate normalizer for the given tokenizer version.\n\n    Examples:\n        &gt;&gt;&gt; normalizer = normalizer_for_tokenizer_version(TokenizerVersion.v1)\n    \"\"\"\n    if version in {TokenizerVersion.v1, TokenizerVersion.v2, TokenizerVersion.v3}:\n        return InstructRequestNormalizer.normalizer()\n    elif version in {TokenizerVersion.v7, TokenizerVersion.v11}:\n        return InstructRequestNormalizerV7.normalizer()\n    elif version == TokenizerVersion.v13:\n        return InstructRequestNormalizerV13.normalizer()\n    raise ValueError(f\"Unknown tokenizer version {version}\")\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/","title":"request","text":""},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request","title":"<code>mistral_common.protocol.instruct.request</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ChatCompletionRequest","title":"<code>ChatCompletionRequest(**data)</code>","text":"<p>               Bases: <code>BaseCompletionRequest</code>, <code>Generic[ChatMessageType]</code></p> <p>Request for a chat completion.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str | None</code> <p>The model to use for the chat completion.</p> <code>messages</code> <code>list[ChatMessageType]</code> <p>The messages to use for the chat completion.</p> <code>response_format</code> <code>ResponseFormat</code> <p>The format of the response.</p> <code>tools</code> <code>list[Tool] | None</code> <p>The tools to use for the chat completion.</p> <code>tool_choice</code> <code>ToolChoice</code> <p>The tool choice to use for the chat completion.</p> <code>truncate_for_context_length</code> <code>bool</code> <p>Whether to truncate the messages for the context length.</p> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import ToolTypes, Function\n&gt;&gt;&gt; request = ChatCompletionRequest(\n...     messages=[\n...         UserMessage(content=\"Hello!\"),\n...         AssistantMessage(content=\"Hi! How can I help you?\"),\n...     ],\n...     response_format=ResponseFormat(type=ResponseFormats.text),\n...     tools=[Tool(type=ToolTypes.function, function=Function(name=\"get_weather\", parameters={}))],\n...     tool_choice=ToolChoice.auto,\n...     truncate_for_context_length=True,\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ChatCompletionRequest.from_openai","title":"<code>from_openai(messages, tools=None, continue_final_message=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a chat completion request from the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[dict[str, str | list[dict[str, str | dict[str, Any]]]]]</code> <p>The messages in the OpenAI format.</p> required <code>tools</code> <code>list[dict[str, Any]] | None</code> <p>The tools in the OpenAI format.</p> <code>None</code> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the constructor. These should be the same as the fields of the request class or the OpenAI API equivalent.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletionRequest</code> <p>The chat completion request.</p> Source code in <code>src/mistral_common/protocol/instruct/request.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls,\n    messages: list[dict[str, str | list[dict[str, str | dict[str, Any]]]]],\n    tools: list[dict[str, Any]] | None = None,\n    continue_final_message: bool = False,\n    **kwargs: Any,\n) -&gt; \"ChatCompletionRequest\":\n    r\"\"\"Create a chat completion request from the OpenAI format.\n\n    Args:\n        messages: The messages in the OpenAI format.\n        tools: The tools in the OpenAI format.\n        continue_final_message: Whether to continue the final message.\n        **kwargs: Additional keyword arguments to pass to the constructor. These should be the same as the fields\n            of the request class or the OpenAI API equivalent.\n\n\n    Returns:\n        The chat completion request.\n    \"\"\"\n    if \"seed\" in kwargs and \"random_seed\" in kwargs:\n        raise ValueError(\"Cannot specify both `seed` and `random_seed`.\")\n\n    random_seed = kwargs.pop(\"seed\", None) or kwargs.pop(\"random_seed\", None)\n\n    _check_openai_fields_names(set(cls.model_fields.keys()), set(kwargs.keys()))\n\n    converted_messages: list[ChatMessage] = convert_openai_messages(messages)\n\n    converted_tools = convert_openai_tools(tools) if tools is not None else None\n\n    return cls(\n        messages=converted_messages,  # type: ignore[arg-type]\n        tools=converted_tools,\n        random_seed=random_seed,\n        continue_final_message=continue_final_message,\n        **kwargs,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ChatCompletionRequest.to_openai","title":"<code>to_openai(**kwargs)</code>","text":"<p>Convert the request messages and tools into the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Additional parameters to be added to the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>The request in the OpenAI format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n&gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n&gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], temperature=0.15)\n&gt;&gt;&gt; request.to_openai(stream=True)\n{'temperature': 0.15, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'stream': True}\n&gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], tools=[\n...     Tool(function=Function(\n...         name=\"get_current_weather\",\n...         description=\"Get the current weather in a given location\",\n...         parameters={\n...             \"type\": \"object\",\n...             \"properties\": {\n...                 \"location\": {\n...                     \"type\": \"string\",\n...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n...                 },\n...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...             },\n...             \"required\": [\"location\"],\n...         },\n...     ),\n... )])\n&gt;&gt;&gt; request.to_openai()\n{'temperature': 0.7, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/request.py</code> <pre><code>def to_openai(self, **kwargs: Any) -&gt; dict[str, list[dict[str, Any]]]:\n    r\"\"\"Convert the request messages and tools into the OpenAI format.\n\n    Args:\n        kwargs: Additional parameters to be added to the request.\n\n    Returns:\n        The request in the OpenAI format.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n        &gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], temperature=0.15)\n        &gt;&gt;&gt; request.to_openai(stream=True)\n        {'temperature': 0.15, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'stream': True}\n        &gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello, how are you?\")], tools=[\n        ...     Tool(function=Function(\n        ...         name=\"get_current_weather\",\n        ...         description=\"Get the current weather in a given location\",\n        ...         parameters={\n        ...             \"type\": \"object\",\n        ...             \"properties\": {\n        ...                 \"location\": {\n        ...                     \"type\": \"string\",\n        ...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n        ...                 },\n        ...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n        ...             },\n        ...             \"required\": [\"location\"],\n        ...         },\n        ...     ),\n        ... )])\n        &gt;&gt;&gt; request.to_openai()\n        {'temperature': 0.7, 'top_p': 1.0, 'response_format': {'type': 'text'}, 'tool_choice': 'auto', 'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n    \"\"\"  # noqa: E501\n\n    # Handle messages and tools separately.\n    openai_request: dict[str, Any] = self.model_dump(\n        exclude={\"messages\", \"tools\", \"truncate_for_context_length\"}, exclude_none=True\n    )\n\n    # Rename random_seed to seed.\n    seed = openai_request.pop(\"random_seed\", None)\n    if seed is not None:\n        openai_request[\"seed\"] = seed\n\n    if self.truncate_for_context_length:\n        raise NotImplementedError(\"Truncating for context length is not implemented for OpenAI requests.\")\n\n    for kwarg in kwargs:\n        # Check for duplicate keyword arguments.\n        if kwarg in openai_request:\n            raise ValueError(f\"Duplicate keyword argument: {kwarg}\")\n        # Check if kwarg should have been set in the request.\n        # This occurs when the field is different between the Mistral and OpenAI API.\n        elif kwarg in ChatCompletionRequest.model_fields:\n            raise ValueError(f\"Keyword argument {kwarg} is already set in the request.\")\n        # Check if kwarg is a valid OpenAI field name.\n        elif not _is_openai_field_name(kwarg):\n            raise ValueError(f\"Invalid keyword argument: {kwarg}, it should be an OpenAI field name.\")\n\n    openai_messages = []\n    for message in self.messages:\n        openai_messages.append(message.to_openai())\n\n    openai_request[\"messages\"] = openai_messages\n    if self.tools is not None:\n        openai_request[\"tools\"] = [tool.to_openai() for tool in self.tools]\n\n    openai_request.update(kwargs)\n\n    return openai_request\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.InstructRequest","title":"<code>InstructRequest(**data)</code>","text":"<p>               Bases: <code>MistralBase</code>, <code>Generic[ChatMessageType, ToolType]</code></p> <p>A valid Instruct request to be tokenized.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[ChatMessageType]</code> <p>The history of the conversation.</p> <code>system_prompt</code> <code>str | None</code> <p>The system prompt to be used for the conversation.</p> <code>available_tools</code> <code>list[ToolType] | None</code> <p>The tools available to the assistant.</p> <code>truncate_at_max_tokens</code> <code>int | None</code> <p>The maximum number of tokens to truncate the conversation at.</p> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, SystemMessage\n&gt;&gt;&gt; request = InstructRequest(\n...     messages=[UserMessage(content=\"Hello, how are you?\")], system_prompt=\"You are a helpful assistant.\"\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.InstructRequest.from_openai","title":"<code>from_openai(messages, tools=None, continue_final_message=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an instruct request from the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[dict[str, str | list[dict[str, str | dict[str, Any]]]]]</code> <p>The messages in the OpenAI format.</p> required <code>tools</code> <code>list[dict[str, Any]] | None</code> <p>The tools in the OpenAI format.</p> <code>None</code> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the constructor. These should be the same as the fields of the request class or the OpenAI API equivalent.</p> <code>{}</code> <p>Returns:</p> Type Description <code>InstructRequest</code> <p>The instruct request.</p> Source code in <code>src/mistral_common/protocol/instruct/request.py</code> <pre><code>@classmethod\ndef from_openai(\n    cls,\n    messages: list[dict[str, str | list[dict[str, str | dict[str, Any]]]]],\n    tools: list[dict[str, Any]] | None = None,\n    continue_final_message: bool = False,\n    **kwargs: Any,\n) -&gt; \"InstructRequest\":\n    r\"\"\"Create an instruct request from the OpenAI format.\n\n    Args:\n        messages: The messages in the OpenAI format.\n        tools: The tools in the OpenAI format.\n        continue_final_message: Whether to continue the final message.\n        **kwargs: Additional keyword arguments to pass to the constructor. These should be the same as the fields\n            of the request class or the OpenAI API equivalent.\n\n    Returns:\n        The instruct request.\n    \"\"\"\n    # Handle the case where the tools are passed as `available_tools`.\n    # This is to maintain compatibility with the OpenAI API.\n    if \"available_tools\" in kwargs:\n        if tools is None:\n            tools = kwargs.pop(\"available_tools\")\n        else:\n            raise ValueError(\"Cannot specify both `tools` and `available_tools`.\")\n\n    _check_openai_fields_names(set(cls.model_fields.keys()), set(kwargs.keys()))\n\n    converted_messages: list[ChatMessage] = convert_openai_messages(messages)\n\n    converted_tools = convert_openai_tools(tools) if tools is not None else None\n\n    return cls(\n        messages=converted_messages,  # type: ignore[arg-type]\n        available_tools=converted_tools,  # type: ignore[arg-type]\n        continue_final_message=continue_final_message,\n        **kwargs,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.InstructRequest.to_openai","title":"<code>to_openai(**kwargs)</code>","text":"<p>Convert the request messages and tools into the OpenAI format.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Additional parameters to be added to the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>The request in the OpenAI format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n&gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n&gt;&gt;&gt; request = InstructRequest(messages=[UserMessage(content=\"Hello, how are you?\")])\n&gt;&gt;&gt; request.to_openai(temperature=0.15, stream=True)\n{'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'temperature': 0.15, 'stream': True}\n&gt;&gt;&gt; request = InstructRequest(\n...     messages=[UserMessage(content=\"Hello, how are you?\")],\n...     available_tools=[\n...     Tool(function=Function(\n...         name=\"get_current_weather\",\n...         description=\"Get the current weather in a given location\",\n...         parameters={\n...             \"type\": \"object\",\n...             \"properties\": {\n...                 \"location\": {\n...                     \"type\": \"string\",\n...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n...                 },\n...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...             },\n...             \"required\": [\"location\"],\n...         },\n...     ),\n... )])\n&gt;&gt;&gt; request.to_openai()\n{'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/request.py</code> <pre><code>def to_openai(self, **kwargs: Any) -&gt; dict[str, list[dict[str, Any]]]:\n    r\"\"\"Convert the request messages and tools into the OpenAI format.\n\n    Args:\n        kwargs: Additional parameters to be added to the request.\n\n    Returns:\n        The request in the OpenAI format.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.tool_calls import Tool, Function\n        &gt;&gt;&gt; request = InstructRequest(messages=[UserMessage(content=\"Hello, how are you?\")])\n        &gt;&gt;&gt; request.to_openai(temperature=0.15, stream=True)\n        {'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'temperature': 0.15, 'stream': True}\n        &gt;&gt;&gt; request = InstructRequest(\n        ...     messages=[UserMessage(content=\"Hello, how are you?\")],\n        ...     available_tools=[\n        ...     Tool(function=Function(\n        ...         name=\"get_current_weather\",\n        ...         description=\"Get the current weather in a given location\",\n        ...         parameters={\n        ...             \"type\": \"object\",\n        ...             \"properties\": {\n        ...                 \"location\": {\n        ...                     \"type\": \"string\",\n        ...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n        ...                 },\n        ...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n        ...             },\n        ...             \"required\": [\"location\"],\n        ...         },\n        ...     ),\n        ... )])\n        &gt;&gt;&gt; request.to_openai()\n        {'continue_final_message': False, 'messages': [{'role': 'user', 'content': 'Hello, how are you?'}], 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}\n    \"\"\"  # noqa: E501\n\n    # Handle messages, tools, and truncate_at_max_tokens separately.\n    openai_request: dict[str, Any] = self.model_dump(\n        exclude={\"messages\", \"available_tools\", \"truncate_at_max_tokens\"}, exclude_none=True\n    )\n\n    for kwarg in kwargs:\n        # Check for duplicate keyword arguments.\n        if kwarg in openai_request:\n            raise ValueError(f\"Duplicate keyword argument: {kwarg}\")\n        # Check if kwarg should have been set in the request.\n        # This occurs when the field is different between the Mistral and OpenAI API.\n        elif kwarg in InstructRequest.model_fields:\n            raise ValueError(f\"Keyword argument {kwarg} is already set in the request.\")\n        # Check if the keyword argument is a valid OpenAI field name.\n        elif not _is_openai_field_name(kwarg):\n            raise ValueError(f\"Invalid keyword argument: {kwarg}, it should be an OpenAI field name.\")\n\n    openai_messages: list[dict[str, Any]] = []\n    if self.system_prompt is not None:\n        openai_messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n\n    for message in self.messages:\n        openai_messages.append(message.to_openai())\n\n    openai_request[\"messages\"] = openai_messages\n    if self.available_tools is not None:\n        # Rename available_tools to tools\n        openai_request[\"tools\"] = [tool.to_openai() for tool in self.available_tools]\n\n    if self.truncate_at_max_tokens is not None:\n        raise NotImplementedError(\"Truncating at max tokens is not implemented for OpenAI requests.\")\n\n    openai_request.update(kwargs)\n\n    return openai_request\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ResponseFormat","title":"<code>ResponseFormat(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>The format of the response.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>ResponseFormats</code> <p>The type of the response.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response_format = ResponseFormat(type=ResponseFormats.text)\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/request/#mistral_common.protocol.instruct.request.ResponseFormats","title":"<code>ResponseFormats</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of the different formats of an instruct response.</p> <p>Attributes:</p> Name Type Description <code>text</code> <p>The response is a plain text.</p> <code>json</code> <p>The response is a JSON object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; response_format = ResponseFormats.text\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/","title":"tool_calls","text":""},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls","title":"<code>mistral_common.protocol.instruct.tool_calls</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.Function","title":"<code>Function(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Function definition for tools.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function.</p> <code>description</code> <code>str</code> <p>A description of what the function does.</p> <code>parameters</code> <code>dict[str, Any]</code> <p>The parameters the functions accepts, described as a JSON Schema object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; function = Function(\n...     name=\"get_current_weather\",\n...     description=\"Get the current weather in a given location\",\n...     parameters={\n...         \"type\": \"object\",\n...         \"properties\": {\n...             \"location\": {\n...                 \"type\": \"string\",\n...                 \"description\": \"The city and state, e.g. San Francisco, CA\",\n...             },\n...             \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...         },\n...         \"required\": [\"location\"],\n...     },\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.FunctionCall","title":"<code>FunctionCall(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Function call.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function to call.</p> <code>arguments</code> <code>str</code> <p>The arguments to pass to the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; function_call = FunctionCall(\n...     name=\"get_current_weather\",\n...     arguments={\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"},\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.FunctionCall.validate_arguments","title":"<code>validate_arguments(v)</code>","text":"<p>Convert arguments to a JSON string if they are a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>str | dict[str, Any]</code> <p>The arguments to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The arguments as a JSON string.</p> Source code in <code>src/mistral_common/protocol/instruct/tool_calls.py</code> <pre><code>@field_validator(\"arguments\", mode=\"before\")\ndef validate_arguments(cls, v: str | dict[str, Any]) -&gt; str:\n    \"\"\"Convert arguments to a JSON string if they are a dictionary.\n\n    Args:\n        v: The arguments to validate.\n\n    Returns:\n        The arguments as a JSON string.\n    \"\"\"\n    if isinstance(v, dict):\n        return json.dumps(v)\n    return v\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.Tool","title":"<code>Tool(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Tool definition.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>ToolTypes</code> <p>The type of the tool.</p> <code>function</code> <code>Function</code> <p>The function definition.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool = Tool(\n...     function=Function(\n...         name=\"get_current_weather\",\n...         description=\"Get the current weather in a given location\",\n...         parameters={\n...             \"type\": \"object\",\n...             \"properties\": {\n...                 \"location\": {\n...                     \"type\": \"string\",\n...                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n...                 },\n...                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n...             },\n...             \"required\": [\"location\"],\n...         },\n...     ),\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.ToolCall","title":"<code>ToolCall(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>Tool call.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The ID of the tool call. Required for V3+ tokenization</p> <code>type</code> <code>ToolTypes</code> <p>The type of the tool call.</p> <code>function</code> <code>FunctionCall</code> <p>The function call.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool_call = ToolCall(\n...     id=\"call_abc123\",\n...     function=FunctionCall(\n...         name=\"get_current_weather\",\n...         arguments={\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"},\n...     ),\n... )\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.ToolChoice","title":"<code>ToolChoice</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of tool choice types.</p> <p>Attributes:</p> Name Type Description <code>auto</code> <p>Automatically choose the tool.</p> <code>none</code> <p>Do not use any tools.</p> <code>any</code> <p>Use any tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool_choice = ToolChoice.auto\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/tool_calls/#mistral_common.protocol.instruct.tool_calls.ToolTypes","title":"<code>ToolTypes</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of tool types.</p> <p>Attributes:</p> Name Type Description <code>function</code> <p>A function tool.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tool_type = ToolTypes.function\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/","title":"validator","text":""},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator","title":"<code>mistral_common.protocol.instruct.validator</code>","text":""},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidator","title":"<code>MistralRequestValidator(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>Generic[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType]</code></p> <p>Validator for Mistral requests.</p> <p>This class validates the structure and content of Mistral requests.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; validator = MistralRequestValidator()\n&gt;&gt;&gt; messages = [UserMessage(content=\"Hello how are you ?\")]\n&gt;&gt;&gt; validator.validate_messages(messages, False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>ValidationMode</code> <p>The validation mode. Defaults to ValidationMode.test.</p> <code>test</code> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidator.validate_messages","title":"<code>validate_messages(messages, continue_final_message)</code>","text":"<p>Validates the list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[UATS]</code> <p>The list of messages to validate.</p> required <code>continue_final_message</code> <code>bool</code> <p>Whether to continue the final message.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n&gt;&gt;&gt; validator = MistralRequestValidator()\n&gt;&gt;&gt; messages = [AssistantMessage(content=\"Hi\"), UserMessage(content=\"Hello\")]\n&gt;&gt;&gt; validator.validate_messages(messages, False)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def validate_messages(self, messages: list[UATS], continue_final_message: bool) -&gt; None:\n    r\"\"\"Validates the list of messages.\n\n    Args:\n        messages: The list of messages to validate.\n        continue_final_message: Whether to continue the final message.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n        &gt;&gt;&gt; validator = MistralRequestValidator()\n        &gt;&gt;&gt; messages = [AssistantMessage(content=\"Hi\"), UserMessage(content=\"Hello\")]\n        &gt;&gt;&gt; validator.validate_messages(messages, False)\n    \"\"\"\n    self._validate_message_list_structure(messages, continue_final_message=continue_final_message)\n    self._validate_message_list_content(messages)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidator.validate_request","title":"<code>validate_request(request)</code>","text":"<p>Validates the request</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest</code> <p>The request to validate.</p> required <p>Returns:</p> Type Description <code>ChatCompletionRequest[UATS]</code> <p>The validated request.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n&gt;&gt;&gt; validator = MistralRequestValidator()\n&gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello\")])\n&gt;&gt;&gt; validated_request = validator.validate_request(request)\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def validate_request(self, request: ChatCompletionRequest) -&gt; ChatCompletionRequest[UATS]:\n    r\"\"\"Validates the request\n\n    Args:\n        request: The request to validate.\n\n    Returns:\n        The validated request.\n\n    Examples:\n        &gt;&gt;&gt; from mistral_common.protocol.instruct.messages import UserMessage\n        &gt;&gt;&gt; validator = MistralRequestValidator()\n        &gt;&gt;&gt; request = ChatCompletionRequest(messages=[UserMessage(content=\"Hello\")])\n        &gt;&gt;&gt; validated_request = validator.validate_request(request)\n    \"\"\"\n\n    if self._mode == ValidationMode.serving:\n        if request.model is None:\n            raise InvalidRequestException(\"Model name parameter is required for serving mode\")\n\n    # Validate the messages\n    self.validate_messages(request.messages, continue_final_message=request.continue_final_message)\n\n    # Validate the tools\n    self._validate_tools(request.tools or [])\n\n    return request\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidatorV13","title":"<code>MistralRequestValidatorV13(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>MistralRequestValidatorV5</code></p> <p>Validator for v13 Mistral requests.</p> <p>This validator extends v5 functionality by: - Adding stricter tool call ID validation: they should be distinct and called. - Allowing system prompts with audio chunks</p> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidatorV3","title":"<code>MistralRequestValidatorV3(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>MistralRequestValidator</code></p> <p>Validator for v3 Mistral requests.</p> <p>This validator adds additional validation for tool call IDs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validator = MistralRequestValidatorV3()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.MistralRequestValidatorV5","title":"<code>MistralRequestValidatorV5(mode=ValidationMode.test)</code>","text":"<p>               Bases: <code>MistralRequestValidatorV3</code></p> <p>Validator for v5 Mistral requests.</p> <p>This validator allows for both tool calls and content in the assistant message.</p> Note <p>For requests containing audio, this validator ensures that no system prompt is present.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validator = MistralRequestValidatorV5()\n</code></pre> Source code in <code>src/mistral_common/protocol/instruct/validator.py</code> <pre><code>def __init__(self, mode: ValidationMode = ValidationMode.test):\n    r\"\"\"Initializes the `MistralRequestValidator`.\n\n    Args:\n        mode: The validation mode. Defaults to ValidationMode.test.\n    \"\"\"\n    self._mode = mode\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/instruct/validator/#mistral_common.protocol.instruct.validator.ValidationMode","title":"<code>ValidationMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the validation mode.</p> <p>Attributes:</p> Name Type Description <code>serving</code> <p>The serving mode.</p> <code>finetuning</code> <p>The finetuning mode.</p> <code>test</code> <p>The test mode.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mode = ValidationMode.serving\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/transcription/request/","title":"request","text":""},{"location":"code_reference/mistral_common/protocol/transcription/request/#mistral_common.protocol.transcription.request","title":"<code>mistral_common.protocol.transcription.request</code>","text":""},{"location":"code_reference/mistral_common/protocol/transcription/request/#mistral_common.protocol.transcription.request.TranscriptionRequest","title":"<code>TranscriptionRequest(**data)</code>","text":"<p>               Bases: <code>BaseCompletionRequest</code></p> <p>A class representing a request for audio transcription.</p> <p>This class handles the conversion of audio data into a format suitable for transcription using the OpenAI API. It includes methods to convert the request to and from the OpenAI format.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str | None</code> <p>An optional identifier for the transcription request.</p> <code>model</code> <code>str | None</code> <p>The model to be used for transcription.</p> <code>audio</code> <code>RawAudio</code> <p>The audio data to be transcribed.</p> <code>language</code> <code>LanguageAlpha2 | None</code> <p>The language of the input audio in ISO-639-1 format (optional).</p> <code>strict_audio_validation</code> <code>bool</code> <p>A flag indicating whether to perform strict validation of the audio data.</p> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/transcription/request/#mistral_common.protocol.transcription.request.TranscriptionRequest.from_openai","title":"<code>from_openai(openai_request, strict=False)</code>  <code>classmethod</code>","text":"<p>Create a TranscriptionRequest instance from an OpenAI request dictionary.</p> <p>This method converts an OpenAI request dictionary into a TranscriptionRequest instance, handling the conversion of audio data and other parameters.</p> <p>Parameters:</p> Name Type Description Default <code>openai_request</code> <code>dict[str, Any]</code> <p>The OpenAI request dictionary.</p> required <code>strict</code> <code>bool</code> <p>A flag indicating whether to perform strict validation of the audio data.</p> <code>False</code> <p>Returns:</p> Type Description <code>TranscriptionRequest</code> <p>An instance of TranscriptionRequest.</p> Source code in <code>src/mistral_common/protocol/transcription/request.py</code> <pre><code>@classmethod\ndef from_openai(cls, openai_request: dict[str, Any], strict: bool = False) -&gt; \"TranscriptionRequest\":\n    r\"\"\"Create a TranscriptionRequest instance from an OpenAI request dictionary.\n\n    This method converts an OpenAI request dictionary into a TranscriptionRequest instance,\n    handling the conversion of audio data and other parameters.\n\n    Args:\n        openai_request: The OpenAI request dictionary.\n        strict: A flag indicating whether to perform strict validation of the audio data.\n\n    Returns:\n       An instance of TranscriptionRequest.\n    \"\"\"\n    file = openai_request.get(\"file\")\n    seed = openai_request.get(\"seed\")\n    converted_dict = {\n        k: v\n        for k, v in openai_request.items()\n        if (k in cls.model_fields and not (v is None and k in [\"temperature\", \"top_p\"]))\n    }\n\n    assert file is not None, file\n    if isinstance(file, io.BytesIO):\n        audio_bytes = file.getvalue()\n    else:\n        # for example if file is UploadFile, this should work\n        audio_bytes = file.file.read()\n\n    audio = Audio.from_bytes(audio_bytes, strict=strict)\n    audio_str = audio.to_base64(audio.format)\n    raw_audio = RawAudio(data=audio_str, format=audio.format)\n\n    converted_dict[\"audio\"] = raw_audio\n    converted_dict[\"random_seed\"] = seed\n    return cls(**converted_dict)\n</code></pre>"},{"location":"code_reference/mistral_common/protocol/transcription/request/#mistral_common.protocol.transcription.request.TranscriptionRequest.to_openai","title":"<code>to_openai(exclude=(), **kwargs)</code>","text":"<p>Convert the transcription request into the OpenAI format.</p> <p>This method prepares the transcription request data for compatibility with the OpenAI API. It handles the conversion of audio data and additional parameters into the required format.</p> <p>Parameters:</p> Name Type Description Default <code>exclude</code> <code>tuple</code> <p>Fields to exclude from the conversion.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional parameters to be added to the request.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>The request in the OpenAI format.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the required soundfile library is not installed.</p> Source code in <code>src/mistral_common/protocol/transcription/request.py</code> <pre><code>def to_openai(self, exclude: tuple = (), **kwargs: Any) -&gt; dict[str, list[dict[str, Any]]]:\n    r\"\"\"Convert the transcription request into the OpenAI format.\n\n    This method prepares the transcription request data for compatibility with the OpenAI API.\n    It handles the conversion of audio data and additional parameters into the required format.\n\n    Args:\n        exclude: Fields to exclude from the conversion.\n        kwargs: Additional parameters to be added to the request.\n\n    Returns:\n        The request in the OpenAI format.\n\n    Raises:\n        ImportError: If the required soundfile library is not installed.\n    \"\"\"\n    openai_request: dict[str, Any] = self.model_dump(exclude={\"audio\"})\n\n    assert_soundfile_installed()\n\n    if isinstance(self.audio.data, bytes):\n        buffer = io.BytesIO(self.audio.data)\n    else:\n        assert isinstance(self.audio.data, str)\n        audio = Audio.from_base64(self.audio.data)\n\n        buffer = io.BytesIO()\n        sf.write(buffer, audio.audio_array, audio.sampling_rate, format=audio.format)\n        # reset cursor to beginning\n        buffer.seek(0)\n\n    openai_request[\"file\"] = buffer\n    openai_request[\"seed\"] = openai_request.pop(\"random_seed\")\n    openai_request.update(kwargs)\n\n    # remove mistral-specific\n    default_exclude = (\"id\", \"max_tokens\", \"strict_audio_validation\", \"streaming\")\n    default_exclude += exclude\n    for k in default_exclude:\n        openai_request.pop(k, None)\n\n    return openai_request\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/instruct/request/","title":"request","text":""},{"location":"code_reference/mistral_common/tokens/instruct/request/#mistral_common.tokens.instruct.request","title":"<code>mistral_common.tokens.instruct.request</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/","title":"audio","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio","title":"<code>mistral_common.tokens.tokenizers.audio</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioConfig","title":"<code>AudioConfig(sampling_rate, frame_rate, encoding_config, chunk_length_s=None, transcription_format=TranscriptionFormat.INSTRUCT, transcription_delay_ms=None, streaming_look_ahead_ms=None, streaming_look_back_ms=None, streaming_n_left_pad_tokens=None)</code>  <code>dataclass</code>","text":"<p>Configuration for audio processing.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <code>int</code> <p>Sampling rate of the audio.</p> <code>frame_rate</code> <code>float</code> <p>Number of frames per second accepted by the tokenizer model.</p> <code>encoding_config</code> <code>AudioSpectrogramConfig</code> <p>Configuration for audio spectrogram.</p> <code>chunk_length_s</code> <code>float | None</code> <p>Whether to pad an audio into multiples of chunk_length_s seconds (optional).</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioConfig.audio_length_per_tok","title":"<code>audio_length_per_tok</code>  <code>property</code>","text":"<p>Calculate the length of audio per token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioConfig.chunk_frames","title":"<code>chunk_frames</code>  <code>property</code>","text":"<p>Calculate the number of frames per chunk.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder","title":"<code>AudioEncoder(audio_config, special_ids)</code>","text":"<p>Encodes audio chunks into a format suitable for further processing.</p> <p>Attributes:</p> Name Type Description <code>audio_config</code> <p>Configuration for audio processing.</p> <code>encoding_config</code> <p>Configuration for audio spectrogram.</p> <code>special_ids</code> <p>Special tokens for audio encoding.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def __init__(self, audio_config: AudioConfig, special_ids: SpecialAudioIDs) -&gt; None:\n    self.audio_config = audio_config\n    self.encoding_config = audio_config.encoding_config\n    self.special_ids = special_ids\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.audio_token","title":"<code>audio_token</code>  <code>property</code>","text":"<p>Get the audio token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.begin_audio_token","title":"<code>begin_audio_token</code>  <code>property</code>","text":"<p>Get the begin audio token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.streaming_pad","title":"<code>streaming_pad</code>  <code>property</code>","text":"<p>Get the streaming pad token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.__call__","title":"<code>__call__(content)</code>","text":"<p>Call the encoder on an audio chunk or URL chunk.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>AudioChunk | AudioURLChunk</code> <p>Audio or URL chunk to encode.</p> required <p>Returns:</p> Type Description <code>AudioEncoding</code> <p>Encoded audio data and tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def __call__(self, content: AudioChunk | AudioURLChunk) -&gt; AudioEncoding:\n    r\"\"\"Call the encoder on an audio chunk or URL chunk.\n\n    Args:\n        content: Audio or URL chunk to encode.\n\n    Returns:\n        Encoded audio data and tokens.\n    \"\"\"\n    if isinstance(content, AudioURLChunk):\n        return self._encode_audio_url_chunk(content)\n    elif isinstance(content, AudioChunk):\n        return self._encode_audio_chunk(content)\n    else:\n        raise ValueError(f\"Unsupported content type: {type(content)}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.encode_audio","title":"<code>encode_audio(audio, transcription_delay_ms=None)</code>","text":"<p>Encode an audio optionally with transcription delay.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def encode_audio(self, audio: Audio, transcription_delay_ms: float | None = None) -&gt; AudioEncoding:\n    r\"\"\"Encode an audio optionally with transcription delay.\"\"\"\n    audio.resample(self.audio_config.sampling_rate)\n    audio.audio_array = self.pad(audio.audio_array, self.audio_config.sampling_rate, transcription_delay_ms)\n\n    if self.audio_config.transcription_format == TranscriptionFormat.STREAMING:\n        tokens = self.encode_streaming_tokens(transcription_delay_ms)\n    else:\n        tokens = self._encode_audio_tokens(audio.audio_array.shape[0])\n\n    return AudioEncoding(\n        tokens=tokens,\n        audio=audio,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.encode_streaming_tokens","title":"<code>encode_streaming_tokens(transcription_delay_ms=None)</code>","text":"<p>Encode the streaming tokens given a transcription delay.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def encode_streaming_tokens(self, transcription_delay_ms: float | None = None) -&gt; list[int]:\n    r\"\"\"Encode the streaming tokens given a transcription delay.\"\"\"\n    assert isinstance(self.audio_config.encoding_config, AudioSpectrogramConfig), (\n        f\"Audio encoder must be spectrogram encoder, got {self.audio_config.encoding_config=}\"\n    )\n    assert self.audio_config.transcription_delay_ms is not None\n\n    # streaming pad tokens consist of silence we pad on left + delay tokens\n    stream_pad_prefix_len = self.audio_config.n_left_pad_tokens + self.audio_config.get_num_delay_tokens(\n        transcription_delay_ms\n    )\n    tokens = [self.streaming_pad] * stream_pad_prefix_len\n\n    return tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.get_padding_audio","title":"<code>get_padding_audio(transcription_delay_ms=None)</code>","text":"<p>Gets left and right padding for realtime audio models.</p> <p>Parameters:</p> Name Type Description Default <code>transcription_delay_ms</code> <code>optional</code> <p>Delay in milliseconds for transcription.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Audio, Audio]</code> <p>Tuple of left and right padding for realtime audio models.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def get_padding_audio(self, transcription_delay_ms: float | None = None) -&gt; tuple[Audio, Audio]:\n    r\"\"\"Gets left and right padding for realtime audio models.\n\n    Args:\n        transcription_delay_ms (optional): Delay in milliseconds for transcription.\n\n    Returns:\n        Tuple of left and right padding for realtime audio models.\n    \"\"\"\n\n    left_pad, right_pad = self._get_streaming_pad(0, transcription_delay_ms)\n    left_pad_audio = Audio(\n        audio_array=np.zeros(left_pad, dtype=np.float32),\n        sampling_rate=self.audio_config.sampling_rate,\n        format=\"wav\",\n    )\n    right_pad_audio = Audio(\n        audio_array=np.zeros(right_pad, dtype=np.float32),\n        sampling_rate=self.audio_config.sampling_rate,\n        format=\"wav\",\n    )\n    return left_pad_audio, right_pad_audio\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.next_multiple_of_chunk_frames","title":"<code>next_multiple_of_chunk_frames(audio_array_len, sampling_rate)</code>","text":"<p>Calculate the next multiple of chunk frames.</p> <p>Parameters:</p> Name Type Description Default <code>audio_array_len</code> <code>int</code> <p>Length of the audio array.</p> required <code>sampling_rate</code> <code>int</code> <p>Sampling rate of the audio.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The next multiple of chunk frames.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def next_multiple_of_chunk_frames(self, audio_array_len: int, sampling_rate: int) -&gt; int:\n    r\"\"\"Calculate the next multiple of chunk frames.\n\n    Args:\n        audio_array_len: Length of the audio array.\n        sampling_rate: Sampling rate of the audio.\n\n    Returns:\n        The next multiple of chunk frames.\n    \"\"\"\n    assert sampling_rate == self.audio_config.sampling_rate, (\n        f\"Expected {sampling_rate=} to be {self.audio_config.sampling_rate=}\"\n    )\n    assert self.audio_config.chunk_length_s is not None, (\n        f\"Can't call next_multiple_of_chunk_frames if {self.audio_config.chunk_length_s=}.\"\n    )\n\n    return math.ceil(audio_array_len / self.audio_config.chunk_frames) * self.audio_config.chunk_frames\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoder.pad","title":"<code>pad(audio_array, sampling_rate, transcription_delay_ms=None, **kwargs)</code>","text":"<p>Pad the audio array to the desired length.</p> <p>Parameters:</p> Name Type Description Default <code>audio_array</code> <code>ndarray</code> <p>Audio data as a numpy array.</p> required <code>sampling_rate</code> <code>int</code> <p>Sampling rate of the audio.</p> required <code>transcription_delay_ms</code> <code>optional</code> <p>Delay in milliseconds for transcription.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Padded audio array.</p> Source code in <code>src/mistral_common/tokens/tokenizers/audio.py</code> <pre><code>def pad(\n    self,\n    audio_array: np.ndarray,\n    sampling_rate: int,\n    transcription_delay_ms: float | None = None,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    r\"\"\"Pad the audio array to the desired length.\n\n    Args:\n        audio_array: Audio data as a numpy array.\n        sampling_rate: Sampling rate of the audio.\n        transcription_delay_ms (optional): Delay in milliseconds for transcription.\n\n    Returns:\n        Padded audio array.\n    \"\"\"\n    # TODO(Patrick) - remove **kwargs as it's just there to swallow deprecated\n    # keyword args from voxtral_realtime in vLLM. It was\n    # relevant for the release. Remove in mistral_common version 1.11\n    if self.audio_config.chunk_length_s:\n        next_multiple_of_chunk_frames = self.next_multiple_of_chunk_frames(audio_array.shape[-1], sampling_rate)\n        audio_array = np.pad(audio_array, (0, next_multiple_of_chunk_frames - audio_array.shape[-1]))\n    elif self.audio_config.is_streaming:\n        left_pad, right_pad = self._get_streaming_pad(audio_array.shape[-1], transcription_delay_ms)\n        # we pad both left &amp; right as this leads to better performance\n        audio_array = np.pad(audio_array, (left_pad, right_pad))\n    elif (\n        isinstance(self.encoding_config, AudioSpectrogramConfig)\n        and audio_array.shape[-1] &lt; self.encoding_config.window_size\n    ):\n        # minimum length for audios is at least one spectrogram frame\n        audio_array = np.pad(audio_array, (0, self.encoding_config.window_size - audio_array.shape[-1]))\n\n    return audio_array\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioEncoding","title":"<code>AudioEncoding(tokens, audio)</code>  <code>dataclass</code>","text":"<p>Encapsulates the tokens and audio data for an audio chunk.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>list[int]</code> <p>Text tokens corresponding to this audio chunk.</p> <code>audio</code> <code>Audio</code> <p>Original audio waveform data.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.AudioSpectrogramConfig","title":"<code>AudioSpectrogramConfig(num_mel_bins, hop_length, window_size)</code>  <code>dataclass</code>","text":"<p>Configuration for generating an audio spectrogram.</p> <p>Attributes:</p> Name Type Description <code>num_mel_bins</code> <code>int</code> <p>Number of mel bins, typically 80 or 128.</p> <code>hop_length</code> <code>int</code> <p>Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients, typically 160.</p> <code>window_size</code> <code>int</code> <p>Window size of the Fourier transform, typically 400.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.SpecialAudioIDs","title":"<code>SpecialAudioIDs(audio, begin_audio, streaming_pad)</code>  <code>dataclass</code>","text":"<p>Special text tokens corresponding to audio token sequence.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>int | None</code> <p>Token representing audio.</p> <code>begin_audio</code> <code>int | None</code> <p>Token representing the beginning of audio.</p> <code>streaming_pad</code> <code>int | None</code> <p>Token representing streaming pad of audio. Only relevant for steaming models.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/audio/#mistral_common.tokens.tokenizers.audio.TranscriptionFormat","title":"<code>TranscriptionFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Transcription format.</p> <p>Should be set by the tokenizer for correct encoding.</p> <p>Attributes: - INSTRUCT: The instruct format. - STREAMING: The streaming format.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/","title":"base","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base","title":"<code>mistral_common.tokens.tokenizers.base</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer","title":"<code>InstructTokenizer(tokenizer, image_encoder, audio_encoder)</code>","text":"<p>               Bases: <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Base class for instruct tokenizers.</p> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> <code>image_encoder</code> <code>ImageEncoder | None</code> <p>The image encoder to use if any.</p> <code>audio_encoder</code> <code>AudioEncoder | None</code> <p>The audio encoder to use if any.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>ImageEncoder | None</code> <p>The image encoder to use if any.</p> required <code>audio_encoder</code> <code>AudioEncoder | None</code> <p>The audio encoder to use if any.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>def __init__(\n    self, tokenizer: Tokenizer, image_encoder: ImageEncoder | None, audio_encoder: AudioEncoder | None\n) -&gt; None:\n    r\"\"\"Initialize the instruct tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use if any.\n        audio_encoder: The audio encoder to use if any.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.version","title":"<code>version</code>  <code>property</code>","text":"<p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>  <code>abstractmethod</code>","text":"<p>Convert token ids to string</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The token ids to decode.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy to use for special tokens. Passing <code>None</code> will default to <code>self._special_token_policy</code> for Tekkenizer and <code>SpecialTokenPolicy.IGNORE</code> for SentencePieceTokenizer. Note that passing <code>None</code> will be deprecated and <code>special_token_policy</code> will default to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef decode(self, tokens: list[int], special_token_policy: SpecialTokenPolicy | None = None) -&gt; str:\n    r\"\"\"Convert token ids to string\n\n    Args:\n        tokens: The token ids to decode.\n        special_token_policy: The policy to use for special tokens.\n            Passing `None` will default to `self._special_token_policy` for\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer] and `SpecialTokenPolicy.IGNORE`\n            for [SentencePieceTokenizer][mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer].\n            Note that passing `None` will be deprecated and `special_token_policy` will default to\n            `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_fim","title":"<code>encode_fim(request)</code>  <code>abstractmethod</code>","text":"<p>FIM request to Tokenized object</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FIMRequestType</code> <p>The FIM request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The tokenized FIM request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_fim(self, request: FIMRequestType) -&gt; TokenizedType:\n    r\"\"\"FIM request to Tokenized object\n\n    Args:\n        request: The FIM request to encode.\n\n    Returns:\n        The tokenized FIM request.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_instruct","title":"<code>encode_instruct(request)</code>  <code>abstractmethod</code>","text":"<p>Instruct request to Tokenized object</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstructRequestType</code> <p>The instruct request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The tokenized instruct request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_instruct(self, request: InstructRequestType) -&gt; TokenizedType:\n    r\"\"\"Instruct request to Tokenized object\n\n    Args:\n        request: The instruct request to encode.\n\n    Returns:\n        The tokenized instruct request.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_transcription","title":"<code>encode_transcription(request)</code>  <code>abstractmethod</code>","text":"<p>Encodes an audio transcription request into a tokenized format.</p> <p>This method processes a transcription request containing audio data, encodes the user message, and returns the tokenized output.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TranscriptionRequest</code> <p>The transcription request object containing the audio data to be encoded.</p> required <p>Returns:</p> Type Description <code>Tokenized</code> <p>The tokenized representation of the audio data, including processed audio and tokens</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_transcription(self, request: TranscriptionRequest) -&gt; TokenizedType:\n    r\"\"\"\n    Encodes an audio transcription request into a tokenized format.\n\n    This method processes a transcription request containing audio data,\n    encodes the user message, and returns the tokenized output.\n\n    Args:\n        request: The transcription request object containing\n            the audio data to be encoded.\n\n    Returns:\n        Tokenized: The tokenized representation of the audio data, including processed audio and tokens\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>  <code>abstractmethod</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | list[UserContentChunk]</code> <p>The user content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the content is the last one.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_user_content(\n    self,\n    content: str | list[UserContentChunk],\n    is_last: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The user content to encode.\n        is_last: Whether the content is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and images.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.InstructTokenizer.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>  <code>abstractmethod</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The user message to encode.</p> required <code>available_tools</code> <code>list[Tool] | None</code> <p>The available tools.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>is_first</code> <code>bool</code> <p>Whether the message is the first one.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: list[Tool] | None,\n    is_last: bool,\n    is_first: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The user message to encode.\n        available_tools: The available tools.\n        is_last: Whether the message is the last one.\n        is_first: Whether the message is the first one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and images.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.SpecialTokenPolicy","title":"<code>SpecialTokenPolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>What to do with special tokens when encoding/decoding.</p> <p>Attributes:</p> Name Type Description <code>IGNORE</code> <p>Ignore special tokens.</p> <code>KEEP</code> <p>Keep special tokens.</p> <code>RAISE</code> <p>Raise an error if special tokens are found.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.SpecialTokens","title":"<code>SpecialTokens</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of special tokens used in the tokenizer.</p> <p>Attributes:</p> Name Type Description <code>unk</code> <p>The unknown token.</p> <code>bos</code> <p>The beginning of string token.</p> <code>eos</code> <p>The end of string token.</p> <code>begin_inst</code> <p>The beginning of instruction token.</p> <code>end_inst</code> <p>The end of instruction token.</p> <code>begin_tools</code> <p>The beginning of tools token.</p> <code>end_tools</code> <p>The end of tools token.</p> <code>begin_tool_results</code> <p>The beginning of tool results token.</p> <code>end_tool_results</code> <p>The end of tool results token.</p> <code>tool_calls</code> <p>The tool calls token.</p> <code>img</code> <p>The image token.</p> <code>pad</code> <p>The pad token.</p> <code>img_break</code> <p>The image break token.</p> <code>img_end</code> <p>The image end token.</p> <code>prefix</code> <p>The prefix token for FIM.</p> <code>middle</code> <p>The middle token for FIM.</p> <code>suffix</code> <p>The suffix token for FIM.</p> <code>begin_system</code> <p>The beginning of system prompt token.</p> <code>end_system</code> <p>The end of system prompt token.</p> <code>begin_tool_content</code> <p>The beginning of tool content token.</p> <code>args</code> <p>The args token.</p> <code>call_id</code> <p>The call id token.</p> <code>audio</code> <p>The audio token.</p> <code>begin_audio</code> <p>The beginning of audio token.</p> <code>transcribe</code> <p>The transcribe token.</p> <code>begin_think</code> <p>The beginning of think token.</p> <code>end_think</code> <p>The end of think token.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unk = SpecialTokens.unk\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenized","title":"<code>Tokenized(**data)</code>","text":"<p>               Bases: <code>MistralBase</code></p> <p>A tokenized <code>InstructRequest</code>.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>list[int]</code> <p>The token ids.</p> <code>text</code> <code>str | None</code> <p>The text representation of the tokens.</p> <code>prefix_ids</code> <code>list[int] | None</code> <p>The prefix ids for FIM.</p> <code>images</code> <code>list[ndarray]</code> <p>The loaded images associated with the tokens.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokenized = Tokenized(tokens=[1, 2, 3], text=\"Hello world\", prefix_ids=[1], images=[])\n</code></pre> Source code in <code>.venv/lib/python3.13/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.bos_id","title":"<code>bos_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the Beginning of String token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.eos_id","title":"<code>eos_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the End of String token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.file_path","title":"<code>file_path</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The file path of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.n_words","title":"<code>n_words</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Vocabulary size of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.num_special_tokens","title":"<code>num_special_tokens</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The number of special tokens of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.pad_id","title":"<code>pad_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the Pad token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.special_ids","title":"<code>special_ids</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Ids of the special tokens.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.unk_id","title":"<code>unk_id</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>id of the Unk token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.version","title":"<code>version</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>  <code>abstractmethod</code>","text":"<p>Decode the token ids to a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The token ids to decode.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy to use for special tokens. Passing <code>None</code> will default to <code>self._special_token_policy</code> for Tekkenizer and <code>SpecialTokenPolicy.IGNORE</code> for SentencePieceTokenizer. Note that passing <code>None</code> will be deprecated and <code>special_token_policy</code> will default to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef decode(self, tokens: list[int], special_token_policy: SpecialTokenPolicy | None = None) -&gt; str:\n    r\"\"\"Decode the token ids to a string.\n\n    Args:\n        tokens: The token ids to decode.\n        special_token_policy: The policy to use for special tokens.\n            Passing `None` will default to `self._special_token_policy` for\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer] and `SpecialTokenPolicy.IGNORE`\n            for [SentencePieceTokenizer][mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer].\n            Note that passing `None` will be deprecated and `special_token_policy` will default to\n            `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.encode","title":"<code>encode(s, bos, eos)</code>  <code>abstractmethod</code>","text":"<p>Convert a string to a list of token ids.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef encode(self, s: str, bos: bool, eos: bool) -&gt; list[int]:\n    \"\"\"Convert a string to a list of token ids.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.get_special_token","title":"<code>get_special_token(s)</code>  <code>abstractmethod</code>","text":"<p>Get the id of a control token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef get_special_token(self, s: str) -&gt; int:\n    r\"\"\"Get the id of a control token.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.id_to_piece","title":"<code>id_to_piece(token_id)</code>  <code>abstractmethod</code>","text":"<p>Convert a token id to the token str.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef id_to_piece(self, token_id: int) -&gt; str:\n    r\"\"\"Convert a token id to the token str.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.is_special","title":"<code>is_special(token)</code>  <code>abstractmethod</code>","text":"<p>Check if token id or token str is a special token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef is_special(self, token: int | np.integer | str) -&gt; bool:\n    r\"\"\"Check if token id or token str is a special token.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.to_string","title":"<code>to_string(tokens)</code>  <code>abstractmethod</code>","text":"<p>[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.</p> <p>Use <code>decode</code> with <code>special_token_policy=SpecialTokenPolicy.KEEP</code> instead.</p> <p>This is a convenient method for debugging.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef to_string(self, tokens: list[int]) -&gt; str:\n    r\"\"\"[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.\n\n    Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\n\n    This is a convenient method for debugging.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.Tokenizer.vocab","title":"<code>vocab()</code>  <code>abstractmethod</code>","text":"<p>All tokens in the vocabulary as strings.</p> Source code in <code>src/mistral_common/tokens/tokenizers/base.py</code> <pre><code>@abstractmethod\ndef vocab(self) -&gt; list[str]:\n    r\"\"\"All tokens in the vocabulary as strings.\"\"\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.TokenizerVersion","title":"<code>TokenizerVersion</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of tokenizer versions.</p> <p>Allow to distinguish between different versions of the tokenizer and maintain backward compatibility.</p> <p>Attributes:</p> Name Type Description <code>v1</code> <p>The first version of the tokenizer.</p> <code>v2</code> <p>The second version of the tokenizer that includes special control tokens [INST], [\\INST].</p> <code>v3</code> <p>The third version of the tokenizer that includes improved function calling.</p> <code>v7</code> <p>The seventh version of the tokenizer that includes improved system prompt and function calling.</p> <code>v11</code> <p>The eleventh version of the tokenizer that includes improved function calling.</p> <code>v13</code> <p>The thirteenth version of the tokenizer that includes no call id tokenization and better prompt caching.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; version = TokenizerVersion.v1\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/base/#mistral_common.tokens.tokenizers.base.UserMessagePosition","title":"<code>UserMessagePosition</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Where to encode available tools</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/","title":"image","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image","title":"<code>mistral_common.tokens.tokenizers.image</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageConfig","title":"<code>ImageConfig(image_patch_size, max_image_size, spatial_merge_size=1)</code>  <code>dataclass</code>","text":"<p>Configuration for the image tokenizers.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageEncoder","title":"<code>ImageEncoder(image_config, special_ids)</code>","text":"<p>Image encoder for the image tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>image_config</code> <code>ImageConfig</code> <p>Configuration for the image tokenizer.</p> required <code>special_ids</code> <code>SpecialImageIDs</code> <p>Special image tokens ids.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def __init__(self, image_config: ImageConfig, special_ids: SpecialImageIDs) -&gt; None:\n    r\"\"\"Initialize the image encoder.\n\n    Args:\n        image_config: Configuration for the image tokenizer.\n        special_ids: Special image tokens ids.\n    \"\"\"\n    self.image_config = image_config\n    self.special_ids = special_ids\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageEncoder.__call__","title":"<code>__call__(content)</code>","text":"<p>Converts an image chunk to an image encoding.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>ImageChunk | ImageURLChunk</code> <p>image chunk to be converted.</p> required <p>Returns:</p> Type Description <code>ImageEncoding</code> <p>Image encoding.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def __call__(self, content: ImageChunk | ImageURLChunk) -&gt; ImageEncoding:\n    r\"\"\"Converts an image chunk to an image encoding.\n\n    Args:\n        content: image chunk to be converted.\n\n    Returns:\n        Image encoding.\n    \"\"\"\n    image = image_from_chunk(content)\n    w, h = self._image_to_num_tokens(image)\n    assert w &gt; 0\n    assert h &gt; 0\n    image_tokens = ([self.special_ids.img] * w + [self.special_ids.img_break]) * h\n    image_tokens[-1] = self.special_ids.img_end\n    new_image_size = (\n        w * self.image_config.image_patch_size * self.image_config.spatial_merge_size,\n        h * self.image_config.image_patch_size * self.image_config.spatial_merge_size,\n    )\n    processed_image = transform_image(image, new_image_size)\n    return ImageEncoding(tokens=image_tokens, image=processed_image)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.ImageEncoding","title":"<code>ImageEncoding(tokens, image)</code>  <code>dataclass</code>","text":"<p>A tokenized image.</p> <p>Attributes:</p> Name Type Description <code>tokens</code> <code>list[int]</code> <p>The token ids.</p> <code>image</code> <code>ndarray</code> <p>The image as a numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; image_encoding = ImageEncoding(tokens=[1, 2, 3], image=np.array([[0., 0.5, 1.]]))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.MultiModalVersion","title":"<code>MultiModalVersion</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Version of the image tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.SpecialImageIDs","title":"<code>SpecialImageIDs(img, img_break, img_end)</code>  <code>dataclass</code>","text":"<p>Special image tokens ids.</p> <p>Attributes:</p> Name Type Description <code>img</code> <code>int</code> <p>The image token id.</p> <code>img_break</code> <code>int</code> <p>The image break token id.</p> <code>img_end</code> <code>int</code> <p>The image end token id.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; special_image_ids = SpecialImageIDs(img=1, img_break=2, img_end=3)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.image_from_chunk","title":"<code>image_from_chunk(chunk)</code>","text":"<p>Get a serializable image from a chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>ImageURLChunk | ImageChunk</code> <p>The chunk to get the image from.</p> required <p>Returns:</p> Type Description <code>SerializableImage</code> <p>The image as a PIL Image object.</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def image_from_chunk(chunk: ImageURLChunk | ImageChunk) -&gt; SerializableImage:\n    r\"\"\"Get a serializable image from a chunk.\n\n    Args:\n        chunk: The chunk to get the image from.\n\n    Returns:\n        The image as a PIL Image object.\n    \"\"\"\n    if isinstance(chunk, ImageChunk):\n        return chunk.image\n    if chunk.get_url().startswith(\"data:image\"):\n        data = chunk.get_url().split(\",\")[1]\n        image_data = base64.b64decode(data)\n        return Image.open(BytesIO(image_data))\n    if chunk.get_url().startswith(\"file\"):\n        return Image.open(open(chunk.get_url().replace(\"file://\", \"\"), \"rb\"))\n    if chunk.get_url().startswith(\"http\"):\n        return download_image(chunk.get_url())\n\n    raise RuntimeError(f\"Unsupported image url scheme {chunk.get_url()}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.normalize","title":"<code>normalize(np_image, mean, std)</code>","text":"<p>Normalize a tensor image with mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>np_image</code> <code>ndarray</code> <p>Image to be normalized.</p> required <code>mean</code> <code>tuple[float, float, float]</code> <p>Mean for each channel.</p> required <code>std</code> <code>tuple[float, float, float]</code> <p>Standard deviation for each channel.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized image with shape (C, H, W).</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def normalize(\n    np_image: np.ndarray,\n    mean: tuple[float, float, float],\n    std: tuple[float, float, float],\n) -&gt; np.ndarray:\n    r\"\"\"Normalize a tensor image with mean and standard deviation.\n\n    Args:\n        np_image: Image to be normalized.\n        mean: Mean for each channel.\n        std: Standard deviation for each channel.\n\n    Returns:\n        Normalized image with shape (C, H, W).\n    \"\"\"\n    np_image = np_image / 255.0\n\n    assert len(np_image.shape) == 3, f\"{np_image.shape=}\"\n    assert np_image.shape[2] == len(mean) == len(std), f\"{np_image.shape=}, {mean=}, {std=}\"\n\n    np_image = (np_image - mean) / std\n\n    return np_image.transpose(2, 0, 1)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/image/#mistral_common.tokens.tokenizers.image.transform_image","title":"<code>transform_image(image, new_size)</code>","text":"<p>Transform an image to a numpy array with the given size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image to be transformed.</p> required <code>new_size</code> <code>tuple[int, int]</code> <p>New size of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Transformed image with shape (C, H, W).</p> Source code in <code>src/mistral_common/tokens/tokenizers/image.py</code> <pre><code>def transform_image(image: Image.Image, new_size: tuple[int, int]) -&gt; np.ndarray:\n    r\"\"\"Transform an image to a numpy array with the given size.\n\n    Args:\n        image: Image to be transformed.\n        new_size: New size of the image.\n\n    Returns:\n        Transformed image with shape (C, H, W).\n    \"\"\"\n    assert_opencv_installed()\n\n    np_image = cv2.resize(np.array(_convert_to_rgb(image), dtype=np.float32), new_size, interpolation=cv2.INTER_CUBIC)\n    return normalize(np_image, DATASET_MEAN, DATASET_STD)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/","title":"instruct","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct","title":"<code>mistral_common.tokens.tokenizers.instruct</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase","title":"<code>InstructTokenizerBase(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizer</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Base instruct tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>ImageEncoder | None</code> <p>The image encoder to use if any.</p> <code>None</code> <code>audio_encoder</code> <code>AudioEncoder | None</code> <p>The audio encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n):\n    r\"\"\"Initialize the instruct tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use if any.\n        audio_encoder: The audio encoder to use.\n    \"\"\"\n    self.tokenizer = tokenizer\n    self.image_encoder = image_encoder\n    self.audio_encoder = audio_encoder\n    super().__init__(tokenizer, image_encoder, audio_encoder)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decode tokens to a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The tokens to decode.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy to use for special tokens. Passing <code>None</code> will default to <code>self._special_token_policy</code> for Tekkenizer and <code>SpecialTokenPolicy.IGNORE</code> for SentencePieceTokenizer. Note that passing <code>None</code> will be deprecated and <code>special_token_policy</code> will default to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def decode(self, tokens: list[int], special_token_policy: SpecialTokenPolicy | None = None) -&gt; str:\n    r\"\"\"Decode tokens to a string.\n\n    Args:\n        tokens: The tokens to decode.\n        special_token_policy: The policy to use for special tokens.\n            Passing `None` will default to `self._special_token_policy` for\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer] and `SpecialTokenPolicy.IGNORE`\n            for [SentencePieceTokenizer][mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer].\n            Note that passing `None` will be deprecated and `special_token_policy` will default to\n            `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    return self.tokenizer.decode(tokens, special_token_policy=special_token_policy)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>  <code>abstractmethod</code>","text":"<p>Encode an assistant message.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The assistant message is not implemented for the base tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@abstractmethod\ndef encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; list[int]:\n    r\"\"\"Encode an assistant message.\n\n    Raises:\n        NotImplementedError: The assistant message is not implemented for the base tokenizer.\n    \"\"\"\n    raise NotImplementedError(\"Assistant message not implemented\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_instruct","title":"<code>encode_instruct(request)</code>","text":"<p>Encode an instruct request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstructRequest[AssistantMessageType, Tool]</code> <p>The request to encode.</p> required <p>Returns:</p> Type Description <code>Tokenized</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_instruct(\n    self,\n    request: InstructRequest[AssistantMessageType, Tool],\n) -&gt; Tokenized:\n    r\"\"\"Encode an instruct request.\n\n    Args:\n        request: The request to encode.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    # init at bos\n    images: list[np.ndarray] = []\n    audios: list[Audio] = []\n    prefix_ids: list[int] | None = None\n    tokens_list: list[list[int] | None] = []\n\n    # validate messages\n    self.validate_messages(request.messages)\n\n    # find last user message\n    first_user_idx, last_user_idx = self.find_first_last_user(request)\n    for msg_idx, msg in enumerate(request.messages):\n        if (\n            request.continue_final_message\n            and (msg_idx == len(request.messages) - 1)\n            and not isinstance(msg, AssistantMessage)\n        ):\n            raise InvalidMessageStructureException(\n                \"Cannot continue final message if it is not an assistant message\"\n            )\n        if isinstance(msg, UserMessage):\n            new_tokens, new_images, new_audios = self.encode_user_message(\n                msg,\n                request.available_tools,\n                msg_idx == last_user_idx,\n                msg_idx == first_user_idx,\n                system_prompt=request.system_prompt,\n                force_img_first=True,  # img is always first when providing text/img chunk pair\n            )\n            images.extend(new_images)\n            audios.extend(new_audios)\n        elif isinstance(msg, ToolMessage):\n            new_tokens = self.encode_tool_message(msg, msg_idx &lt; last_user_idx)\n        elif isinstance(msg, AssistantMessage):\n            continue_message = request.continue_final_message and (msg_idx == len(request.messages) - 1)\n\n            new_tokens = self.encode_assistant_message(\n                msg, msg_idx &lt; last_user_idx, continue_message=continue_message\n            )\n            if msg_idx == len(request.messages) - 1:\n                prefix_ids = new_tokens\n        elif isinstance(msg, SystemMessage):\n            new_tokens = self.encode_system_message(msg)\n        else:\n            raise TokenizerException(f\"Unknown message type {type(msg)}\")\n\n        tokens_list.append(new_tokens)\n\n    if request.truncate_at_max_tokens is not None:\n        self._truncate_for_max_tokens(\n            tokens_list,\n            request.messages,\n            request.truncate_at_max_tokens,\n            last_user_idx,\n        )\n    tokens = self.start()\n\n    for tok in tokens_list:\n        if tok is not None:\n            tokens.extend(tok)\n\n    return Tokenized(\n        tokens=tokens,\n        text=self.decode(tokens, special_token_policy=SpecialTokenPolicy.KEEP),\n        prefix_ids=prefix_ids,\n        images=images,\n        audios=audios,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_think","title":"<code>encode_think(chunk)</code>  <code>abstractmethod</code>","text":"<p>Encode a think chunk.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The think chunk is not implemented for the base tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@abstractmethod\ndef encode_think(self, chunk: ThinkChunk) -&gt; list[int]:\n    r\"\"\"Encode a think chunk.\n\n    Raises:\n        NotImplementedError: The think chunk is not implemented for the base tokenizer.\n    \"\"\"\n    raise NotImplementedError(\"Think chunk not implemented\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>  <code>abstractmethod</code>","text":"<p>Encode a tool message.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>The tool message is not implemented for the base tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@abstractmethod\ndef encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; list[int]:\n    r\"\"\"Encode a tool message.\n\n    Raises:\n        NotImplementedError: The tool message is not implemented for the base tokenizer.\n    \"\"\"\n    raise NotImplementedError(\"Tool message not implemented\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.find_first_last_user","title":"<code>find_first_last_user(request)</code>  <code>staticmethod</code>","text":"<p>Find the first and last user message in the request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InstructRequest</code> <p>The request to search for user messages.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The index of the first and last user message.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@staticmethod\ndef find_first_last_user(request: InstructRequest) -&gt; tuple[int, int]:\n    r\"\"\"Find the first and last user message in the request.\n\n    Args:\n        request: The request to search for user messages.\n\n    Returns:\n        The index of the first and last user message.\n    \"\"\"\n    last_user_idx = -1\n    first_user_idx = -1\n    for i, msg in list(enumerate(request.messages)):\n        if isinstance(msg, UserMessage):\n            if first_user_idx == -1:\n                first_user_idx = i\n            last_user_idx = i\n    return first_user_idx, last_user_idx\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerBase.start","title":"<code>start()</code>","text":"<p>Return the start tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def start(self) -&gt; list[int]:\n    r\"\"\"Return the start tokens.\"\"\"\n    return [self.tokenizer.bos_id]\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1","title":"<code>InstructTokenizerV1(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerBase</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Instruct tokenizer V1.</p> <p>This tokenizer has basic for messages. It does not support tools or image inputs.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n):\n    r\"\"\"Initialize the instruct tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use if any.\n        audio_encoder: The audio encoder to use.\n    \"\"\"\n    self.tokenizer = tokenizer\n    self.image_encoder = image_encoder\n    self.audio_encoder = audio_encoder\n    super().__init__(tokenizer, image_encoder, audio_encoder)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <code>continue_message</code> <code>bool</code> <p>Whether to continue the message generation. Only use this if the assistant message is the last message.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; list[int]:\n    r\"\"\"Encode an assistant message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert isinstance(message, AssistantMessage), message\n    if message.tool_calls is not None and len(message.tool_calls) &gt; 0:\n        raise TokenizerException(\"Tools not implemented for tokenizer V1\")\n    if continue_message and message.prefix:\n        raise InvalidAssistantMessageException(\n            \"`continue_message` is only supported for assistant messages that have `prefix=False`.\"\n        )\n    elif message.content:\n        assert isinstance(message.content, str), \"Message content must be a string for tokenizer &lt; V13\"\n        curr_tokens = self.tokenizer.encode(message.content, bos=False, eos=False)\n    else:\n        raise TokenizerException(f\"{message.content} // {message.tool_calls}\")\n    if not message.prefix and not continue_message:\n        curr_tokens.append(self.tokenizer.eos_id)\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_fim","title":"<code>encode_fim(request)</code>","text":"<p>Encode a FIM request.</p> <p>Raises:</p> Type Description <code>TokenizerException</code> <p>The FIM request is not implemented for this version.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_fim(self, request: FIMRequest) -&gt; Tokenized:\n    r\"\"\"Encode a FIM request.\n\n    Raises:\n       TokenizerException: The FIM request is not implemented for this version.\n    \"\"\"\n    raise TokenizerException(f\"FIM not available for {self.tokenizer.version}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_think","title":"<code>encode_think(chunk)</code>","text":"<p>Encode a think chunk.</p> <p>Raises:</p> Type Description <code>TokenizerException</code> <p>The think chunk is not implemented for this version.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_think(self, chunk: ThinkChunk) -&gt; list[int]:\n    r\"\"\"Encode a think chunk.\n\n    Raises:\n        TokenizerException: The think chunk is not implemented for this version.\n    \"\"\"\n    raise TokenizerException(\"Think not implemented for tokenizer &lt; V13.\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> <p>Raises:</p> Type Description <code>TokenizerException</code> <p>The tool message is not implemented for this version.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; list[int]:\n    r\"\"\"Encode a tool message.\n\n    Raises:\n        TokenizerException: The tool message is not implemented for this version.\n    \"\"\"\n    raise TokenizerException(\"Tools not implemented for tokenizer V1\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | list[UserContentChunk]</code> <p>The content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Not used.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and empty list.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_content(\n    self,\n    content: str | list[UserContentChunk],\n    is_last: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The content to encode.\n        is_last: Whether the message is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Not used.\n\n    Returns:\n        The encoded tokens and empty list.\n    \"\"\"\n    assert isinstance(content, str)\n\n    if is_last and system_prompt:\n        content = system_prompt + \"\\n\\n\" + content\n\n    tokens = self.tokenizer.encode(content, bos=False, eos=False)\n    return tokens, [], []\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV1.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The message to encode.</p> required <code>available_tools</code> <code>list[Tool] | None</code> <p>Not used.</p> required <code>is_last</code> <code>bool</code> <p>Not used.</p> required <code>is_first</code> <code>bool</code> <p>Whether the message is the first one.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Not used.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and empty list.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: list[Tool] | None,\n    is_last: bool,\n    is_first: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The message to encode.\n        available_tools: Not used.\n        is_last: Not used.\n        is_first: Whether the message is the first one.\n        system_prompt: The system prompt.\n        force_img_first: Not used.\n\n    Returns:\n        The encoded tokens and empty list.\n    \"\"\"\n    assert isinstance(message.content, str), \"Message content must be normalized\"\n    assert self.image_encoder is None, \"InstructTokenizerV1 cannot encode images\"\n\n    content = \"\"\n    if is_first and system_prompt:\n        content = system_prompt + \"\\n\\n\" + message.content\n    else:\n        content = message.content\n\n    message_txt = f\"[INST] {content} [/INST]\"\n    curr_tokens, image, audio = self.encode_user_content(content=message_txt, is_last=False, system_prompt=None)\n    return curr_tokens, image, audio\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV11","title":"<code>InstructTokenizerV11(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV7</code></p> <p>Instruct tokenizer V11.</p> <p>The difference with V7 tokenizer is that it encodes tool calls differently: Tool call results are encoded as : - [begin tool call] call_name_tokens [call id] call_id_tokens [args] content tokens</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n) -&gt; None:\n    super().__init__(tokenizer, image_encoder, audio_encoder)\n    self.ARGS = self.tokenizer.get_special_token(SpecialTokens.args.value)\n    self.CALL_ID = self.tokenizer.get_special_token(SpecialTokens.call_id.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV13","title":"<code>InstructTokenizerV13(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV11</code></p> <p>Instruct tokenizer V13.</p> The difference with V11 tokenizer is that it encodes tool calls differently <ul> <li>available tools are tokenized at the first user message.</li> <li>call id is no longer tokenized for tool calls or results.</li> </ul> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n) -&gt; None:\n    super().__init__(tokenizer, image_encoder, audio_encoder)\n    assert isinstance(tokenizer, Tekkenizer), f\"Tokenizer must be a Tekkenizer. Got {type(tokenizer)}\"\n    if (\n        SpecialTokens.begin_think.value in tokenizer._special_tokens_reverse_vocab\n        and SpecialTokens.end_think.value in tokenizer._special_tokens_reverse_vocab\n    ):\n        self.BEGIN_THINK: int | None = tokenizer.get_special_token(SpecialTokens.begin_think.value)\n        self.END_THINK: int | None = tokenizer.get_special_token(SpecialTokens.end_think.value)\n    else:\n        self.BEGIN_THINK = None\n        self.END_THINK = None\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV13.encode_think","title":"<code>encode_think(chunk)</code>","text":"<p>Encode a thinking chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>ThinkChunk</code> <p>The thinking chunk to encode.</p> required <p>Returns:     The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_think(self, chunk: ThinkChunk) -&gt; list[int]:\n    r\"\"\"Encode a thinking chunk.\n\n    Args:\n        chunk: The thinking chunk to encode.\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert self.BEGIN_THINK is not None, \"think tokens are not available for this tokenizer.\"\n    assert self.END_THINK is not None, \"think tokens are not available for this tokenizer.\"\n    tokens = self.tokenizer.encode(chunk.thinking, bos=False, eos=False)\n    think_tokens = [self.BEGIN_THINK, *tokens]\n    if chunk.closed:\n        think_tokens.append(self.END_THINK)\n    return think_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV13.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <p>Returns:     The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; list[int]:\n    r\"\"\"Encode a tool message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert message.tool_call_id is not None, \"Tool call id must be provided for tokenizer &gt;= v13\"\n\n    content = message.content\n    if not isinstance(content, str):\n        content = \"\".join(chunk.text for chunk in content)\n\n    tokens = self.tokenizer.encode(content, bos=False, eos=False)\n    curr_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *tokens,\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV13.validate_messages","title":"<code>validate_messages(messages)</code>  <code>classmethod</code>","text":"<p>Allows system prompts and audio chunks to coexist in v13.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@classmethod\ndef validate_messages(cls, messages: list[UATS]) -&gt; None:\n    r\"\"\"Allows system prompts and audio chunks to coexist in v13.\"\"\"\n    return\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2","title":"<code>InstructTokenizerV2(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV1</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Instruct tokenizer V2.</p> <p>This tokenizer adds supports to images, tools and FIM requests.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>ImageEncoder | None</code> <p>The image encoder to use.</p> <code>None</code> <code>audio_encoder</code> <code>AudioEncoder | None</code> <p>The audio encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n):\n    r\"\"\"Initialize the tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use.\n        audio_encoder: The audio encoder to use.\n    \"\"\"\n    super().__init__(tokenizer, image_encoder, audio_encoder)\n    self.BEGIN_INST = self.tokenizer.get_special_token(SpecialTokens.begin_inst.value)\n    self.END_INST = self.tokenizer.get_special_token(SpecialTokens.end_inst.value)\n    self.BEGIN_AVAILABLE_TOOLS = self.tokenizer.get_special_token(SpecialTokens.begin_tools.value)\n    self.END_AVAILABLE_TOOLS = self.tokenizer.get_special_token(SpecialTokens.end_tools.value)\n    self.BEGIN_TOOL_RESULTS = self.tokenizer.get_special_token(SpecialTokens.begin_tool_results.value)\n    self.END_TOOL_RESULTS = self.tokenizer.get_special_token(SpecialTokens.end_tool_results.value)\n    self.TOOL_CALLS = self.tokenizer.get_special_token(SpecialTokens.tool_calls.value)\n    self.BOS = self.tokenizer.get_special_token(SpecialTokens.bos.value)\n    self.PREFIX = self.tokenizer.get_special_token(SpecialTokens.prefix.value)\n    self.SUFFIX = self.tokenizer.get_special_token(SpecialTokens.suffix.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Whether the message is before the last user message. If has tools and true, the message is not encoded.</p> required <code>continue_message</code> <code>bool</code> <p>Whether to continue the message generation. Only use this if the assistant message is the last message.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; list[int]:\n    r\"\"\"Encode an assistant message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Whether the message is before the last user message. If has tools and true, the\n            message is not encoded.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    if message.tool_calls and message.content:\n        raise ValueError(f\"Cannot have tool calls and content defined in the same assistant message {message}\")\n    if continue_message and message.prefix:\n        raise InvalidAssistantMessageException(\n            \"`continue_message` is only supported for assistant messages that have `prefix=False`.\"\n        )\n\n    if message.tool_calls:\n        if is_before_last_user_message:\n            # don't tokenize tool call before last user message\n            return []\n        curr_tokens = self._encode_tool_calls_in_assistant_message(message)\n    elif message.content:\n        assert isinstance(message.content, str), \"Message content must be a string for tokenizer &lt; V7\"\n        curr_tokens = self._encode_normal_content_assistant_message(message)\n    else:\n        raise TokenizerException(f\"Invalid assistant message: {message.content}\")\n    if not message.prefix and not continue_message:\n        curr_tokens.append(self.tokenizer.eos_id)\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_fim","title":"<code>encode_fim(request)</code>","text":"<p>Encode a FIM request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FIMRequest</code> <p>The request to encode.</p> required <p>Returns:</p> Type Description <code>Tokenized</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_fim(self, request: FIMRequest) -&gt; Tokenized:\n    r\"\"\"Encode a FIM request.\n\n    Args:\n        request: The request to encode.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    prefix_tokens = self.tokenizer.encode(request.prompt, bos=False, eos=False)\n    suffix_tokens = self._encode_infilling(request.suffix) if request.suffix else []\n    tokens = [\n        self.BOS,\n        self.SUFFIX,\n        *suffix_tokens,\n        self.PREFIX,\n        *prefix_tokens,\n    ]\n    return Tokenized(tokens=tokens, text=self.decode(tokens, special_token_policy=SpecialTokenPolicy.KEEP))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Whether the message is before the last user message. If true, the message is not encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; list[int]:\n    r\"\"\"Encode a tool message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Whether the message is before the last user message. If true, the message is\n            not encoded.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    if is_before_last_user_message:\n        # don't tokenize last tool response before last user msg\n        return []\n\n    # Currently only supports single tool results\n    tool_result_str = json.dumps([self._prepare_tool_result(message)], ensure_ascii=False)\n    curr_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *self.tokenizer.encode(tool_result_str, bos=False, eos=False),\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The message to encode.</p> required <code>available_tools</code> <code>list[Tool] | None</code> <p>The list of available tools if any.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>is_first</code> <code>bool</code> <p>Not used.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and the list of images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: list[Tool] | None,\n    is_last: bool,\n    is_first: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The message to encode.\n        available_tools: The list of available tools if any.\n        is_last: Whether the message is the last one.\n        is_first: Not used.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the list of images.\n    \"\"\"\n    do_encode_tools = False\n    do_encode_tools |= is_first and (self._user_message_position_to_encode_tools == UserMessagePosition.first)\n    do_encode_tools |= is_last and (self._user_message_position_to_encode_tools == UserMessagePosition.last)\n    tools_tokens: list[int] = []\n\n    if do_encode_tools and available_tools:\n        tools = [tool.model_dump() for tool in available_tools]\n        tools_json_tokens = self.tokenizer.encode(json.dumps(tools, ensure_ascii=False), bos=False, eos=False)\n        tools_tokens = [\n            self.BEGIN_AVAILABLE_TOOLS,\n            *tools_json_tokens,\n            self.END_AVAILABLE_TOOLS,\n        ]\n\n    tokens, image, audio = self.encode_user_content(\n        content=message.content,\n        is_last=is_last,\n        system_prompt=system_prompt,\n        force_img_first=force_img_first,\n    )\n\n    prefix_tokens = [*tools_tokens, self.BEGIN_INST]\n    suffix_tokens = [self.END_INST]\n\n    curr_tokens = prefix_tokens + tokens + suffix_tokens\n\n    return curr_tokens, image, audio\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3","title":"<code>InstructTokenizerV3(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV2</code>, <code>Generic[InstructRequestType, FIMRequestType, TokenizedType, AssistantMessageType]</code></p> <p>Instruct tokenizer V3.</p> <p>The only difference with V2 tokenizer is that it encodes the tool messages differently.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>ImageEncoder | None</code> <p>The image encoder to use.</p> <code>None</code> <code>audio_encoder</code> <code>AudioEncoder | None</code> <p>The audio encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n):\n    r\"\"\"Initialize the tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use.\n        audio_encoder: The audio encoder to use.\n    \"\"\"\n    super().__init__(tokenizer, image_encoder=image_encoder, audio_encoder=audio_encoder)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> Note <p>Same as V2 but always encode the tool history. continue_message: Whether to continue the message generation.     Only use this if the assistant message is the last message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; list[int]:\n    r\"\"\"Encode an assistant message.\n\n    Note:\n        Same as [V2][mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_assistant_message] but\n        always encode the tool history.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    return super().encode_assistant_message(message, False, continue_message)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> Note <p>Same as V2 but tools are not wrapped in a list and the history is also tokenized.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Whether the message is before the last user message. If true, the message is not encoded.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; list[int]:\n    r\"\"\"Encode a tool message.\n\n    Note:\n        Same as [V2][mistral_common.tokens.tokenizers.instruct.InstructTokenizerV2.encode_tool_message] but tools\n        are not wrapped in a list and the history is also tokenized.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Whether the message is before the last user message. If true, the message is\n            not encoded.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    tool_result_str = json.dumps(self._prepare_tool_result(message), ensure_ascii=False)\n    curr_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *self.tokenizer.encode(tool_result_str, bos=False, eos=False),\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | list[UserContentChunk]</code> <p>The content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and the images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_content(\n    self,\n    content: str | list[UserContentChunk],\n    is_last: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The content to encode.\n        is_last: Whether the message is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the images.\n    \"\"\"\n    if isinstance(content, str):\n        return super().encode_user_content(content, is_last, system_prompt)\n\n    tokens: list[int] = []\n    images: list[np.ndarray] = []\n    audio: list[Audio] = []\n\n    has_one_img_one_text_first = len(content) == 2 and isinstance(content[1], (ImageChunk, ImageURLChunk))\n    if force_img_first and has_one_img_one_text_first:\n        # make sure that if exactly one image and text chunk are passed we force the image chunk to be first\n        content = [content[1], content[0]]\n\n    first_chunk = True\n    for chunk in content:\n        content_str = \"\"\n        if first_chunk and is_last and system_prompt:\n            first_chunk = False\n            content_str = system_prompt + \"\\n\\n\"\n            tokens += self.tokenizer.encode(content_str, bos=False, eos=False)\n\n        if isinstance(chunk, (AudioChunk, AudioURLChunk)):\n            assert not content_str, (\n                f\"It is not possible that `content` is non-empty when chunk is of type {type(chunk)}.\"\n            )\n            chunk_tokens, _, chunk_audio = self._encode_content_chunk(chunk)\n            audio.append(chunk_audio)\n        elif isinstance(chunk, (ImageChunk, ImageURLChunk)):\n            chunk_tokens, chunk_image, _ = self._encode_content_chunk(chunk)\n            images.append(chunk_image)\n        else:\n            chunk_tokens = self._encode_content_chunk(chunk)[0]\n        tokens.extend(chunk_tokens)\n\n    return tokens, images, audio\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7","title":"<code>InstructTokenizerV7(tokenizer, image_encoder=None, audio_encoder=None)</code>","text":"<p>               Bases: <code>InstructTokenizerV3</code></p> <p>Instruct tokenizer V7.</p> <p>The difference with V3 tokenizer is that it encodes the system prompts differently: - in V7 the system prompts are treated as separate SystemMessages - they are no longer prepended to the last user message - they are printed between special tokens</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>image_encoder</code> <code>ImageEncoder | None</code> <p>The image encoder to use.</p> <code>None</code> <code>audio_encoder</code> <code>AudioEncoder | None</code> <p>The audio encoder to use.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: Tokenizer,\n    image_encoder: ImageEncoder | None = None,\n    audio_encoder: AudioEncoder | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the tokenizer.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        image_encoder: The image encoder to use.\n        audio_encoder: The audio encoder to use.\n    \"\"\"\n\n    super().__init__(tokenizer, image_encoder, audio_encoder)\n    self.BEGIN_SYSTEM = self.tokenizer.get_special_token(SpecialTokens.begin_system.value)\n    self.END_SYSTEM = self.tokenizer.get_special_token(SpecialTokens.end_system.value)\n    self.BEGIN_TOOL_CONTENT = self.tokenizer.get_special_token(SpecialTokens.begin_tool_content.value)\n\n    self.TRANSCRIBE = None\n    if audio_encoder is not None and not audio_encoder.audio_config.is_streaming:\n        self.TRANSCRIBE = self.tokenizer.get_special_token(SpecialTokens.transcribe.value)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_assistant_message","title":"<code>encode_assistant_message(message, is_before_last_user_message, continue_message)</code>","text":"<p>Encode an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>AssistantMessageType</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <code>continue_message</code> <code>bool</code> <p>Whether to continue the message generation. Only use this if the assistant message is the last message.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_assistant_message(\n    self, message: AssistantMessageType, is_before_last_user_message: bool, continue_message: bool\n) -&gt; list[int]:\n    r\"\"\"Encode an assistant message.\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n        continue_message: Whether to continue the message generation.\n            Only use this if the assistant message is the last message.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    if not message.content and not message.tool_calls:\n        raise TokenizerException(f\"Invalid assistant message: {message}\")\n    if continue_message and message.prefix:\n        raise InvalidAssistantMessageException(\n            \"`continue_message` is only supported for assistant messages that have `prefix=False`.\"\n        )\n\n    curr_tokens: list = []\n    if message.content:\n        if isinstance(message.content, str):\n            curr_tokens = self._encode_normal_content_assistant_message(message)\n        elif isinstance(message.content, list):\n            curr_tokens += self._encode_content_chunks(message.content)[0]\n    if message.tool_calls:\n        curr_tokens += self._encode_tool_calls_in_assistant_message(message)\n    if not message.prefix and not continue_message:\n        curr_tokens.append(self.tokenizer.eos_id)\n\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_system_message","title":"<code>encode_system_message(message)</code>","text":"<p>Encode a system message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>SystemMessage</code> <p>The message to encode.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_system_message(self, message: SystemMessage) -&gt; list[int]:\n    r\"\"\"Encode a system message.\n\n    Args:\n        message: The message to encode.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n\n    tokens = [self.BEGIN_SYSTEM]\n    if isinstance(content := message.content, str):\n        content = [TextChunk(text=content)]\n    tokens += self._encode_content_chunks(content)[0]\n    tokens.append(self.END_SYSTEM)\n    return tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_tool_message","title":"<code>encode_tool_message(message, is_before_last_user_message)</code>","text":"<p>Encode a tool message.</p> Note <p>Same as V3 but tools are not wrapped in a list and history is also tokenized</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>ToolMessage</code> <p>The message to encode.</p> required <code>is_before_last_user_message</code> <code>bool</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The encoded tokens.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_tool_message(self, message: ToolMessage, is_before_last_user_message: bool) -&gt; list[int]:\n    r\"\"\"Encode a tool message.\n\n    Note:\n        Same as [V3][mistral_common.tokens.tokenizers.instruct.InstructTokenizerV3.encode_tool_message]\n        but tools are not wrapped in a list and history is also tokenized\n\n    Args:\n        message: The message to encode.\n        is_before_last_user_message: Not used.\n\n    Returns:\n        The encoded tokens.\n    \"\"\"\n    assert message.tool_call_id is not None\n    assert isinstance(message.content, str), \"Message content must be normalized\"\n    tool_call_id_tokens = self.tokenizer.encode(message.tool_call_id, bos=False, eos=False)\n    tokens = self.tokenizer.encode(message.content, bos=False, eos=False)\n\n    prefix_tokens = [\n        self.BEGIN_TOOL_RESULTS,\n        *tool_call_id_tokens,\n        self.BEGIN_TOOL_CONTENT,\n    ]\n    curr_tokens = [\n        *prefix_tokens,\n        *tokens,\n        self.END_TOOL_RESULTS,\n    ]\n    return curr_tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_transcription","title":"<code>encode_transcription(request)</code>","text":"<p>Encodes an audio transcription request into a tokenized format.</p> <p>This method processes a transcription request containing audio data, encodes the user message, and returns the tokenized output.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TranscriptionRequest</code> <p>The transcription request object containing the audio data to be encoded.</p> required <p>Returns:</p> Type Description <code>Tokenized</code> <p>The tokenized representation of the audio data, including processed audio and tokens</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_transcription(self, request: TranscriptionRequest) -&gt; Tokenized:\n    r\"\"\"\n    Encodes an audio transcription request into a tokenized format.\n\n    This method processes a transcription request containing audio data,\n    encodes the user message, and returns the tokenized output.\n\n    Args:\n        request: The transcription request object containing\n            the audio data to be encoded.\n\n    Returns:\n        Tokenized: The tokenized representation of the audio data, including processed audio and tokens\n    \"\"\"\n    assert self.audio_encoder is not None, f\"Audio encoder must be defined, got {self.audio_encoder=}\"\n    if self.audio_encoder.audio_config.transcription_format == TranscriptionFormat.INSTRUCT:\n        return self._encode_instruct_transcription(request)\n    elif self.audio_encoder.audio_config.transcription_format == TranscriptionFormat.STREAMING:\n        return self._encode_streaming_transcription(request)\n\n    raise InvalidRequestException(\n        \"Transcription format should be one of 'instruct', 'streaming', got \"\n        f\"{self.audio_encoder.audio_config.transcription_format=}.\"\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_user_content","title":"<code>encode_user_content(content, is_last, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | list[UserContentChunk]</code> <p>The content to encode.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>system_prompt</code> <code>str | None</code> <p>The system prompt.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and the images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_content(\n    self,\n    content: str | list[UserContentChunk],\n    is_last: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user content.\n\n    Args:\n        content: The content to encode.\n        is_last: Whether the message is the last one.\n        system_prompt: The system prompt.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the images.\n    \"\"\"\n    assert system_prompt is None, \"in Tokenizer V7 we don't encode system prompts in user messages\"\n\n    if isinstance(content, str):\n        return super().encode_user_content(content, is_last, system_prompt)\n\n    has_one_img_one_text_first = len(content) == 2 and isinstance(content[1], (ImageChunk, ImageURLChunk))\n    if force_img_first and has_one_img_one_text_first:\n        # make sure that if exactly one image and text chunk are passed we force the image chunk to be first\n        content = [content[1], content[0]]\n\n    tokens, images, audio = self._encode_content_chunks(content)\n    return tokens, images, audio\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.encode_user_message","title":"<code>encode_user_message(message, available_tools, is_last, is_first, system_prompt=None, force_img_first=False)</code>","text":"<p>Encode a user message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>UserMessage</code> <p>The message to encode.</p> required <code>available_tools</code> <code>list[Tool] | None</code> <p>The list of available tools if any.</p> required <code>is_last</code> <code>bool</code> <p>Whether the message is the last one.</p> required <code>is_first</code> <code>bool</code> <p>Whether the message is the first one.</p> required <code>system_prompt</code> <code>str | None</code> <p>Not used.</p> <code>None</code> <code>force_img_first</code> <code>bool</code> <p>Whether to force the image to be first.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[ndarray], list[Audio]]</code> <p>The encoded tokens and the list of images.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>def encode_user_message(\n    self,\n    message: UserMessage,\n    available_tools: list[Tool] | None,\n    is_last: bool,\n    is_first: bool,\n    system_prompt: str | None = None,\n    force_img_first: bool = False,\n) -&gt; tuple[list[int], list[np.ndarray], list[Audio]]:\n    r\"\"\"Encode a user message.\n\n    Args:\n        message: The message to encode.\n        available_tools: The list of available tools if any.\n        is_last: Whether the message is the last one.\n        is_first: Whether the message is the first one.\n        system_prompt: Not used.\n        force_img_first: Whether to force the image to be first.\n\n    Returns:\n        The encoded tokens and the list of images.\n    \"\"\"\n    assert system_prompt is None, \"in Tokenizer V7 we don't encode system prompts in user messages\"\n\n    tokens, images, audio = super().encode_user_message(\n        message,\n        available_tools,\n        is_last=is_last,\n        is_first=is_first,\n        system_prompt=None,\n        force_img_first=force_img_first,\n    )\n\n    return tokens, images, audio\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/instruct/#mistral_common.tokens.tokenizers.instruct.InstructTokenizerV7.validate_messages","title":"<code>validate_messages(messages)</code>  <code>classmethod</code>","text":"<p>Validates that system prompts and audio chunks are not used together in v7.</p> Source code in <code>src/mistral_common/tokens/tokenizers/instruct.py</code> <pre><code>@classmethod\ndef validate_messages(cls, messages: list[UATS]) -&gt; None:\n    r\"\"\"Validates that system prompts and audio chunks are not used together in v7.\"\"\"\n    if cls._has_audio(messages):\n        if any(isinstance(message, SystemMessage) for message in messages):\n            raise ValueError(\"System messages are not yet allowed when audio is present\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/","title":"mistral","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral","title":"<code>mistral_common.tokens.tokenizers.mistral</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer","title":"<code>MistralTokenizer(instruct_tokenizer, validator, request_normalizer)</code>","text":"<p>               Bases: <code>Generic[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, TokenizedType]</code></p> <p>Mistral tokenizer.</p> <p>This class is a wrapper around a InstructTokenizer, a MistralRequestValidator and a InstructRequestNormalizer.</p> <p>It provides a convenient interface to tokenize, validate ad normalize Mistral requests.</p> <p>Attributes:</p> Name Type Description <code>instruct_tokenizer</code> <code>InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType]</code> <p>The instruct tokenizer to use. See InstructTokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>instruct_tokenizer</code> <code>InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType]</code> <p>The instruct tokenizer to use.</p> required <code>validator</code> <code>MistralRequestValidator[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType]</code> <p>The request validator to use.</p> required <code>request_normalizer</code> <code>InstructRequestNormalizer[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, InstructRequestType]</code> <p>The request normalizer to use.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def __init__(\n    self,\n    instruct_tokenizer: InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType],\n    validator: MistralRequestValidator[UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType],\n    request_normalizer: InstructRequestNormalizer[\n        UserMessageType, AssistantMessageType, ToolMessageType, SystemMessageType, InstructRequestType\n    ],\n):\n    r\"\"\"Initializes a `MistralTokenizer`.\n\n    Args:\n        instruct_tokenizer: The instruct tokenizer to use.\n        validator: The request validator to use.\n        request_normalizer: The request normalizer to use.\n    \"\"\"\n    self._chat_completion_request_validator = validator\n    self._instruct_request_normalizer = request_normalizer\n    self.instruct_tokenizer: InstructTokenizer[InstructRequest, FIMRequest, TokenizedType, AssistantMessageType] = (\n        instruct_tokenizer\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.mode","title":"<code>mode</code>  <code>property</code>","text":"<p>The validation mode of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.version","title":"<code>version</code>  <code>property</code>","text":"<p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.__reduce__","title":"<code>__reduce__()</code>","text":"<p>Provides a recipe for pickling (serializing) this object, which is necessary for use with multiprocessing.</p> <p>Returns:</p> Type Description <code>tuple[Callable, tuple[Any, ...]]</code> <p>A tuple of the factory function and the arguments to reconstruct the object from its source file.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def __reduce__(self) -&gt; tuple[Callable, tuple[Any, ...]]:\n    \"\"\"\n    Provides a recipe for pickling (serializing) this object, which is necessary for use with multiprocessing.\n\n    Returns:\n        A tuple of the factory function and the arguments to reconstruct the object from its source file.\n    \"\"\"\n    return MistralTokenizer.from_file, (self.instruct_tokenizer.tokenizer.file_path, self.mode)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decodes a list of tokens into a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The tokens to decode.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy to use for special tokens. Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def decode(self, tokens: list[int], special_token_policy: SpecialTokenPolicy | None = None) -&gt; str:\n    r\"\"\"Decodes a list of tokens into a string.\n\n    Args:\n        tokens: The tokens to decode.\n        special_token_policy: The policy to use for special tokens. Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    return self.instruct_tokenizer.decode(tokens, special_token_policy=special_token_policy)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.encode_chat_completion","title":"<code>encode_chat_completion(request, max_model_input_len=None)</code>","text":"<p>Encodes a chat completion request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ChatCompletionRequest[UATS]</code> <p>The chat completion request to encode.</p> required <code>max_model_input_len</code> <code>int | None</code> <p>The maximum length of the input to the model. If <code>None</code>, the input will not be truncated.</p> <code>None</code> <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The encoded chat completion request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def encode_chat_completion(\n    self, request: ChatCompletionRequest[UATS], max_model_input_len: int | None = None\n) -&gt; TokenizedType:\n    r\"\"\"Encodes a chat completion request.\n\n    Args:\n        request: The chat completion request to encode.\n        max_model_input_len: The maximum length of the input to the model.\n            If `None`, the input will not be truncated.\n\n    Returns:\n        The encoded chat completion request.\n    \"\"\"\n\n    validated_request = self._chat_completion_request_validator.validate_request(request)\n\n    if max_model_input_len is None and request.truncate_for_context_length:\n        # the max_model_input_len arg should not be optional ;\n        # but this function is used in many small scripts that have no use\n        # for truncation, and don't provide the max model len\n        raise TokenizerException(\n            \"encoding a chat completion request with truncation, but no max model len was provided\",\n        )\n\n    instruct_request = self._instruct_request_normalizer.from_chat_completion_request(validated_request)\n\n    if request.truncate_for_context_length:\n        instruct_request.truncate_at_max_tokens = max_model_input_len\n\n    return self.instruct_tokenizer.encode_instruct(instruct_request)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.encode_fim","title":"<code>encode_fim(request)</code>","text":"<p>Encodes a fill in the middle request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FIMRequest</code> <p>The fill in the middle request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The encoded fill in the middle request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def encode_fim(self, request: FIMRequest) -&gt; TokenizedType:\n    r\"\"\"Encodes a fill in the middle request.\n\n    Args:\n        request: The fill in the middle request to encode.\n\n    Returns:\n        The encoded fill in the middle request.\n    \"\"\"\n    return self.instruct_tokenizer.encode_fim(request)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.encode_transcription","title":"<code>encode_transcription(request)</code>","text":"<p>Encodes a transcription request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>TranscriptionRequest</code> <p>The transcription request to encode.</p> required <p>Returns:</p> Type Description <code>TokenizedType</code> <p>The encoded transcription request.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def encode_transcription(self, request: TranscriptionRequest) -&gt; TokenizedType:\n    r\"\"\"Encodes a transcription request.\n\n    Args:\n        request: The transcription request to encode.\n\n    Returns:\n        The encoded transcription request.\n    \"\"\"\n    return self.instruct_tokenizer.encode_transcription(request)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.from_file","title":"<code>from_file(tokenizer_filename, mode=ValidationMode.test)</code>  <code>classmethod</code>","text":"<p>Loads a tokenizer from a file.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer_filename</code> <code>str | Path</code> <p>The path to the tokenizer file.</p> required <code>mode</code> <code>ValidationMode</code> <p>The validation mode to use.</p> <code>test</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The loaded tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    tokenizer_filename: str | Path,\n    mode: ValidationMode = ValidationMode.test,\n) -&gt; \"MistralTokenizer\":\n    r\"\"\"Loads a tokenizer from a file.\n\n    Args:\n        tokenizer_filename: The path to the tokenizer file.\n        mode: The validation mode to use.\n\n    Returns:\n        The loaded tokenizer.\n    \"\"\"\n    tokenizer: SentencePieceTokenizer | Tekkenizer\n\n    if is_tekken(tokenizer_filename):\n        tokenizer = Tekkenizer.from_file(tokenizer_filename)\n        image_config = tokenizer.image\n        audio_config = tokenizer.audio\n    elif is_sentencepiece(tokenizer_filename):\n        tokenizer = SentencePieceTokenizer(tokenizer_filename)\n        image_config = get_image_config(tokenizer_filename)\n        # spm can't have audio\n        audio_config = None\n    else:\n        raise TokenizerException(f\"Unrecognized tokenizer file: {tokenizer_filename}\")\n\n    image_encoder = load_image_encoder(image_config, tokenizer) if image_config is not None else None\n\n    audio_encoder = None\n    if audio_config is not None:\n        assert isinstance(tokenizer, Tekkenizer), \"Audio is only supported for tekken tokenizers\"\n        audio_encoder = load_audio_encoder(audio_config, tokenizer)\n\n    request_normalizer = normalizer_for_tokenizer_version(tokenizer.version)\n\n    if tokenizer.version == TokenizerVersion.v1:\n        assert image_encoder is None, \"Tokenizer version needs to be &gt;= v3\"\n        assert audio_encoder is None, \"Tokenizer version needs to be &gt;= v7\"\n        return MistralTokenizer(\n            InstructTokenizerV1(tokenizer),\n            validator=MistralRequestValidator(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v2:\n        assert image_encoder is None, \"Tokenizer version needs to be &gt;= v3\"\n        assert audio_encoder is None, \"Tokenizer version needs to be &gt;= v7\"\n        return MistralTokenizer(\n            InstructTokenizerV2(tokenizer),\n            validator=MistralRequestValidator(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v3:\n        assert audio_encoder is None, \"Tokenizer version needs to be &gt;= v7\"\n        return MistralTokenizer(\n            InstructTokenizerV3(tokenizer, image_encoder=image_encoder),\n            validator=MistralRequestValidatorV3(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v7:\n        return MistralTokenizer(\n            InstructTokenizerV7(tokenizer, image_encoder=image_encoder, audio_encoder=audio_encoder),\n            validator=MistralRequestValidatorV5(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v11:\n        return MistralTokenizer(\n            InstructTokenizerV11(tokenizer, image_encoder=image_encoder, audio_encoder=audio_encoder),\n            validator=MistralRequestValidatorV5(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n    elif tokenizer.version == TokenizerVersion.v13:\n        return MistralTokenizer(\n            InstructTokenizerV13(tokenizer, image_encoder=image_encoder, audio_encoder=audio_encoder),\n            validator=MistralRequestValidatorV13(mode=mode),\n            request_normalizer=request_normalizer,\n        )\n\n    raise TokenizerException(f\"Unrecognized tokenizer filename: {tokenizer_filename}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.from_hf_hub","title":"<code>from_hf_hub(repo_id, token=None, revision=None, force_download=False, local_files_only=False, mode=ValidationMode.test)</code>  <code>staticmethod</code>","text":"<p>Download the Mistral tokenizer for a given Hugging Face repository ID.</p> <p>See here for a list of our OSS models.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face repo ID.</p> required <code>token</code> <code>bool | str | None</code> <p>The Hugging Face token to use to download the tokenizer.</p> <code>None</code> <code>revision</code> <code>str | None</code> <p>The revision of the model to use. If <code>None</code>, the latest revision will be used.</p> <code>None</code> <code>mode</code> <code>ValidationMode</code> <p>The validation mode to use.</p> <code>test</code> <code>force_download</code> <code>bool</code> <p>Whether to force the download of the tokenizer. If <code>True</code>, the tokenizer will be downloaded even if it is already cached.</p> <code>False</code> <code>local_files_only</code> <code>bool</code> <p>Whether to only use local files. If <code>True</code>, the tokenizer will be downloaded only if it is already cached.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer for the given model.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@staticmethod\ndef from_hf_hub(\n    repo_id: str,\n    token: bool | str | None = None,\n    revision: str | None = None,\n    force_download: bool = False,\n    local_files_only: bool = False,\n    mode: ValidationMode = ValidationMode.test,\n) -&gt; \"MistralTokenizer\":\n    r\"\"\"Download the Mistral tokenizer for a given Hugging Face repository ID.\n\n    See [here](https://huggingface.co/mistralai/models) for a list of our OSS models.\n\n    Args:\n        repo_id: The Hugging Face repo ID.\n        token: The Hugging Face token to use to download the tokenizer.\n        revision: The revision of the model to use. If `None`, the latest revision will be used.\n        mode: The validation mode to use.\n        force_download: Whether to force the download of the tokenizer. If `True`, the tokenizer will be downloaded\n            even if it is already cached.\n        local_files_only: Whether to only use local files. If `True`, the tokenizer will be downloaded only if it is\n            already cached.\n\n    Returns:\n        The Mistral tokenizer for the given model.\n    \"\"\"\n    tokenizer_path = download_tokenizer_from_hf_hub(\n        repo_id=repo_id,\n        token=token,\n        revision=revision,\n        force_download=force_download,\n        local_files_only=local_files_only,\n    )\n    return MistralTokenizer.from_file(tokenizer_path, mode=mode)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.from_model","title":"<code>from_model(model, strict=False)</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model name.</p> required <code>strict</code> <code>bool</code> <p>Whether to use strict model name matching. If <code>False</code>, the model name is matched as a substring. This is deprecated and will be removed in <code>mistral_common=1.10.0</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer for the given model.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef from_model(cls, model: str, strict: bool = False) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer for a given model.\n\n    Args:\n        model: The model name.\n        strict: Whether to use strict model name matching. If `False`, the model name is matched as a substring.\n            This is deprecated and will be removed in `mistral_common=1.10.0`.\n\n    Returns:\n        The Mistral tokenizer for the given model.\n    \"\"\"\n    if not strict:\n        warnings.warn(\n            \"Calling `MistralTokenizer.from_model(..., strict=False)` is deprecated as it can lead to incorrect \"\n            \"tokenizers. It is strongly recommended to use MistralTokenizer.from_model(..., strict=True)` \"\n            \"which will become the default in `mistral_common=1.10.0`.\"\n            \"If you are using `mistral_common` for open-sourced model weights, we recommend using \"\n            \"`MistralTokenizer.from_file('&lt;path/to/tokenizer/file&gt;')` instead.\",\n            FutureWarning,\n        )\n\n        # TODO(Delete this code in mistral_common &gt;= 1.10.0\n        # Prefix search the model name mapping\n        for model_name, tokenizer_cls in MODEL_NAME_TO_TOKENIZER_CLS.items():\n            if model_name in model.lower():\n                return tokenizer_cls()\n\n    if model not in MODEL_NAME_TO_TOKENIZER_CLS:\n        raise TokenizerException(f\"Unrecognized model: {model}\")\n\n    return MODEL_NAME_TO_TOKENIZER_CLS[model]()\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v1","title":"<code>v1()</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v1.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v1(cls) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer v1.\"\"\"\n    return cls.from_file(str(cls._data_path() / \"tokenizer.model.v1\"), mode=ValidationMode.test)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v2","title":"<code>v2()</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v2.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v2(cls) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer v2.\"\"\"\n    return cls.from_file(\n        str(cls._data_path() / \"mistral_instruct_tokenizer_240216.model.v2\"), mode=ValidationMode.test\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v3","title":"<code>v3(is_tekken=False, is_mm=False)</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v3.</p> <p>Parameters:</p> Name Type Description Default <code>is_tekken</code> <code>bool</code> <p>Whether the tokenizer is a tekken tokenizer. See Tekkenizer.</p> <code>False</code> <code>is_mm</code> <code>bool</code> <p>Whether to load image tokenizer.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer v3.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v3(cls, is_tekken: bool = False, is_mm: bool = False) -&gt; \"MistralTokenizer\":\n    r\"\"\"Get the Mistral tokenizer v3.\n\n    Args:\n        is_tekken: Whether the tokenizer is a tekken tokenizer. See\n            [Tekkenizer][mistral_common.tokens.tokenizers.tekken.Tekkenizer].\n        is_mm: Whether to load image tokenizer.\n\n    Returns:\n        The Mistral tokenizer v3.\n    \"\"\"\n    if is_tekken and is_mm:\n        tokenizer_name = \"tekken_240911.json\"\n    elif is_tekken and not is_mm:\n        tokenizer_name = \"tekken_240718.json\"\n    elif not is_tekken and is_mm:\n        raise ValueError(\"Multimodal tokenizer is currently only supported for tekken\")\n    else:\n        tokenizer_name = \"mistral_instruct_tokenizer_240323.model.v3\"\n\n    return cls.from_file(str(cls._data_path() / tokenizer_name), mode=ValidationMode.test)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.MistralTokenizer.v7","title":"<code>v7(is_mm=False)</code>  <code>classmethod</code>","text":"<p>Get the Mistral tokenizer v7.</p> <p>Parameters:</p> Name Type Description Default <code>is_mm</code> <code>bool</code> <p>Whether to load the image tokenizer.</p> <code>False</code> <p>Returns:</p> Type Description <code>MistralTokenizer</code> <p>The Mistral tokenizer v7.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>@classmethod\ndef v7(cls, is_mm: bool = False) -&gt; \"MistralTokenizer\":\n    \"\"\"Get the Mistral tokenizer v7.\n\n    Args:\n        is_mm: Whether to load the image tokenizer.\n\n    Returns:\n        The Mistral tokenizer v7.\n    \"\"\"\n    if is_mm:\n        return cls.from_file(\n            str(cls._data_path() / \"mistral_instruct_tokenizer_241114.model.v7m1\"), mode=ValidationMode.test\n        )\n    else:\n        return cls.from_file(\n            str(cls._data_path() / \"mistral_instruct_tokenizer_241114.model.v7\"), mode=ValidationMode.test\n        )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.load_audio_encoder","title":"<code>load_audio_encoder(audio_config, tokenizer)</code>","text":"<p>Load a audio encoder from a config and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>audio_config</code> <code>AudioConfig</code> <p>The audio config.</p> required <code>tokenizer</code> <code>Tekkenizer</code> <p>The tokenizer.</p> required <p>Returns:</p> Type Description <code>AudioEncoder</code> <p>The audio encoder.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def load_audio_encoder(audio_config: AudioConfig, tokenizer: Tekkenizer) -&gt; AudioEncoder:\n    r\"\"\"Load a audio encoder from a config and a tokenizer.\n\n    Args:\n        audio_config: The audio config.\n        tokenizer: The tokenizer.\n\n    Returns:\n        The audio encoder.\n    \"\"\"\n\n    def get_special_token_or_none(token: str) -&gt; int | None:\n        if not tokenizer.is_special(token):\n            return None\n\n        return tokenizer.get_special_token(token)\n\n    special_ids = SpecialAudioIDs(\n        audio=get_special_token_or_none(SpecialTokens.audio.value),\n        begin_audio=get_special_token_or_none(SpecialTokens.begin_audio.value),\n        streaming_pad=get_special_token_or_none(SpecialTokens.streaming_pad.value),\n    )\n    return AudioEncoder(audio_config, special_ids)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/mistral/#mistral_common.tokens.tokenizers.mistral.load_image_encoder","title":"<code>load_image_encoder(image_config, tokenizer)</code>","text":"<p>Load a image encoder from a config and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>image_config</code> <code>ImageConfig</code> <p>The image config.</p> required <code>tokenizer</code> <code>Tekkenizer | SentencePieceTokenizer</code> <p>The tokenizer.</p> required <p>Returns:</p> Type Description <code>ImageEncoder</code> <p>The image encoder.</p> Source code in <code>src/mistral_common/tokens/tokenizers/mistral.py</code> <pre><code>def load_image_encoder(image_config: ImageConfig, tokenizer: Tekkenizer | SentencePieceTokenizer) -&gt; ImageEncoder:\n    r\"\"\"Load a image encoder from a config and a tokenizer.\n\n    Args:\n        image_config: The image config.\n        tokenizer: The tokenizer.\n\n    Returns:\n        The image encoder.\n    \"\"\"\n    special_ids = SpecialImageIDs(\n        img=tokenizer.get_special_token(SpecialTokens.img.value),\n        img_break=tokenizer.get_special_token(SpecialTokens.img_break.value),\n        img_end=tokenizer.get_special_token(SpecialTokens.img_end.value),\n    )\n    return ImageEncoder(image_config, special_ids)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/","title":"sentencepiece","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece","title":"<code>mistral_common.tokens.tokenizers.sentencepiece</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer","title":"<code>SentencePieceTokenizer(model_path, tokenizer_version=None)</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>SentencePiece tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path</code> <p>The path to the <code>SentencePiece</code> model.</p> required <code>tokenizer_version</code> <code>TokenizerVersion | None</code> <p>The version of the tokenizer. If not provided, it will be inferred from the model path.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def __init__(self, model_path: str | Path, tokenizer_version: TokenizerVersion | None = None) -&gt; None:\n    r\"\"\"Initialize the `SentencePieceTokenizer`.\n\n    Args:\n        model_path: The path to the `SentencePiece` model.\n        tokenizer_version: The version of the tokenizer. If not provided, it will be inferred from the model path.\n    \"\"\"\n    assert_sentencepiece_installed()\n\n    self._logger = logging.getLogger(self.__class__.__name__)\n    # reload tokenizer\n    assert os.path.isfile(model_path), model_path\n    self._model = SentencePieceProcessor(\n        model_file=model_path if isinstance(model_path, str) else model_path.as_posix()\n    )\n\n    assert self._model.vocab_size() == self._model.get_piece_size()\n    self._vocab = [self._model.id_to_piece(i) for i in range(self.n_words)]\n\n    self._version: TokenizerVersion = tokenizer_version or get_spm_version(model_path, raise_deprecated=False)\n\n    self._file_path = Path(model_path)\n    super().__init__()\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.bos_id","title":"<code>bos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The beginning of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.eos_id","title":"<code>eos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The end of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>The path to the tokenizer model.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.n_words","title":"<code>n_words</code>  <code>property</code>","text":"<p>Vocabulary size of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.num_special_tokens","title":"<code>num_special_tokens</code>  <code>property</code>","text":"<p>The number of special tokens of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.pad_id","title":"<code>pad_id</code>  <code>property</code>","text":"<p>The padding token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.special_ids","title":"<code>special_ids</code>  <code>cached</code> <code>property</code>","text":"<p>Ids of the special tokens.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.unk_id","title":"<code>unk_id</code>  <code>property</code>","text":"<p>The unknown token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.version","title":"<code>version</code>  <code>property</code>","text":"<p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decode the given list of token ids into a string.</p> Note <p>Using <code>special_token_policy=SpecialTokenPolicy.KEEP</code> will keep the special tokens and the normal tokens as SentencePiece pieces.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The list of token ids.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy to use for special tokens. If <code>None</code>, the default policy is <code>SpecialTokenPolicy.IGNORE</code>.  Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def decode(self, tokens: list[int], special_token_policy: SpecialTokenPolicy | None = None) -&gt; str:\n    r\"\"\"Decode the given list of token ids into a string.\n\n    Note:\n        Using `special_token_policy=SpecialTokenPolicy.KEEP` will keep the special tokens and the normal tokens as\n        SentencePiece pieces.\n\n    Args:\n        tokens: The list of token ids.\n        special_token_policy: The policy to use for special tokens. If `None`, the default policy\n            is `SpecialTokenPolicy.IGNORE`.  Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    if special_token_policy is not None and not isinstance(special_token_policy, (str, SpecialTokenPolicy)):\n        raise ValueError(\n            f\"Expected `special_token_policy` to be None or SpecialTokenPolicy, got {type(special_token_policy)}.\"\n        )\n\n    if special_token_policy is None:\n        warnings.warn(\n            (\n                \"Using the tokenizer's special token policy `None` is deprecated. \"\n                \"It will be removed in 1.10.0. \"\n                \"Please pass a special token policy explicitly. \"\n                \"Future default will be SpecialTokenPolicy.IGNORE.\"\n            ),\n            FutureWarning,\n        )\n        special_token_policy = SpecialTokenPolicy.IGNORE\n\n    if special_token_policy in [SpecialTokenPolicy.KEEP, SpecialTokenPolicy.RAISE]:\n        return self._decode_with_special_tokens(tokens, special_token_policy)\n\n    return self._model.decode(tokens)  # type: ignore\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.encode","title":"<code>encode(s, bos, eos)</code>","text":"<p>Encode the given string into a list of token ids.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to encode.</p> required <code>bos</code> <code>bool</code> <p>Whether to add the beginning of sentence token.</p> required <code>eos</code> <code>bool</code> <p>Whether to add the end of sentence token.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The list of token ids.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def encode(self, s: str, bos: bool, eos: bool) -&gt; list[int]:\n    r\"\"\"Encode the given string into a list of token ids.\n\n    Args:\n        s: The string to encode.\n        bos: Whether to add the beginning of sentence token.\n        eos: Whether to add the end of sentence token.\n\n    Returns:\n        The list of token ids.\n    \"\"\"\n    assert isinstance(s, str)\n    t: list[int] = self._model.encode(s)\n    if bos:\n        t = [self.bos_id, *t]\n    if eos:\n        t = [*t, self.eos_id]\n    return t\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.get_special_token","title":"<code>get_special_token(s)</code>","text":"<p>Get the special token for the given string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def get_special_token(self, s: str) -&gt; int:\n    r\"\"\"Get the special token for the given string.\"\"\"\n    return self._model.piece_to_id(s)  # type: ignore\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.id_to_piece","title":"<code>id_to_piece(token_id)</code>","text":"<p>Convert the given token id to a token piece.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def id_to_piece(self, token_id: int) -&gt; str:\n    r\"\"\"Convert the given token id to a token piece.\"\"\"\n    return self._model.id_to_piece(token_id)  # type: ignore\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.is_special","title":"<code>is_special(token)</code>","text":"<p>Return <code>True</code> if the passed <code>token</code> is a special token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def is_special(self, token: int | np.integer | str) -&gt; bool:\n    \"\"\"Return `True` if the passed `token` is a special token.\"\"\"\n    if isinstance(token, (int, np.integer)):\n        return self._model.IsControl(int(token))  # type: ignore\n    elif isinstance(token, str):\n        token_int = self._model.piece_to_id(token)\n        return self._model.IsControl(token_int)  # type: ignore\n    else:\n        raise TypeError(f\"Expected int or str, got {type(token).__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.to_string","title":"<code>to_string(tokens)</code>","text":"<p>[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.</p> <p>Use <code>decode</code> with <code>special_token_policy=SpecialTokenPolicy.KEEP</code> instead.</p> <p>This is a convenient method for debugging.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def to_string(self, tokens: list[int]) -&gt; str:\n    r\"\"\"[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.\n\n    Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\n\n    This is a convenient method for debugging.\n    \"\"\"\n    warnings.warn(\n        (\n            \"`to_string` is deprecated and will be removed in 1.10.0. \"\n            \"Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\"\n        ),\n        FutureWarning,\n    )\n    return self._to_string(tokens)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.SentencePieceTokenizer.vocab","title":"<code>vocab()</code>","text":"<p>Get all tokens in the vocabulary as strings.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def vocab(self) -&gt; list[str]:\n    r\"\"\"Get all tokens in the vocabulary as strings.\"\"\"\n    return self._vocab\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.get_image_config","title":"<code>get_image_config(tokenizer_filename)</code>","text":"<p>Get the image config from the tokenizer filename.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def get_image_config(tokenizer_filename: str | Path) -&gt; ImageConfig | None:\n    r\"\"\"Get the image config from the tokenizer filename.\"\"\"\n    tokenizer_filename = str(tokenizer_filename)\n\n    _version_str = tokenizer_filename.split(\".\")[-1]\n    if _version_str == \"model\" or \"m\" not in _version_str:\n        return None\n\n    _mm_version_str = \"m\" + _version_str.split(\"m\")[-1]\n\n    if _mm_version_str not in MultiModalVersion.__members__:\n        raise TokenizerException(f\"Unrecognized tokenizer filename: {tokenizer_filename}\")\n\n    return MultiModalVersion(_mm_version_str).config\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.get_spm_version","title":"<code>get_spm_version(tokenizer_filename, raise_deprecated=False)</code>","text":"<p>Get the version of the tokenizer from the filename.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def get_spm_version(tokenizer_filename: str | Path, raise_deprecated: bool = False) -&gt; TokenizerVersion:\n    r\"\"\"Get the version of the tokenizer from the filename.\"\"\"\n    tokenizer_filename = str(tokenizer_filename)\n\n    _version_str = tokenizer_filename.split(\".\")[-1]\n    if _version_str != \"model\":  # filter tokenizer_filename == \"/path/to/tokenizer.model\" case\n        _version_str = _version_str.split(\"m\")[0]\n\n    if _version_str == \"model\":\n        if raise_deprecated:\n            raise TokenizerException(f\"Make sure to rename your tokenizer file to end with {tokenizer_filename}.v1.\")\n\n        # tokenizer.model =&gt; tokenizer.model.v1\n        return TokenizerVersion(\"v1\")\n\n    if _version_str not in TokenizerVersion.__members__:\n        raise TokenizerException(f\"Unrecognized tokenizer filename: {tokenizer_filename}\")\n\n    return TokenizerVersion(_version_str)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/sentencepiece/#mistral_common.tokens.tokenizers.sentencepiece.is_sentencepiece","title":"<code>is_sentencepiece(path)</code>","text":"<p>Check if the given path is a SentencePiece model.</p> Source code in <code>src/mistral_common/tokens/tokenizers/sentencepiece.py</code> <pre><code>def is_sentencepiece(path: str | Path) -&gt; bool:\n    r\"\"\"Check if the given path is a SentencePiece model.\"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n\n    instruct_versions = list(TokenizerVersion.__members__)\n    mm_versions = list(MultiModalVersion.__members__) + [\"\"]  # allow no mm version\n    suffixes = [f\".model.{v}{m}\" for v in instruct_versions for m in mm_versions] + [\".model\"]\n\n    return path.is_file() and any(path.name.endswith(suffix) for suffix in suffixes)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/","title":"tekken","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken","title":"<code>mistral_common.tokens.tokenizers.tekken</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.ModelData","title":"<code>ModelData</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The data of the tekken tokenizer model.</p> <p>Attributes:</p> Name Type Description <code>vocab</code> <code>list[TokenInfo]</code> <p>The vocabulary of the tokenizer.</p> <code>config</code> <code>TekkenConfig</code> <p>The configuration of the tokenizer.</p> <code>version</code> <code>int</code> <p>The version of the tokenizer.</p> <code>type</code> <code>str</code> <p>The type of the tokenizer.</p> <code>image</code> <code>ImageConfig</code> <p>The image configuration of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.SpecialTokenInfo","title":"<code>SpecialTokenInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Special token information in the JSON file.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>The rank of the token.</p> <code>token_str</code> <code>str</code> <p>The token in string format.</p> <code>is_control</code> <code>bool</code> <p>Whether the token is a control token.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.TekkenConfig","title":"<code>TekkenConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tekken configuration in the JSON file.</p> <p>Attributes:</p> Name Type Description <code>pattern</code> <code>str</code> <p>The pattern of the tokenizer.</p> <code>num_vocab_tokens</code> <code>int</code> <p>The number of vocabulary tokens.</p> <code>default_vocab_size</code> <code>int</code> <p>The default vocabulary size.</p> <code>default_num_special_tokens</code> <code>int</code> <p>The default number of special tokens.</p> <code>version</code> <code>str</code> <p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer","title":"<code>Tekkenizer(vocab, special_tokens, pattern, vocab_size, num_special_tokens, version, *, name='tekkenizer', _path=None, image_config=None, audio_config=None)</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Tekken tokenizer.</p> <p>This tokenizer is based on the tiktoken library. It fastens the tokenization for multiple languages.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list[TokenInfo]</code> <p>The vocabulary of the tokenizer.</p> required <code>special_tokens</code> <code>list[SpecialTokenInfo]</code> <p>The special tokens of the tokenizer.</p> required <code>pattern</code> <code>str</code> <p>The pattern of the tokenizer.</p> required <code>vocab_size</code> <code>int</code> <p>The vocabulary size of the tokenizer.</p> required <code>num_special_tokens</code> <code>int</code> <p>The number of special tokens of the tokenizer.</p> required <code>version</code> <code>TokenizerVersion</code> <p>The version of the tokenizer.</p> required <code>name</code> <code>str</code> <p>The name of the tokenizer.</p> <code>'tekkenizer'</code> <code>image_config</code> <code>ImageConfig | None</code> <p>The image configuration of the tokenizer.</p> <code>None</code> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def __init__(\n    self,\n    vocab: list[TokenInfo],\n    special_tokens: list[SpecialTokenInfo],\n    pattern: str,\n    vocab_size: int,\n    num_special_tokens: int,\n    version: TokenizerVersion,\n    *,\n    name: str = \"tekkenizer\",\n    _path: str | Path | None = None,\n    image_config: ImageConfig | None = None,\n    audio_config: AudioConfig | None = None,\n):\n    r\"\"\"Initialize the tekken tokenizer.\n\n    Args:\n        vocab: The vocabulary of the tokenizer.\n        special_tokens: The special tokens of the tokenizer.\n        pattern: The pattern of the tokenizer.\n        vocab_size: The vocabulary size of the tokenizer.\n        num_special_tokens: The number of special tokens of the tokenizer.\n        version: The version of the tokenizer.\n        name: The name of the tokenizer.\n        image_config: The image configuration of the tokenizer.\n    \"\"\"\n    assert vocab_size &lt;= len(vocab) + num_special_tokens, (\n        vocab_size,\n        len(vocab),\n        num_special_tokens,\n    )\n    self._vocab_size = vocab_size\n\n    # The number of special tokens defined in the tokenizer json\n    num_defined_special_tokens = len(set([t[\"token_str\"] for t in special_tokens]))\n\n    assert len(special_tokens) == num_defined_special_tokens, f\"Special tokens must be unique: {special_tokens}\"\n    assert len(special_tokens) &lt;= num_special_tokens\n\n    special_filler = [\n        SpecialTokenInfo(rank=i, token_str=self.SPECIAL_TOKEN_TEMPLATE.format(id=i), is_control=True)\n        for i in range(len(special_tokens), num_special_tokens)\n    ]\n    if special_filler:\n        logger.info(\n            f\"Adding special tokens {special_filler[0]['token_str']}, ..., {special_filler[-1]['token_str']}\"\n        )\n    special_tokens = special_tokens + special_filler\n\n    assert len(set([t[\"token_str\"] for t in special_tokens])) == len(special_tokens) == num_special_tokens, (\n        special_tokens\n    )\n    inner_vocab_size = vocab_size - num_special_tokens\n\n    # reload vocab\n    logger.info(f\"Non special vocabulary size is {inner_vocab_size} with {num_special_tokens} special tokens.\")\n    self._tekken_token2id_nospecial = _reload_mergeable_ranks(vocab, max_vocab=inner_vocab_size)\n    assert set(range(inner_vocab_size)) == set(self._tekken_token2id_nospecial.values()), (\n        inner_vocab_size,\n        self._tekken_token2id_nospecial,\n    )\n    self._model = tiktoken.Encoding(\n        name=name,\n        pat_str=pattern,\n        mergeable_ranks=self._tekken_token2id_nospecial,\n        special_tokens={},  # special tokens are handled manually\n    )\n\n    self._version = version\n\n    self._image_config = image_config\n    self._audio_config = audio_config\n\n    self._all_special_tokens = special_tokens\n    self._special_token_ids = {t[\"rank\"] for t in special_tokens}\n    self._special_tokens_reverse_vocab = {t[\"token_str\"]: t[\"rank\"] for t in special_tokens}\n    self._vocab = [self.id_to_piece(i) for i in range(vocab_size)]\n    self._special_token_policy = SpecialTokenPolicy.IGNORE\n    self._file_path = Path(_path) if _path is not None else None\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.audio","title":"<code>audio</code>  <code>property</code> <code>writable</code>","text":"<p>The audio configuration of the tokenizer.</p> <p>Returns:</p> Type Description <code>AudioConfig | None</code> <p>The audio configuration object if it exists, otherwise None.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.bos_id","title":"<code>bos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The beginning of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.eos_id","title":"<code>eos_id</code>  <code>cached</code> <code>property</code>","text":"<p>The end of sentence token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>The path to the tokenizer file.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.image","title":"<code>image</code>  <code>property</code> <code>writable</code>","text":"<p>The image configuration of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.n_words","title":"<code>n_words</code>  <code>property</code>","text":"<p>Vocabulary size of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.num_special_tokens","title":"<code>num_special_tokens</code>  <code>property</code>","text":"<p>The number of special tokens of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.pad_id","title":"<code>pad_id</code>  <code>cached</code> <code>property</code>","text":"<p>The padding token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_ids","title":"<code>special_ids</code>  <code>cached</code> <code>property</code>","text":"<p>Ids of the special tokens.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_token_policy","title":"<code>special_token_policy</code>  <code>property</code> <code>writable</code>","text":"<p>The policy for handling special tokens.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.unk_id","title":"<code>unk_id</code>  <code>cached</code> <code>property</code>","text":"<p>The unknown token id.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.version","title":"<code>version</code>  <code>property</code>","text":"<p>The version of the tokenizer.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.decode","title":"<code>decode(tokens, special_token_policy=None)</code>","text":"<p>Decode a list of token ids into a string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The list of token ids to decode.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy for handling special tokens. Use the tokenizer's attribute if <code>None</code>. Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def decode(self, tokens: list[int], special_token_policy: SpecialTokenPolicy | None = None) -&gt; str:\n    r\"\"\"Decode a list of token ids into a string.\n\n    Args:\n        tokens: The list of token ids to decode.\n        special_token_policy: The policy for handling special tokens.\n            Use the tokenizer's [attribute][mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_token_policy]\n            if `None`. Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    if special_token_policy is not None and not isinstance(special_token_policy, (str, SpecialTokenPolicy)):\n        raise ValueError(\n            f\"Expected `special_token_policy` to be None or SpecialTokenPolicy, got {type(special_token_policy)}.\"\n        )\n\n    if special_token_policy is None:\n        warnings.warn(\n            (\n                f\"Using the tokenizer's special token policy ({self._special_token_policy}) is deprecated. \"\n                \"It will be removed in 1.10.0. \"\n                \"Please pass a special token policy explicitly. \"\n                \"Future default will be SpecialTokenPolicy.IGNORE.\"\n            ),\n            FutureWarning,\n        )\n        special_token_policy = self._special_token_policy\n\n    return \"\".join(self._decode_all(tokens, special_token_policy=special_token_policy))\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.encode","title":"<code>encode(s, bos, eos)</code>","text":"<p>Encode a string into a list of token ids.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to encode.</p> required <code>bos</code> <code>bool</code> <p>Whether to add the beginning of sentence token.</p> required <code>eos</code> <code>bool</code> <p>Whether to add the end of sentence token.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The list of token ids.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def encode(self, s: str, bos: bool, eos: bool) -&gt; list[int]:\n    r\"\"\"Encode a string into a list of token ids.\n\n    Args:\n        s: The string to encode.\n        bos: Whether to add the beginning of sentence token.\n        eos: Whether to add the end of sentence token.\n\n    Returns:\n        The list of token ids.\n    \"\"\"\n    tokens: list[int] = self._model.encode(s)\n    tokens = [t + self.num_special_tokens for t in tokens]\n    if bos:\n        tokens = [self.bos_id, *tokens]\n    if eos:\n        tokens = [*tokens, self.eos_id]\n    return tokens\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Load the tekken tokenizer from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The path to the tokenizer file.</p> required <p>Returns:</p> Type Description <code>Tekkenizer</code> <p>The tekken tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>@classmethod\ndef from_file(cls: type[\"Tekkenizer\"], path: str | Path) -&gt; \"Tekkenizer\":\n    r\"\"\"Load the tekken tokenizer from a file.\n\n    Args:\n        path: The path to the tokenizer file.\n\n    Returns:\n        The tekken tokenizer.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n    assert path.exists(), path\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        untyped = json.load(f)\n\n    _version_str = untyped[\"config\"].get(\"version\")\n    if _version_str not in TokenizerVersion.__members__:\n        raise ValueError(\n            f\"Unknown version: {_version_str} in {path}. \"\n            f\"Make sure to use a valid version string: {list(TokenizerVersion.__members__)}\"\n        )\n\n    assert _version_str is not None\n    version = TokenizerVersion(_version_str)\n\n    special_tokens_dicts: list[SpecialTokenInfo] | None = untyped.get(\"special_tokens\", None)\n    if special_tokens_dicts is None:\n        # Tokenizer &gt; v7 should find special tokens in the tokenizer file\n        if version &gt; TokenizerVersion(\"v7\"):\n            raise ValueError(\n                f\"Special tokens not found in {path}. \"\n                \"Please update your tokenizer file and include all special tokens you need.\"\n            )\n        else:\n            special_tokens = list(Tekkenizer.DEPRECATED_SPECIAL_TOKENS)\n    else:\n        special_tokens = [token for token in special_tokens_dicts]\n\n    untyped[\"special_tokens\"] = special_tokens\n\n    if mm := untyped.get(\"multimodal\"):\n        # deprecated - only allowed for tokenizers &lt;= v11\n        if version &gt; TokenizerVersion(\"v11\"):\n            raise ValueError(\n                f\"The image config has to be called 'image' in {path} for tokenizers of version {version.value}.\"\n            )\n\n        untyped[\"image\"] = ImageConfig(**mm)\n    elif image := untyped.get(\"image\"):\n        untyped[\"image\"] = ImageConfig(**image)\n\n    if audio := untyped.get(\"audio\"):\n        encoding_config = audio.pop(\"audio_encoding_config\")\n        encoding_config = AudioSpectrogramConfig(**encoding_config)\n        untyped[\"audio\"] = AudioConfig(encoding_config=encoding_config, **audio)\n\n    model_data: ModelData = untyped\n\n    return cls(\n        vocab=model_data[\"vocab\"],\n        special_tokens=special_tokens,\n        pattern=model_data[\"config\"][\"pattern\"],\n        vocab_size=model_data[\"config\"][\"default_vocab_size\"],\n        num_special_tokens=model_data[\"config\"][\"default_num_special_tokens\"],\n        version=version,\n        name=path.name.replace(\".json\", \"\"),\n        image_config=model_data.get(\"image\"),\n        audio_config=model_data.get(\"audio\"),\n        _path=path,\n    )\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.get_special_token","title":"<code>get_special_token(s)</code>","text":"<p>Get the token id of a special token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def get_special_token(self, s: str) -&gt; int:\n    r\"\"\"Get the token id of a special token.\"\"\"\n    if s in self._special_tokens_reverse_vocab:\n        return self._special_tokens_reverse_vocab[s]\n    else:\n        raise ValueError(f\"Unknown control token {s}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.id_to_byte_piece","title":"<code>id_to_byte_piece(token_id, special_token_policy=None)</code>","text":"<p>Convert a token id to its byte representation.</p> <p>Parameters:</p> Name Type Description Default <code>token_id</code> <code>int</code> <p>The token id to convert.</p> required <code>special_token_policy</code> <code>SpecialTokenPolicy | None</code> <p>The policy for handling special tokens. Use the tokenizer's attribute if <code>None</code>. Passing <code>None</code> is deprecated and will be changed to <code>SpecialTokenPolicy.IGNORE</code> in <code>mistral_common=1.10.0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The byte representation of the token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def id_to_byte_piece(self, token_id: int, special_token_policy: SpecialTokenPolicy | None = None) -&gt; bytes:\n    r\"\"\"Convert a token id to its byte representation.\n\n    Args:\n        token_id: The token id to convert.\n        special_token_policy: The policy for handling special tokens.\n            Use the tokenizer's [attribute][mistral_common.tokens.tokenizers.tekken.Tekkenizer.special_token_policy]\n            if `None`. Passing `None` is deprecated and will be changed\n            to `SpecialTokenPolicy.IGNORE` in `mistral_common=1.10.0`.\n\n    Returns:\n        The byte representation of the token.\n    \"\"\"\n    if special_token_policy is None:\n        warnings.warn(\n            (\n                f\"Using the tokenizer's special token policy ({self._special_token_policy}) is deprecated. \"\n                \"It will be removed in 1.10.0. \"\n                \"Please pass a special token policy explicitly. \"\n                \"Future default will be SpecialTokenPolicy.IGNORE.\"\n            ),\n            FutureWarning,\n        )\n        special_token_policy = self._special_token_policy\n\n    if token_id &lt; self.num_special_tokens:\n        if special_token_policy == SpecialTokenPolicy.KEEP:\n            return self._all_special_tokens[token_id][\"token_str\"].encode(\"utf-8\")\n        elif special_token_policy == SpecialTokenPolicy.RAISE:\n            raise ValueError(f\"{token_id} is a special token\")\n        elif special_token_policy == SpecialTokenPolicy.IGNORE:\n            return b\"\"\n        else:\n            raise ValueError(f\"Unknown special token policy {special_token_policy}\")\n\n    return self._model.decode_single_token_bytes(token_id - self.num_special_tokens)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.id_to_piece","title":"<code>id_to_piece(token_id)</code>","text":"<p>Convert a token id to its string representation.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def id_to_piece(self, token_id: int) -&gt; str:\n    r\"\"\"Convert a token id to its string representation.\"\"\"\n    return self.decode([token_id], special_token_policy=SpecialTokenPolicy.KEEP)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.is_byte","title":"<code>is_byte(token_id)</code>","text":"<p>Check if a token id is a byte token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def is_byte(self, token_id: int) -&gt; bool:\n    r\"\"\"Check if a token id is a byte token.\"\"\"\n    return 0 &lt;= token_id - self.num_special_tokens &lt; 256\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.is_special","title":"<code>is_special(token)</code>","text":"<p>Return <code>True</code> if the passed <code>token</code> is a special token.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def is_special(self, token: int | np.integer | str) -&gt; bool:\n    \"\"\"Return `True` if the passed `token` is a special token.\"\"\"\n    if isinstance(token, (int, np.integer)):\n        return token in self._special_token_ids\n    elif isinstance(token, str):\n        return token in self._special_tokens_reverse_vocab\n    else:\n        raise TypeError(f\"Expected int or str, got {type(token).__name__}\")\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.to_string","title":"<code>to_string(tokens)</code>","text":"<p>[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.</p> <p>Use <code>decode</code> with <code>special_token_policy=SpecialTokenPolicy.KEEP</code> instead.</p> <p>This is a convenient method for debugging.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def to_string(self, tokens: list[int]) -&gt; str:\n    r\"\"\"[DEPRECATED] Converts a list of token ids into a string, keeping special tokens.\n\n    Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\n\n    This is a convenient method for debugging.\n    \"\"\"\n    warnings.warn(\n        (\n            \"`to_string` is deprecated and will be removed in 1.10.0. \"\n            \"Use `decode` with `special_token_policy=SpecialTokenPolicy.KEEP` instead.\"\n        ),\n        FutureWarning,\n    )\n    return self._to_string(tokens)\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.Tekkenizer.vocab","title":"<code>vocab()</code>","text":"<p>Get all tokens in the vocabulary as strings.</p> Note <p>This will collapse all tokens for which we have a decoding error into the &lt;?&gt; string. This is bad and results in things like len(set(vocab)) != len(vocab)).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>The vocabulary of the tokenizer.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def vocab(self) -&gt; list[str]:\n    r\"\"\"Get all tokens in the vocabulary as strings.\n\n    Note:\n       This will collapse all tokens for which we have a decoding error into\n       the &lt;?&gt; string. This is bad and results in things like len(set(vocab)) != len(vocab)).\n\n    Returns:\n        The vocabulary of the tokenizer.\n    \"\"\"\n    # when returning self._vocab this will collapse\n    # all tokens for which we have a decoding error into\n    # the &lt;?&gt; string. This is bad and results in things\n    # like len(set(vocab)) != len(vocab))\n    # be careful when using self._vocab\n    return self._vocab\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.TokenInfo","title":"<code>TokenInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Token information in the JSON file.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>The rank of the token.</p> <code>token_bytes</code> <code>str</code> <p>The token in bytes, base64 encoded.</p> <code>token_str</code> <code>str | None</code> <p>The token in string format.</p>"},{"location":"code_reference/mistral_common/tokens/tokenizers/tekken/#mistral_common.tokens.tokenizers.tekken.is_tekken","title":"<code>is_tekken(path)</code>","text":"<p>Check if the given path is a tekken tokenizer file.</p> Source code in <code>src/mistral_common/tokens/tokenizers/tekken.py</code> <pre><code>def is_tekken(path: str | Path) -&gt; bool:\n    r\"\"\"Check if the given path is a tekken tokenizer file.\"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n    return path.is_file() and \"tekken\" in path.name and path.suffix == \".json\"\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/","title":"utils","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils","title":"<code>mistral_common.tokens.tokenizers.utils</code>","text":""},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.chunks","title":"<code>chunks(lst, chunk_size)</code>","text":"<p>Chunk a list into smaller lists of a given size.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list[str]</code> <p>The list to chunk.</p> required <code>chunk_size</code> <code>int</code> <p>The size of each chunk.</p> required <p>Returns:</p> Type Description <code>Iterator[list[str]]</code> <p>An iterator over the chunks.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; all_chunks = list(chunks([1, 2, 3, 4, 5], 2))\n</code></pre> Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def chunks(lst: list[str], chunk_size: int) -&gt; Iterator[list[str]]:\n    r\"\"\"Chunk a list into smaller lists of a given size.\n\n    Args:\n        lst: The list to chunk.\n        chunk_size: The size of each chunk.\n\n    Returns:\n        An iterator over the chunks.\n\n    Examples:\n        &gt;&gt;&gt; all_chunks = list(chunks([1, 2, 3, 4, 5], 2))\n    \"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i : i + chunk_size]\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.download_tokenizer_from_hf_hub","title":"<code>download_tokenizer_from_hf_hub(repo_id, cache_dir=None, token=None, revision=None, force_download=False, local_files_only=False)</code>","text":"<p>Download the tokenizer file of a Mistral model from the Hugging Face Hub.</p> <p>See here for a list of our OSS models.</p> Note <p>You need to install the <code>huggingface_hub</code> package to use this method.</p> <p>Please run <code>pip install mistral-common[hf-hub]</code> to install it.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face repo ID.</p> required <code>cache_dir</code> <code>str | Path | None</code> <p>The directory where the tokenizer will be cached.</p> <code>None</code> <code>token</code> <code>bool | str | None</code> <p>The Hugging Face token to use to download the tokenizer.</p> <code>None</code> <code>revision</code> <code>str | None</code> <p>The revision of the model to use. If <code>None</code>, the latest revision will be used.</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>Whether to force the download of the tokenizer. If <code>True</code>, the tokenizer will be downloaded even if it is already cached.</p> <code>False</code> <code>local_files_only</code> <code>bool</code> <p>Whether to only use local files. If <code>True</code>, the tokenizer will be downloaded only if it is already cached.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The downloaded tokenizer local path for the given model ID.</p> Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def download_tokenizer_from_hf_hub(\n    repo_id: str,\n    cache_dir: str | Path | None = None,\n    token: bool | str | None = None,\n    revision: str | None = None,\n    force_download: bool = False,\n    local_files_only: bool = False,\n) -&gt; str:\n    r\"\"\"Download the tokenizer file of a Mistral model from the Hugging Face Hub.\n\n    See [here](https://huggingface.co/mistralai/models) for a list of our OSS models.\n\n    Note:\n        You need to install the `huggingface_hub` package to use this method.\n\n        Please run `pip install mistral-common[hf-hub]` to install it.\n\n    Args:\n        repo_id: The Hugging Face repo ID.\n        cache_dir: The directory where the tokenizer will be cached.\n        token: The Hugging Face token to use to download the tokenizer.\n        revision: The revision of the model to use. If `None`, the latest revision will be used.\n        force_download: Whether to force the download of the tokenizer. If `True`, the tokenizer will be downloaded\n            even if it is already cached.\n        local_files_only: Whether to only use local files. If `True`, the tokenizer will be downloaded only if it is\n            already cached.\n\n    Returns:\n        The downloaded tokenizer local path for the given model ID.\n    \"\"\"\n    _assert_hub_installed()\n\n    if force_download and local_files_only:\n        raise ValueError(\"You cannot force the download of the tokenizer if you only want to use local files.\")\n\n    if not local_files_only:\n        try:\n            hf_api = huggingface_hub.HfApi()\n            repo_files = hf_api.list_repo_files(repo_id, revision=revision, token=token)\n            local_files_only = False\n        except (requests.ConnectionError, requests.HTTPError, requests.Timeout) as e:\n            if force_download:\n                raise e\n\n            repo_files = list_local_hf_repo_files(repo_id=repo_id, revision=revision)\n            local_files_only = True\n\n            logger.info(\"Could not connect to the Hugging Face Hub. Using local files only.\")\n\n            if len(repo_files) == 0:\n                raise FileNotFoundError(\n                    f\"Could not connect to the Hugging Face Hub and no local files were found for the repo ID {repo_id}\"\n                    f\" and revision {revision}. Please check your internet connection and try again.\"\n                ) from e\n    else:\n        repo_files = list_local_hf_repo_files(repo_id=repo_id, revision=revision)\n        if len(repo_files) == 0:\n            raise FileNotFoundError(\n                f\"No local files found for the repo ID {repo_id} and revision {revision}. Please check the repo ID and\"\n                \" the revision or try to download the tokenizer without setting `local_files_only` to `True`.\"\n            )\n\n    try:\n        tokenizer_file = get_one_valid_tokenizer_file(files=repo_files)\n    except ValueError:\n        raise ValueError(f\"No valid tokenizer file found in the repo {repo_id}.\")\n\n    tokenizer_path = huggingface_hub.hf_hub_download(\n        repo_id=repo_id,\n        cache_dir=cache_dir,\n        filename=tokenizer_file,\n        token=token,\n        revision=revision,\n        local_files_only=local_files_only,\n        force_download=force_download,\n    )\n    return tokenizer_path\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.get_one_valid_tokenizer_file","title":"<code>get_one_valid_tokenizer_file(files)</code>","text":"<p>Get one valid tokenizer file from a list of files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>The list of files to filter.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the tokenizer file.</p> Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def get_one_valid_tokenizer_file(files: list[str]) -&gt; str:\n    r\"\"\"Get one valid tokenizer file from a list of files.\n\n    Args:\n        files: The list of files to filter.\n\n    Returns:\n        The path to the tokenizer file.\n    \"\"\"\n    valid_tokenizer_file_names_and_files = _filter_valid_tokenizer_files(files)\n\n    if len(valid_tokenizer_file_names_and_files) == 0:\n        raise ValueError(\"No tokenizer file found.\")\n    # If there are multiple tokenizer files, we use tekken.json if it exists, otherwise the versioned one.\n    if len(valid_tokenizer_file_names_and_files) &gt; 1:\n        for file_name, tokenizer_file in valid_tokenizer_file_names_and_files:\n            if \"tekken.json\" == file_name:\n                return tokenizer_file\n        tokenizer_file = sorted(valid_tokenizer_file_names_and_files, key=lambda x: x[0])[-1][1]\n        logger.warning(f\"Multiple valid tokenizer files found. Using {tokenizer_file}.\")\n    else:\n        tokenizer_file = valid_tokenizer_file_names_and_files[0][1]\n\n    return tokenizer_file\n</code></pre>"},{"location":"code_reference/mistral_common/tokens/tokenizers/utils/#mistral_common.tokens.tokenizers.utils.list_local_hf_repo_files","title":"<code>list_local_hf_repo_files(repo_id, revision)</code>","text":"<p>list the files of a local Hugging Face repo.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face repo ID.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model to use. If <code>None</code>, the latest revision will be used.</p> required Source code in <code>src/mistral_common/tokens/tokenizers/utils.py</code> <pre><code>def list_local_hf_repo_files(repo_id: str, revision: str | None) -&gt; list[str]:\n    r\"\"\"list the files of a local Hugging Face repo.\n\n    Args:\n        repo_id: The Hugging Face repo ID.\n        revision: The revision of the model to use. If `None`, the latest revision will be used.\n    \"\"\"\n    _assert_hub_installed()\n\n    repo_cache = Path(huggingface_hub.constants.HF_HUB_CACHE) / huggingface_hub.constants.REPO_ID_SEPARATOR.join(\n        [\"models\", *repo_id.split(\"/\")]\n    )\n\n    if revision is None:\n        revision_file = repo_cache / \"refs\" / huggingface_hub.constants.DEFAULT_REVISION\n        if revision_file.is_file():\n            with revision_file.open(\"r\") as file:\n                revision = file.read()\n\n    if revision:\n        revision_dir = repo_cache / \"snapshots\" / revision\n        if revision_dir.is_dir():\n            return os.listdir(revision_dir)\n\n    return []\n</code></pre>"},{"location":"examples/inference/","title":"Inference","text":"<p>We have a few examples of how to use the library with our models:</p> <ul> <li>Chat Completion</li> <li>Text only</li> <li>Image</li> <li>Function calling</li> <li>Audio</li> <li>Fill-in-the-middle (FIM) Completion</li> <li>Audio Transcription</li> </ul>"},{"location":"examples/inference/#chat-completion","title":"Chat Completion","text":""},{"location":"examples/inference/#text-only","title":"Text-only","text":"<pre><code>from mistral_common.protocol.instruct.chunk import TextChunk\nfrom mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nrepo_id = \"mistralai/Mistral-Large-Instruct-2411\"\ntokenizer = MistralTokenizer.from_hf_hub(repo_id)\n\n\nmessages = [\n    UserMessage(content=\"What is the capital of France?\"),\n    AssistantMessage(content=\"The capital of France is Paris.\"),\n    UserMessage(content=\"And the capital of Spain?\"),\n]\n\nrequest = ChatCompletionRequest(messages=messages)\ntokenized = tokenizer.encode_chat_completion(request)\n\n# pass tokenized.tokens to your favorite model\nprint(tokenized.tokens)\n\n# print text to visually see tokens\nprint(tokenized.text)\n</code></pre>"},{"location":"examples/inference/#image","title":"Image","text":"<pre><code>from mistral_common.protocol.instruct.chunk import (\n    ImageURLChunk,\n    TextChunk\n)\nfrom mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nrepo_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\ntokenizer = MistralTokenizer.from_hf_hub(repo_id)\n\nsystem_prompt = \"\"\"You are Mistral Small 3.2, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is 03-06-2025. You have the ability to process images.\"\"\"\n\nmessages = [\n    SystemMessage(content=system_prompt),\n    UserMessage(content=\"What is the capital of France?\"),\n    AssistantMessage(content=\"The capital of France is Paris.\"),\n    UserMessage(content=[TextChunk(text=\"Does this photo comes from there ?\")]),\n    UserMessage(\n        content=[\n            ImageURLChunk(\n                image_url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/1280px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg\"\n            )\n        ]\n    ),\n]\nrequest = ChatCompletionRequest(messages=messages)\ntokenized = tokenizer.encode_chat_completion(request)\n\n# pass tokenized.tokens to your favorite image model\nprint(tokenized.tokens)\nprint(tokenized.images)\n\n# print text to visually see tokens\nprint(tokenized.text)\n</code></pre>"},{"location":"examples/inference/#function-calling","title":"Function calling","text":"<pre><code>from mistral_common.protocol.instruct.chunk import TextChunk\nfrom mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\n\nrepo_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\ntokenizer = MistralTokenizer.from_hf_hub(repo_id)\n\n\nmessages = [UserMessage(content=\"What is the weather in France like?\")]\n\ntool = Tool(\n  function=Function(\n      name=\"get_current_weather\",\n      description=\"Get the current weather\",\n      parameters={\n          \"type\": \"object\",\n          \"properties\": {\n              \"location\": {\n                  \"type\": \"string\",\n                  \"description\": \"The city and state, e.g. San Francisco, CA\",\n              },\n              \"format\": {\n                  \"type\": \"string\",\n                  \"enum\": [\"celsius\", \"fahrenheit\"],\n                  \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n              },\n          },\n          \"required\": [\"location\", \"format\"],\n      },\n  )\n)\n\nrequest = ChatCompletionRequest(messages=messages, tools=[tool])\ntokenized = tokenizer.encode_chat_completion(request)\n\n# pass tokenized.tokens to your favorite agent model\nprint(tokenized.tokens)\n\n# print text to visually see tokens\nprint(tokenized.text)\n</code></pre>"},{"location":"examples/inference/#audio","title":"Audio","text":"<pre><code>from mistral_common.protocol.instruct.chunk import TextChunk, AudioChunk, RawAudio\nfrom mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\n\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\ntokenizer = MistralTokenizer.from_hf_hub(repo_id)\n\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\nbcn_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"bcn_weather.mp3\", repo_type=\"dataset\")\n\ndef file_to_chunk(file: str) -&gt; AudioChunk:\n    audio = Audio.from_file(file, strict=False)\n    return AudioChunk.from_audio(audio)\n\ntext_chunk = TextChunk(text=\"Which speaker do you prefer between the two? Why? How are they different from each other?\")\nuser_msg = UserMessage(content=[file_to_chunk(obama_file), file_to_chunk(bcn_file), text_chunk]).to_openai()\n\n\nrequest = ChatCompletionRequest(messages=[user_msg])\ntokenized = tokenizer.encode_chat_completion(request)\n\n# pass tokenized.tokens to your favorite audio model\nprint(tokenized.tokens)\nprint(tokenized.audios)\n\n# print text to visually see tokens\nprint(tokenized.text)\n</code></pre>"},{"location":"examples/inference/#fim","title":"FIM","text":"<pre><code>from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.fim.request import FIMRequest\n\ntokenizer = MistralTokenizer.from_hf_hub(\"mistralai/Codestral-22B-v0.1\")\n\nprefix = \"\"\"def add(\"\"\"\nsuffix = \"\"\"    return sum\"\"\"\n\nrequest = FIMRequest(prompt=prefix, suffix=suffix)\n\ntokenized = tokenizer.encode_fim(request)\n\n# pass tokenized.tokens to your favorite model\nprint(tokenized.tokens)\n\n# print text to visually see tokens\nprint(tokenized.text)\n</code></pre>"},{"location":"examples/inference/#audio-transcription","title":"Audio Transcription","text":"<pre><code>from mistral_common.protocol.transcription.request import TranscriptionRequest\nfrom mistral_common.protocol.instruct.chunk import RawAudio\nfrom mistral_common.audio import Audio\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nfrom huggingface_hub import hf_hub_download\n\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\ntokenizer = MistralTokenizer.from_hf_hub(repo_id)\n\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\naudio = Audio.from_file(obama_file, strict=False)\n\naudio = RawAudio.from_audio(audio)\nrequest = TranscriptionRequest(model=repo_id, audio=audio, language=\"en\")\n\ntokenized = tokenizer.encode_transcription(request)\n\n# pass tokenized.tokens to your favorite audio model\nprint(tokenized.tokens)\nprint(tokenized.audios)\n\n# print text to visually see tokens\nprint(tokenized.text)\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#installation","title":"Installation","text":"<p>See installation instructions.</p>"},{"location":"usage/#getting-started","title":"Getting Started","text":"<p><code>mistral-common</code> is a collection of common utilities for Mistral to construct valid requests and call tokenizers.</p> <p>To take full advantage of the features of our models, you will need to use text, images, and tools. Building a valid request can be challenging, so we've created a set of tools to help you get started. </p> <p>See the following sections for more details:</p> <ul> <li>The requests and their different types.</li> <li>The images.</li> <li>The tools.</li> <li>The tokenizers and their different layer of abstractions that we use.</li> </ul>"},{"location":"usage/experimental/","title":"Mistral-common Experimental API","text":""},{"location":"usage/experimental/#context","title":"Context","text":"<p>The <code>experimental</code> module provides access to a FastAPI server designed to handle tokenization and detokenization operations through a REST API. This server features:</p> <ol> <li>Tokenization: Converting chat completion requests into token sequences</li> <li>Detokenization: Converting token sequences back into human-readable formats to an AssistantMessage object or a raw string.</li> <li>Chat Completion: Generating an AssistantMessage from a ChatCompletionRequest using an engine backend.</li> </ol> <p>This API serves as a bridge between different providers and tokenization needs regardless of the provider programming language.</p>"},{"location":"usage/experimental/#features","title":"Features","text":"<ol> <li>Tokenizer Version Support: Handles all our tokenizer versions automatically</li> <li>Tool Call Parsing: Automatically extracts tool calls from token sequences</li> <li>Think Chunk Support: Handles think chunks for v13+ tokenizers</li> <li>OpenAI Compatibility: Accepts OpenAI-formatted requests for easier integration</li> </ol>"},{"location":"usage/experimental/#error-handling","title":"Error Handling","text":"<p>The API provides detailed error messages for:</p> <ul> <li>Invalid token sequences</li> <li>Empty token lists</li> <li>Missing tokenizer configuration</li> <li>Validation errors in requests</li> <li>Tool call parsing failures</li> </ul>"},{"location":"usage/experimental/#usage","title":"Usage","text":""},{"location":"usage/experimental/#installation","title":"Installation","text":"<p>To use the experimental API server, you need to install mistral-common with the appropriate dependencies:</p> <pre><code>pip install mistral-common[server]\n</code></pre>"},{"location":"usage/experimental/#launching-the-server","title":"Launching the Server","text":"<p>You can launch the server using the follwing CLI command:</p> <pre><code>mistral_common serve mistralai/Magistral-Small-2507 [validation_mode] \\\n--host 127.0.0.1 --port 8000 \\\n--engine-url http://127.0.0.1:8080 --engine-backend llama_cpp \\\n--timeout 60\n</code></pre>"},{"location":"usage/experimental/#command-line-options","title":"Command Line Options","text":"<ul> <li><code>tokenizer_path</code>: Path to the tokenizer. Can be a HuggingFace model ID or a local path.</li> <li><code>validation_mode</code>: Validation mode to use, choices in: \"test\", \"finetuning\", \"serving\" (Optional, defaults to <code>\"test\"</code>)</li> <li><code>--host</code>: API host (default: <code>127.0.0.1</code>)</li> <li><code>--port</code>: API port (default: <code>0</code> - auto-selects available port)</li> <li><code>--engine-url</code>: URL of the engine (default: <code>http://127.0.0.1:8080</code>)</li> <li><code>--engine-backend</code>: Engine backend to use (default: <code>llama_cpp</code>)</li> <li><code>--timeout</code>: Timeout for the engine (default: <code>60</code>)</li> </ul>"},{"location":"usage/experimental/#available-routes","title":"Available Routes","text":""},{"location":"usage/experimental/#documentation","title":"Documentation","text":"<ul> <li>Route: <code>/</code> or <code>/docs</code></li> <li>Method: GET</li> <li>Description: The Swagger documentation</li> </ul> <p>The Swagger UI provides interactive documentation for all available endpoints.</p>"},{"location":"usage/experimental/#tokenization","title":"Tokenization","text":""},{"location":"usage/experimental/#tokenize-request","title":"Tokenize Request","text":"<ul> <li>Route: <code>/tokenize/request</code></li> <li>Method: POST</li> <li>Description: Tokenizes a chat completion request</li> <li>Request Body: <ul> <li>Chat completion request in either:<ul> <li>Mistral-common format (ChatCompletionRequest)</li> <li>OpenAI-compatible format (OpenAIChatCompletionRequest). Streaming is not supported.</li> </ul> </li> </ul> </li> <li>Response: List of token IDs</li> </ul> <p>Example requests:</p> <pre><code>import requests\n\nfrom fastapi.encoders import jsonable_encoder\n\nfrom mistral_common.protocol.instruct.chunk import TextChunk, ThinkChunk\nfrom mistral_common.protocol.instruct.messages import SystemMessage, UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n# Simple request\nresponse = requests.post(\"http://localhost:8000/v1/tokenize/request\", json={\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n})\nprint(response.json())\n# [1, 3, 22177, 1044, 2606, 1584, 1636, 1063, 4]\n\n# Complex request with tools and think chunks\nsystem_message = SystemMessage(\n    content=[\n        TextChunk(\n            text=(\n                \"First draft your thinking process (inner monologue) until you arrive at a response. Format your \"\n                \"response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and \"\n                \"the response in the same language as the input.\\n\\nYour thinking process must follow the template \"\n                \"below:\"\n            )\n        ),\n        ThinkChunk(\n            thinking=(\n                \"Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and \"\n                \"as long as you want until you are confident to generate the response. Use the same language as the \"\n                \"input.\"\n            ),\n            closed=True,\n        ),\n        TextChunk(text=\"Here, provide a self-contained response.\"),\n    ],\n)\n\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/tokenize/request\",\n    json=jsonable_encoder(\n        ChatCompletionRequest(messages=[\n            system_message,\n            UserMessage(content=\"How many 'r' are there in strawberry ?\")\n        ])\n    ),\n)\nprint(response.json())\n# [1, 17, 10107, 19564, 2143, ..., 33681, 3082, 4]\n</code></pre>"},{"location":"usage/experimental/#detokenization","title":"Detokenization","text":""},{"location":"usage/experimental/#detokenize-to-assistant-message","title":"Detokenize to Assistant Message","text":"<ul> <li>Route: <code>/detokenize/</code></li> <li>Method: POST</li> <li>Description: Converts tokens to an assistant message with tool call parsing</li> <li>Request Body: <ul> <li>List of token IDs</li> </ul> </li> <li>Response: AssistantMessage object with parsed content and tool calls</li> </ul> <p>Example requests: <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/detokenize/\",\n    json=[1, 3, 22177, 1044, 2606, 1584, 1636, 1063, 4]\n)\nprint(response.json())\n# {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Hello, how are you?'}], 'tool_calls': None, 'prefix': True}\n\n# With think chunks\nresponse = requests.post(\n    \"http://localhost:8000/v1/detokenize/\",\n    json=[12598, 1639, 3648, 2314, 1494, 1046, 34, 6958, 1584, 1032, 1051, 1576, 1114, 1039, 1294, 51567, 33681, 3082, 35, 19587, 1051, 19587, 2]\n)\nprint(response.json())\n# {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Let me think about it.'}, {'type': 'thinking', 'thinking': \"There are 3 'r' in strawberry ?\", 'closed': True}, {'type': 'text', 'text': '$$3$$'}], 'tool_calls': None, 'prefix': False}\n\n# With tool calls\nresponse = requests.post(\n    \"http://localhost:8000/v1/detokenize/\",\n    json=[9, 12296, 1095, 99571, 32, 38985, 1039, 3494, 4550, 1576, 1314, 3416, 33681, 2096, 1576, 27965, 4550, 1576, 1114, 1039, 27024, 2]\n)\nprint(response.json())\n# {'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'null', 'type': 'function', 'function': {'name': 'count_letters', 'arguments': \"{'word': 'strawberry', 'letter': 'r'}\"}}], 'prefix': False}\n</code></pre></p>"},{"location":"usage/experimental/#detokenize-to-string","title":"Detokenize to String","text":"<ul> <li>Route: <code>/detokenize/string</code></li> <li>Method: POST</li> <li>Description: Converts tokens to a raw string</li> <li>Request Body:<ul> <li><code>tokens</code>: List of token IDs</li> <li><code>special_token_policy</code>: Policy for handling special tokens. See SpecialTokenPolicy</li> </ul> </li> <li>Response: Detokenized string</li> </ul> <p>Example request: <pre><code>import requests\n\nresponse = requests.post(\"http://localhost:8000/v1/detokenize/string\", json={\n    \"tokens\":[1, 3, 22177, 1044, 2606, 1584, 1636, 1063, 4],\n    \"special_token_policy\": 1,\n})\nprint(response.json())\n# &lt;s&gt;[INST]Hello, how are you?[/INST]\n</code></pre></p>"},{"location":"usage/experimental/#chat-completions","title":"Chat Completions","text":"<ul> <li>Route: <code>/v1/chat/completions</code></li> <li>Method: POST</li> <li>Description: Generates text from a chat completion request. This endpoint forwards the request to the engine server.</li> <li>Request Body:<ul> <li>Chat completion request in either:<ul> <li>Mistral-common format (ChatCompletionRequest)</li> <li>OpenAI-compatible format (OpenAIChatCompletionRequest). Streaming is not supported.</li> </ul> </li> </ul> </li> <li>Response: Chat completion response in OpenAI-compatible format</li> </ul> <p>Example request: <pre><code>import requests\n\nresponse = requests.post(\"http://localhost:8000/v1/chat/completions\", json={\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n})\nprint(response.json())\n# {'role': 'assistant', 'content': 'Hello! I am doing well, thank you for asking.'}\n</code></pre></p>"},{"location":"usage/experimental/#issues-and-feedback","title":"Issues and feedback","text":"<p>If you encounter any issues or have feedback, please open an issue or discussion on the GitHub repository.</p>"},{"location":"usage/images/","title":"Images","text":"<p>Most of the recently released Mistral models support image inputs. Images are represented as BaseContentChunk objects within the <code>messages</code> field of the ChatCompletionRequest. Encoding an image via a ImageEncoder will return:</p> <ul> <li>a sequence of special tokens representing the image.</li> <li>the image normalized as a numpy array.</li> </ul>"},{"location":"usage/images/#supported-image-formats","title":"Supported image formats","text":"<p>Mistral Image encoders use Pillow to decode images and OpenCV to encode. Hence, the supported formats are the same as Pillow's. The images can be provided as: - an ImageURLChunk: a pydantic model containing an image URL from which the image will be downloaded. - an ImageChunk: a pydantic model containing a serialized image that can be either a base64 string or a pillow image.</p>"},{"location":"usage/images/#use-an-image-encoder-with-our-tokenizer","title":"Use an Image encoder with our tokenizer","text":"<p>Our tokenizers can an ImageEncoder that is configured with ImageConfig.</p> <p>The attributes of the ImageConfig configure how the images will be patched into tokens:</p> <ul> <li><code>image_patch_size</code>: the square size of a patch in pixels to form one token. E.g if the image is 224x224 and the patch size is 14, then the image will be divided into 16x16 patches.</li> <li><code>max_image_size</code>: the maximum size of the image in pixels. If the image is larger, it will be resized to this size.</li> <li><code>spatial_merge_size</code>: the number of patches to merge into one token. This is useful to reduce the number of redundant tokens in the image. E.g if the image is 224x224 and the patch size is 14, then the image will be divided into 16x16 patches. If the spatial merge size is 2, then the image will be divided into 8x8 patches.</li> </ul> <pre><code>from mistral_common.protocol.instruct.chunk import ImageURLChunk\nfrom mistral_common.tokens.tokenizers.image import ImageEncoder, ImageConfig, SpecialImageIDs\n\nspecial_ids = SpecialImageIDs(img=10, img_break=11, img_end=12)  # These are normally automatically set by the tokenizer\n\nconfig = ImageConfig(image_patch_size=14, max_image_size=224, spatial_merge_size=2)\n\nimage = ImageURLChunk(image_url=\"https://live.staticflickr.com/7250/7534338696_b33e941b7d_b.jpg\")\n\nencoder = ImageEncoder(config, special_ids)\nencoder(image)\n</code></pre>"},{"location":"usage/images/#tokenize-an-image","title":"Tokenize an image","text":"<p>Let's load the tekken tokenizer used for Mistral Small 3.1's tokenizer to encode and tokenize an image.</p> <pre><code>from huggingface_hub import hf_hub_download\n\nfrom mistral_common.protocol.instruct.chunk import ImageURLChunk, TextChunk\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nmodel_id = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\ntokenizer = MistralTokenizer.from_hf_hub(repo_id=model_id, token=\"your_hf_token\")\n\ntokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        messages=[\n            UserMessage(\n                content=[\n                    ImageURLChunk(image_url=\"https://live.staticflickr.com/7250/7534338696_b33e941b7d_b.jpg\"),\n                    TextChunk(text=\"What is displayed in this image?\"),\n                ]\n            )\n        ],\n    )\n)\n# Tokenized(tokens=[1, 3, 10, 10, ...], text='&lt;s&gt;[INST][IMG][IMG][IMG][IMG]...', prefix_ids=None, images=[array[[(0.95238595, 0.95238795, 0.95224484, ...,)]]])\n</code></pre> <p>The output contains:</p> <ul> <li>the text: the string equivalent of the tokens. The image is represented into a sequence of special <code>[IMG]</code> tokens with <code>[IMG_BREAK]</code> at regular intervals and <code>[IMG_END]</code> at the end. An image is a grid and each <code>[IMG]</code> represent a patch, the <code>IMG_BREAK</code> tokens the end of a row. The <code>IMG_END</code> token is used to mark the end of the image.</li> <li>the tokens: identifier used by the model for the text. The special tokens of images are not directly used by the model, but replaced by the features of an image encoder.</li> <li>the prefix_ids: Used for FIM (Fill-In-the-Middle) tasks, here it is <code>None</code>.</li> <li>the images: the images normalized as a numpy array.</li> </ul>"},{"location":"usage/install/","title":"Install","text":""},{"location":"usage/install/#pip","title":"Pip","text":"<p>You can install the library using pip: <pre><code>pip install mistral-common\n</code></pre></p> <p>We propose different dependencies to install depending on your needs: - <code>image</code>: to use the image tokenizers. - <code>audio</code>: to use the audio tokenizers. - <code>hf-hub</code>: to download the tokenizers from the Hugging Face Hub. - <code>sentencepiece</code>: to allow the use of SentencePiece tokenizers. This is now optional as we only release <code>Tekken</code> tokenizers for recent models. - [Experimental] <code>server</code>: to use our tokenizers in a server mode.</p> <p>Each dependency is optional and can be installed separately or all together using the following commands: <pre><code>pip install \"mistral-common[image]\"\npip install \"mistral-common[audio]\"\npip install \"mistral-common[hf-hub]\"\npip install \"mistral-common[sentencepiece]\"\npip install \"mistral-common[server]\"\npip install \"mistral-common[image,audio,hf-hub,sentencepiece,server]\"\n</code></pre></p>"},{"location":"usage/install/#from-source","title":"From source","text":"<p>To build it for source, you can clone the repository and install it using uv or pip. We recommend using uv for faster and more reliable dependency resolution: <pre><code>git clone https://github.com/mistralai/mistral-common.git\ncd mistral-common\nuv sync --frozen --extra image # or --all-extras to install all dependencies.\n</code></pre></p> <p>For development, you can install the <code>dev</code> group and/or the <code>docs</code> groups: <pre><code>uv sync --frozen --all-extras --group dev # and/or --group docs.\n</code></pre></p>"},{"location":"usage/requests/","title":"Requests","text":"<p>To query an AI assistant like Mistral's LeChat or ChatGPT you need to provide the following:</p> <ul> <li>The history of the conversation between the user, the assistant and the tool calls.</li> <li>The tools available to the assistant.</li> <li>The context of the request.</li> </ul> <p>In <code>mistral-common</code>, we currently support the following requests types:</p> <ul> <li>Instruct requests:<ul> <li>Chat completion requests.</li> <li>Fill-In-the-Middle completion.</li> </ul> </li> <li>Transcription requests</li> </ul> <p>Every instruct requests should be encoded with its corresponding <code>encode_function</code> function by the tokenizers.</p>"},{"location":"usage/requests/#chat-completion","title":"Chat completion","text":"<p>Chat completion consists in a conversation between a user and an assistant. The assistant can call tools to enrich its response. Some of our tokenizers also support the use of images (see the images)</p> <p>Every chat completion request are defined via ChatCompletionRequest.</p> <p>To perform actual task every requests should follow the following structure:</p> <ol> <li>validate the request via a MistralRequestValidator</li> <li>normalize the requests via the InstructRequestNormalizer.</li> <li>encode the request.</li> </ol> <p>Using the MistralTokenizer.encode_chat_completion method will perform all these steps for you.</p> <p>Following this design ensures minimizing unexpected behavior from the user.</p>"},{"location":"usage/requests/#conversation","title":"Conversation","text":"<p>A conversation with a model is a sequence of messages. Each message can be a user message, an assistant message, a tool message or a system message. To ease the creation of these messages, we provide a set of Pydantic classes that you can use to create them:</p> <ul> <li>UserMessage: a message from the user. Users are the ones that interact with the model.</li> <li>AssistantMessage: a message from the assistant. The assistant is the model itself.</li> <li>ToolMessage: a message from a tool. Tools are functions that the model can call to get information to answer the user's question.</li> <li>SystemMessage: a message from the system. Also called <code>System Prompt</code>, it is a set of instructions that the model should follow to answer the user's question. This allows you to customize the behavior of the model.</li> </ul>"},{"location":"usage/requests/#tools","title":"Tools","text":"<p>Tools are functions that the model can call to get information to answer the user's question. See the Tools section for more information.</p>"},{"location":"usage/requests/#example","title":"Example","text":"<p>Here is an example of a request where a user asks for the weather in Paris. The model is also given access to a <code>get_current_weather</code> tool to get the weather:</p> <pre><code>from mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nrequest = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris\"),\n    ],\n)\n\ntokenizer = MistralTokenizer.v3()\ntokenizer.encode_chat_completion(request)\n</code></pre>"},{"location":"usage/requests/#fim","title":"FIM","text":"<p>Fill In the Middle (FIM) is a task where the model is given a prefix and a suffix and is asked to fill in the middle. This is useful for code completion, where the model is given a prefix of code and is asked to complete the code.</p> <p>A pydantic class FIMRequest is defined to ease the creation of these requests.</p> <pre><code>from mistral_common.protocol.fim.request import FIMRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nrequest = FIMRequest(\n    prompt=\"def hello_world():\\n    print('Hello, world!')\",\n    suffix=\"\\n\\nhello_world()\",\n)\n\ntokenizer = MistralTokenizer.v3()\ntokenizer.encode_fim(request)\n</code></pre>"},{"location":"usage/requests/#transcription","title":"Transcription","text":"<p>Transcription is a task where the model is given an audio file and returns the text corresponding to the speech contained in the audio.</p> <p>A pydantic class TranscriptionRequest is defined to ease the creation of these requests.</p> <pre><code>from mistral_common.protocol.transcription.request import TranscriptionRequest\nfrom mistral_common.protocol.instruct.chunk import RawAudio\nfrom mistral_common.audio import Audio\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\nfrom huggingface_hub import hf_hub_download\n\nrepo_id = \"mistralai/voxtral-mini\"\ntokenizer = MistralTokenizer.from_hf_hub(repo_id)\n\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\naudio = Audio.from_file(obama_file, strict=False)\n\naudio = RawAudio.from_audio(audio)\nrequest = TranscriptionRequest(model=repo_id, audio=audio, language=\"en\")\n\ntokenizer.encode_transcription(request)\n</code></pre>"},{"location":"usage/tokenizers/","title":"Tokenizers","text":""},{"location":"usage/tokenizers/#how-do-we-perform-tokenization","title":"How do we perform tokenization ?","text":"<p>Tokenization is the process of converting a sequence of characters into a sequence of tokens. To dive deeper into the tokenization process, you can read the tokenization guide on our website.</p> <p>Historically, we based our tokenizers on the following libraries:</p> <ul> <li>SentencePiece: we moved from this library to the one below.  </li> <li>tiktoken: we use this library for our tekken tokenizers. It is more efficient for multilingual tokenization.</li> </ul>"},{"location":"usage/tokenizers/#tokenizer","title":"Tokenizer","text":"<p>For our releases, we provide a tokenizer file that you can use to load a tokenizer. If the tokenizer is based on the <code>tiktoken</code> library, the file is generally named <code>tekken.json</code>. Here is how you could load the Mistral Small 3.1 Instruct's tokenizer:</p> <pre><code>from mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\ntokenizer = MistralTokenizer.from_hf_hub(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", token=\"your_hf_token\")\n\nchat_completion = ChatCompletionRequest(\n    messages=[\n        UserMessage(role=\"user\", content=\"Hello, how are you?\"),\n    ]\n)\ntokenizer.encode_chat_completion(chat_completion).tokens\n# [1, 3, 22177, 1044, 2606, 1584, 1636, 1063, 4]\n</code></pre> <p>We provide three layer of abstraction for our tokenizers:</p> <ul> <li>Tekkenizer or SentencePieceTokenizer: raw tokenizers to handle raw data to convert them into ids and vice versa.</li> <li>InstructTokenizer: instruct tokenizers that wrap the raw tokenizers to add several helper methods for the different tasks (chat completion or FIM). They are versioned.</li> <li>MistralTokenizer: mistral tokenizers that validate the requests, see requests section, and call the instruct tokenizers.</li> </ul> <p>For instance, you can directly load the Tekkenizer:</p> <pre><code>from huggingface_hub import hf_hub_download\n\nfrom mistral_common.tokens.tokenizers.tekken  import Tekkenizer\n\nmodel_id = \"mistralai/Devstral-Small-2505\"\ntekken_file = hf_hub_download(repo_id=model_id, filename=\"tekken.json\", token=\"your_hf_token\")\n\ntokenizer = Tekkenizer.from_file(tekken_file)\ntokenizer.encode(\"Plz tekkenize me\", bos=True, eos=True)\n# [1, 4444, 1122, 16058, 3569, 2033, 1639, 2]\n</code></pre> <p>See the Examples section for examples on how to use the tokenizers with our models.</p>"},{"location":"usage/tokenizers/#special-tokens","title":"Special tokens","text":"<p>Special tokens are tokens that have a special meaning for the model. They are used to mark the beginning or end of a sequence, an image, tools, etc. For example, in the Mistral Small 3.1 Instruct tokenizer, some special tokens include:</p> <ul> <li><code>&lt;s&gt;</code>: Beginning of a sequence</li> <li><code>&lt;/s&gt;</code>: End of a sequence</li> <li><code>[INST]</code>: Beginning of an instruction</li> <li><code>[/INST]</code>: End of an instruction</li> <li><code>[TOOL_CALLS]</code>: Beginning of a tool call</li> <li><code>[IMG]</code>: Content of an image</li> <li><code>[IMG_BREAK]</code>: End of a row in an image</li> <li><code>[IMG_END]</code>: End of an image</li> <li>...</li> </ul> <p>These tokens are defined in the tokenizer configuration file (recommended) or at instantiation for the Tekkenizer (deprecated).</p> <p>In <code>mistral-common</code>, special tokens are never encoded directly. This means that:</p> <pre><code>tokenizer.encode(\"&lt;s&gt;\")\n</code></pre> <p>will not return the ID of the <code>&lt;s&gt;</code> token. Instead, it will return a list of IDs corresponding to the tokenization of the string <code>\"&lt;s&gt;\"</code>. The special token IDs are added directly to the sequence of IDs when encoding the requests.</p> <p>To add new tokens to the tokenizer and use them correctly, you need to:</p> <ol> <li>Add the tokens to the tokenizer configuration file.</li> <li>Include the special token IDs in the input IDs when encoding requests.</li> </ol> <p>We are open to suggestions for improvement. Please open an issue if you have any feedback.</p>"},{"location":"usage/tools/","title":"Tools","text":"<p>To execute external tools, like searching the web, LLMs need to be given the ability to call such tools whenever they see fit. This is usually achieved by adding all available tools that the LLM can call into the context of the ChatCompletionRequest and representing tool calls as well as tool message as its own objects.</p>"},{"location":"usage/tools/#define-tools","title":"Define tools","text":"<p>To define tools in <code>mistral-common</code>, you use the Tool class. Let's define a simple tool that fetches the current weather for a given city.</p> <p>The example function below is not a real call that calls the weather, but it very well be an call to a weather API. We hardcode random numbers here just for demonstrative purposes.</p> <pre><code>from mistral_common.protocol.instruct.tool_calls import Tool, Function\n\n\ndef get_current_weather(location: str, format: str) -&gt; int:\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return 22 if format == \"celsius\" else 72\n    elif \"san francisco\" in location.lower():\n        return 14 if format == \"celsius\" else 57\n    elif \"paris\" in location.lower():\n        return 18 if format == \"celsius\" else 64\n    else:\n        return 20 if format == \"celsius\" else 68\n\nweather_tool = Tool(\n    function=Function(\n        name=\"get_current_weather\",\n        description=\"Get the current weather\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"format\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                },\n            },\n            \"required\": [\"location\", \"format\"],\n        },\n    )\n)\n</code></pre>"},{"location":"usage/tools/#tool-calling-and-messages","title":"Tool calling and messages","text":"<p>When a model decides to call a tool, it will add a ToolCall to the AssistantMessage. The tool call will have an <code>id</code> and a FunctionCall with the name of the function and the arguments to pass to it.</p> <pre><code>from mistral_common.protocol.instruct.messages import AssistantMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import (\n    Function,\n    FunctionCall,\n    Tool,\n    ToolCall,\n)\n\n\nAssistantMessage(\n    content=None,\n    tool_calls=[\n        ToolCall(\n            id=\"VvvODy9mT\",\n            function=FunctionCall(\n                name=\"get_current_weather\",\n                arguments='{\"location\": \"Paris, France\", \"format\": \"celsius\"}',\n            ),\n        )\n    ],\n)\n</code></pre> <p>Then you can execute the function and return the result in a ToolMessage. The ToolMessage must have the same <code>id</code> as the ToolCall it's responding to.</p> <pre><code>from mistral_common.protocol.instruct.messages import ToolMessage\n\n\nToolMessage(tool_call_id=\"VvvODy9mT\", name=\"get_current_weather\", content=\"22\")\n</code></pre>"},{"location":"usage/tools/#putting-it-all-together","title":"Putting it all together","text":"<p>Now we can put it all together and create a ChatCompletionRequest with the tools and the messages. We can then use the MistralTokenizer to encode the request.</p> <pre><code>from mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    SystemMessage,\n    ToolMessage,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import (\n    Function,\n    FunctionCall,\n    Tool,\n    ToolCall,\n)\n\nrequest = ChatCompletionRequest(\n    messages=[\n        SystemMessage(content=\"You are a helpful assistant.\"),\n        UserMessage(content=\"What's the weather like in Paris?\"),\n        AssistantMessage(\n            content=None,\n            tool_calls=[\n                ToolCall(\n                    id=\"VvvODy9mT\",\n                    function=FunctionCall(\n                        name=\"get_current_weather\",\n                        arguments='{\"location\": \"Paris, France\", \"format\": \"celsius\"}',\n                    ),\n                )\n            ],\n        ),\n        ToolMessage(tool_call_id=\"VvvODy9mT\", name=\"get_current_weather\", content=\"22\"),\n    ],\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the user's location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n)\n\ntokenizer = MistralTokenizer.v3()\ntokens = tokenizer.encode_chat_completion(request).tokens\n</code></pre> <p>Now the model can use the tool result to answer the user's question.</p>"}]}